22ème Traitement Automatique des Langues Naturelles, Caen, 2015
Recherche des indices permettant une identification: l'anonymisation des
transcriptions du corpus ESLO
Iris Eshkol-Taravella1 Olivier Baude1 Denis Maurel2 Layal Kanaan-Caillol1

(1) LLL, UMR 7270, CNRS, UFR LLSH, 10 Rue de Tours 45065 ORLEANS cedex 2
(2) Université François-Rabelais de Tours, LI

iris.eshkol@univ-orleans.fr, olivier.baude@ univ-orleans.fr, denis.maurel@univ-tours.fr,
layal.kanaan@univ-orleans.fr
Résumé.         Cet article aborde la question de l’anonymisation automatique des corpus oraux afin de permettre leur
utilisation et diffusion sur la Toile. Nous proposons une analyse des éléments constituant un « faisceau d’indices » qui,
dans un certain contexte, contribue à l’identification. Ces indices dépassent par leur diversité et leur hétérogénéité les
entités nommées. Nous décrivons ensuite une expérimentation du repérage automatique de ce faisceau d’indices dans les
transcriptions.

Abstract.
Recognizing clues leading to identification: anonymizing the transcriptions of the ESLO speech corpus
This article tackles the question of oral corpus anonymization in preparation for its diffusion on the Web. We first
analyze elements constituting a « clues set » which contribute to the identification. Those clues exceed named entities by
their diversity and heterogeneity. Then we describe an experiment based on a module of automatic recognition of its
clues in the transcriptions.
Mots-clés :       anonymisation, anonymisation automatique, corpus oral, faisceau d’indices, données personnelles,
identification
Keywords: anonymisation, automatic anonymisation, oral corpus, indications set, personal data, identification

1    Introduction
Grâce au développement des outils informatiques, la mise à disposition de différents corpus a modifié le travail des
chercheurs en linguistique, en sciences sociales et humaines et en traitement automatique des langues (TAL). Les
initiatives actuelles se développent autour de la diffusion et de la disponibilité de ces ressources en accès - souvent libre
- sur la Toile. Les corpus oraux en langues étrangères le BNC1, le Russian National Corpus2 ou encore le National
Corpus of Polish3, ou en français, CLAPI4, PFC5, CRFP6, Corpus de la parole, etc. sont apparus sur le Toile et plus
récemment la France s’est doté d’un EQUIPEX dédié à la diffusion des ressources linguistiques (EQUIPEX
ORTOLANG). Pour diffuser ces corpus, les questions juridiques dont celle de leur anonymisation se sont avérées
primordiales.
3
British National Corpus, http://www.natcorp.ox.ac.uk/
2
http://www.ruscorpora.ru/en/index.html
3
http://nkjp.pl/index.php?page=0&lang=1
4
Corpus de langues parlées en interaction, http://clapi.univ-lyon2.fr/
5
Phonologie du français contemporain, http://www.projet-pfc.net/?accueil:intro
6
Corpus de référence du français parlé, http://www.up.univ-mrs.fr/delic/crfp

ESHKOL-TARAVELLA I., BAUDE O., KANAAN-CAILLOL L. ET MAUREL D.
La linguistique sur corpus oraux a bénéficié d’un travail précurseur pour la collecte et la diffusion d’enregistrements
sonores et de leurs transcriptions. Sous l’égide du Ministère de la Culture et du CNRS un groupe de travail constitué de
linguistes, d’informaticiens, de juristes et de conservateurs a réfléchi aux aspects juridiques et éthiques de l’usage des
corpus oraux. Ce travail s’est concrétisé par la publication de l’ouvrage Corpus oraux, guide des bonnes pratiques 2006
(Baude et al., 2006). L’anonymisation est une pratique qui répond à un impératif juridique précis. Sans recueil du
consentement de la personne enregistrée, il est obligatoire d’empêcher son identification. L’impossibilité d’identifier est
une notion complexe qu’on a trop souvent réduite à l’effacement des noms propres. La tâche est bien plus difficile, mais
aussi plus stimulante pour les recherches en linguistique et en TAL.
L’anonymisation relève de procédures différentes selon qu’on traite l’enregistrement sonore, sa transcription ou les
métadonnées descriptives. Toutefois, dans tous les cas, l’objectif reste le même. Si selon certains juristes la voix est une
donnée identifiante ce qui nécessiterait de modifier le signal acoustique de tout enregistrement et par là même obèrerait
toute recherche en linguistique, les pratiques des chercheurs s’orientent plus généralement vers un traitement des
données personnelles au sens large. Que ce soit sur l’oral ou sur l’écrit celles-ci sont diverses, il peut s’agir d’une forme
nominative, d’une profession, d’un statut, d’une caractéristique physique, etc. et/ou du recoupement de plusieurs de ces
informations. Si l’on convient que l’anonymisation ne se réduit pas à l’effacement des noms propres, il est nécessaire de
définir avec précision quels sont les traitements à effectuer pour répondre à l’objectif de réduire les possibilités
d’identification. Dans le cas de grands corpus, ces traitements deviennent une étape fondamentale du travail de
constitution du corpus avec des effets très importants sur la gestion et la diffusion des données.
Le travail décrit dans cet article porte sur le corpus oral ESLO (Enquête Sociolinguistique à Orléans). Il s’agit d’un
grand corpus de données orales qui regroupe deux enquêtes ESLO 1 et ESLO 2 (Baude, Dugua, 2011, Eshkol-Taravella
et al., 2012). ESLO 1 a été réalisé entre 1968 et 1974, à l’initiative d’universitaires britanniques avec une visée
didactique : l’enseignement du français langue étrangère dans le système public d’éducation anglais. Il a été numérisé et
transcrit par l’équipe du Laboratoire Ligérien de Linguistique (LLL). ESLO2 est une nouvelle enquête, débutée en 2008
par le LLL. Réunis, ESLO 1 et ESLO 2 forment une collection de 700 heures d’enregistrement (10 millions de mots), ce
qui est considéré aujourd’hui comme une valeur repère pour les investigations projetées. Il s’agit en somme d’un très
grand corpus dont l’objectif de mise à disposition a déclenché une réflexion sur les éléments permettant l’identification
du locuteur et de toute autre personne mentionné dans le discours de celui-ci et sur leur repérage automatique.
2     Identification à travers un « faisceau d’indices »
Selon le Dictionnaire d’analyse du discours, « l’identité résulte, à la fois, des conditions de production qui contraignent
le sujet, conditions qui sont inscrites dans la situation de communication et/ou dans le préconstruit discursif, et des
stratégies que celui-ci met en œuvre de façon plus ou moins consciente » (Charaudeau, Maingueneau, 2002:300). Les
auteurs distinguent une identité psychosociale consistant en traits qui définissent le sujet selon son âge, son sexe, son
statut, etc. et une identité discursive du sujet énonciateur « qui peut être décrite à l’aide de catégories locutives, de
modes de prise de parole, de rôles énonciatifs et de modes d’interventions » (ib.) Nous n’allons pas nous intéresser, dans
cette étude, aux stratégies discursives que choisit le sujet parlant pour se construire une identité : sa manière de prendre
la parole, de thématiser ses propos, d’organiser son argumentation. Notre objectif est d’étudier, dans le discours oral,
des éléments qui permettent de distinguer le sujet parlant et la personne dont on parle des autres et, par conséquent, de
les reconnaître. Nous avons appelé l’ensemble de ces éléments un « faisceau d’indices ». On peut identifier la personne
en la dénommant, c’est-à-dire en la mentionnant par son nom, ou en la décrivant, c’est-à-dire en représentant certains de
ses traits, voire de ses activités. Anonymiser le corpus consiste dans le repérage de ces indices et leur substitution par un
hyperonyme ou un élément à référents multiples. Ces indices peuvent être de nature lexicale et sémantique très variée :
des entités nommées (section 2.1), d’une part, mais aussi des groupes nominaux fondés sur le nom commun désignant
les traits caractéristiques ou des groupes verbaux, énoncés décrivant les habitudes et les activités sociales de la personne
(section 2.2).
2.1    Entités nommées identifiantes

Traditionnellement la tâche d’anonymisation s’arrête au repérage des entités nommées (noms de personnes, lieux,
organisations, âges, etc.) (Ehrmann, 2008, Nadeau, Sekine, 2004). C’est le cas de plusieurs travaux en TAL dans le
domaine médical (Meyster et al., 2010, Tweit et al., 2004, Raaj, 2012, Uzuner et al., 2007, Grouin, Zweigenbaum,
2011) qui portent sur les documents écrits (rapports, dossiers médicaux, etc.) et où les informations à anonymiser sont
assez homogènes et souvent regroupées dans un endroit précis. Un de ses outils disponible gratuitement est Medina

22ème Traitement Automatique des Langues Naturelles, Caen, 2015
(Medical Information Anonymization7). Il repère automatiquement à l’aide de patrons et de lexiques les noms de
personnes, les lieux, les noms d’hôpitaux et les informations numériques comme les adresses, âges, numéros de
téléphones, etc. dans les documents cliniques en français.

Les entités nommées sont effectivement les candidates idéales à l’anonymisation car par définition, « on appelle entité
nommée tout expression linguistique qui réfère à une entité unique du modèle de manière autonome dans le corpus »
(Ehrmann, 2011), les entités nommées sont donc censées renvoyer vers un référent unique. Or il s’avère que tous les
entités nommées mentionnées dans le discours ne sont pas identifiantes du point de vue de l’anonymisation.

- Noms de personnes

Le premier cas des entités nommées est les noms de personne. C’est un indice fort pour l’anonymisation car le rôle
même de ces noms est de nommer, c’est-à-dire d’indiquer le référent d'une personne mentionnée. Cela est prouvé par
l’anonymisation manuelle des transcriptions (section 3). Dans un test sur 112 fichiers d’ESLO1, 168 éléments ont été
masqués. Parmi eux, les 159 éléments ont été remplacés par hyperonyme NPERS.

Les noms de personne jouent aussi le rôle primordial dans les travaux sur la détection de l’identité du locuteur dans les
journaux télévisés (Charhad, Quénot, 2005). Les auteurs reconnaissent le locuteur grâce aux patrons qui détectent la
personne qui se présente, qui vient de parler (le locuteur remercie, par exemple, l’orateur précédent en l’appelant par
son nom) et la personne qui va parler (le locuteur passe la parole à un autre orateur en le nommant).

Est-ce que tous les noms évoqués dans le discours pointent vers l’identité de la personne ? Les noms de famille ou
prénoms rares comme Eshkol ou Kanaan, dans le cadre de la ville comme Orléans, peuvent éventuellement identifier la
personne. Pourtant, dans le cadre de l’anonymisation, les noms de personnalités Sarkozy, Cotillard, étant les noms
publics ne doivent pas être anonymisés. Les noms de famille très répandus qui renvoient à un nombre élevé de référents
comme Dupont, Durand ne donnent aucune information sur la personne et ne permettent pas à eux-seuls de l’identifier.

- Fonction

Selon le guide d’annotation des entités nommés Quaero (Rosset et al. 2011), la fonction (func) comprend les métiers, les
fonctions et les rôles sociaux de la personne.

Nommer la personne par sa fonction maire d’Orléans, directeur du collège de Saint-Jean de Braye est un acte qui peut
renvoyer à un référent unique. On est de nouveau en présence d’un indice fort. Ce n’est pourtant pas le cas d’un nom de
métier. La mention dans le discours du métier enseignant-chercheur ne veut rien dire sur l’identité de la personne, mais
dans le contexte je suis enseignant-chercheur il devient un indice de l’identification.

- Autres entités nommées

Les autres entités nommées présentes dans le discours doivent aussi avoir un lien avec le locuteur ou la personne qu’il
mentionne dans son discours pour devenir un indice d’identification. Ce lien est souvent exprimé dans le discours même
par le contexte gauche/droite de l’entité ou par la question posée dans le cadre de l’entretien ce qui est le cas du corpus
étudié. Ainsi, le nom de lieu tout seul ne dit rien sur la personne mais employé avec la précision je travaille à …. ou mon
père est originaire de…, il devient un indice, une information personnelle. Il le devient aussi dans les réponses à des
questions portant sur l’identité de la personne comme où travaille votre femme ? vous êtes originaire d’où ?. Les mêmes
observations peuvent se faire pour d’autres types d’entités nommées : les dates ou encore les noms d’organisations.

De manière concomitante, il y a dans le discours d’autres éléments qui ne font pas partie des entités nommées mais qui
peuvent renvoyer vers l’identité de la personne. Ce phénomène a été déjà mentionné dans (Amblard, Fort, 2014) où les
auteurs présentent entre autres le processus d’anonymisation automatique du discours transcrit de schizophrènes. Ils
notent l’insuffisance du simple repérage à l’aide de scripts Python des mots commençant par une majuscule dans les
extrais du corpus où des sujets relatent un événement « s’inscrivant dans une temporalité et une géographie particulière »
et la présence d’autres indices selon lesquelles on peut identifier le locuteur ou ses proches. Cette affirmation se
manifeste à travers les chiffres provenant des résultats de l’expérience de l’annotation automatique du sous-corpus
ESLO1 en indices permettant l’identification éventuelle du locuteur (section 4). Dans 112 fichiers de transcription
d’ESLO1 annotés en entités nommées et en indices, candidats à l’anonymisation, on retrouve 13 909 entités nommées
au total et seulement 1 038 autres indices. Ces chiffres confirment que, d’une part, toutes les entités nommées ne
7
http://medina.limsi.fr/

ESHKOL-TARAVELLA I., BAUDE O., KANAAN-CAILLOL L. ET MAUREL D.
renvoient pas vers le locuteur et que, d’autre part, il existe dans le corpus d’autres éléments qui peuvent permettre
l’identification éventuelle de la personne.
2.2      Autres indices

Si l’on veut anonymiser efficacement le discours, on ne peut pas s’arrêter aux entités nommées car d’autres indices
peuvent renvoyer vers le locuteur ou vers la personne dont il parle. Observons les exemples tirés du corpus ESLO1 :

 j’ai une maladie du foie ça m’a même occasionné une petite scoliose déformation légère de la colonne
vertébrale.
 mon père a fondé un le plus grand cabinet d'ophtalmologiste de la ville 8
 je suis scout de France le jeudi soir où j'anime un un atelier photos 9

Cette catégorie des indices est large. Elle inclut des éléments assez hétérogènes désignant les différentes informations
personnelles sur la personne : événements, activités sociales, loisirs, maladies, handicap, etc. qui peuvent au même titre
que le travail, la famille donner les informations sur le locuteur ou la personne dont on parle.

Ainsi, le « faisceau d’indices » inclut les entités nommées identifiantes, mais peut contenir aussi d’autres éléments qui
permettent l’identification soit directement, soit, par combinaison au sein de ce faisceau : la personne est patron d’un bar
au moment d’enregistrement, et avant elle travaillait dans l’aviation militaire. Le processus d’identification est
progressif, il se construit au fur et à mesure de l’accroissement des indices. On peut supposer qu’un indice identifiant ou
une série de ces indices est associée à un individu particulier dans la mémoire à l’aide d’un certain lien dénominatif qui
sera réactivé lors de leur apparition dans le discours. C’est grâce aux facteurs contextuels, c’est-à-dire grâce aux
connaissances que l’utilisateur du corpus maîtrise concernant le locuteur ou la personne mentionnée dans le discours de
celui-ci, que l’identification peut se faire.

Les parties qui suivent sont consacrées à la description de l’anonymisation du corpus ESLO. Le processus
d’anonymisation du corpus consiste à repérer un faisceau d’indices qui permet d’identifier le sujet parlant ou toute autre
personne mentionnée dans le discours. Dans le processus actuel de l’anonymisation des transcriptions d’ESLO (section
3), ces indices sont repérés manuellement par les transcripteurs. Pour aider ce processus, une expérimentation de
l’automatisation de ce processus a été tentée (section 4). Nous finirons par quelques perspectives liées à l’intégration du
module automatique développé dans le processus actuel (section 5).
3      Procédure semi-automatique d’anonymisation des transcriptions dans le
corpus des ESLO
Nous allons voir dans cette partie la procédure suivie par les gestionnaires du corpus des ESLO afin de procéder à
l’anonymisation des données du corpus.

Du point de vue juridique, le corpus ESLO1, a posé deux problèmes (Baude10). Premièrement, les locuteurs n’ont rempli
aucun document pour exprimer leur consentement ; deuxièmement, les locuteurs de la fin des années soixante ne
pouvaient pas prévoir que leurs enregistrements pourraient être diffusés par Internet qui n’existaient pas à l’époque.
Dans le cas d’ESLO2, les locuteurs signent un document de consentement à la diffusion de l’ensemble des données
brutes. Le choix de l’équipe a néanmoins été d’anonymiser l’ensemble des données d’ESLO1 et d’ESLO2.

L’anonymisation actuelle dans ESLO est semi-automatique et porte sur deux types d’objets : les données (sons et
transcriptions) et les métadonnées. Dans la chaîne de traitement du corpus, la phase d’anonymisation est fractionnée ;
8
L’emploi des déterminants un le dans cet énoncé fait partie des disfluences de l’oral (autocorrection) et est transcrite
comme telle dans les fichiers de transcription.

9
idem.

10
http://eslo.huma-num.fr/index.php/pagemethodologie?id=69

22ème Traitement Automatique des Langues Naturelles, Caen, 2015
elle précède la phase de transcription, coïncide avec elle et lui succède. Nous nous contentons, dans cet article, de
décrire la phase de d’anonymisation des transcriptions11.

Le codage des noms propres des locuteurs est l’action la plus classique et attendue dans une procédure d’anonymisation.
Les transcriptions comportent des informations issues des métadonnées, à savoir l’identifiant du locuteur. Dans la
procédure ESLO, des codes aléatoires sont générés par l’application suite à la création d’une fiche en saisissant les
métadonnées du locuteur (ex : DC738). Ces codes sont repris dans les transcriptions. Le traitement des données
identifiantes contenues dans les énoncés est effectué au niveau-même de la transcription. Il est demandé aux
transcripteurs de remplacer par l’hyperonyme NPERS les noms de personnes (Figure 1) et par NANON12 les autres
segments du discours permettant d’identifier un locuteur.
Figure 1 : Anonymisation dans la transcription
L’anonymisation manuelle des fichiers de transcription a soulevé la question d’automatisation de ce processus grâce aux
outils du TAL. L’expérience a été menée afin de repérer automatiquement les indices permettant l’identification
éventuelle du locuteur ou de toute autre personne mentionnée dans les transcriptions.
4     Expérience de l’anonymisation automatique sur un sous-corpus d’ESLO1
L’expérimentation décrite dans cette partie a été effectuée en collaboration avec le laboratoire LI (Laboratoire
Informatique) de l’université de Tours. Le test portait sur un sous-corpus d’ESLO1 (112 entretiens face-à-face)
contenant de nombreuses données personnelles sur le locuteur car il s’agissait d’un questionnaire concernant la vie des
témoins : « Depuis combien de temps habitez-vous Orléans ? » « Quel âge avez-vous ? » « Qu’est-ce que vous faites
comme métier ? » « Où travaillez-vous ? » « Qu’est-ce que fait votre époux(se) ? », etc.
4.1    Repérage automatique

Lorsqu’on parle de l’anonymisation automatique tout le monde s’accorde sur la nécessité de repérer les entités
nommées. Comme nous l’avons évoqué, ces éléments font très souvent partie, à juste titre, des indices recherchés. C’est
la raison pour laquelle, pour faire une expérimentation d’anonymisation automatique des transcriptions d’ESLO1, il a
été décidé de partir de l’outil permettant d’identifier les entités nommées. La collaboration avec le LI de Tours a permis
d’exploiter le système CasSys développé dans le cadre de la thèse par Nathalie Friburger (Friburger, 2002) et intégré à
la plate-forme Unitex (Paumier, 2003). Il s’agit d’une approche symbolique en surface permettant de construire les
grammaires locales selon le contexte sous forme des cascades de transducteurs qui repèrent et annotent les entités
nommées dans le discours médiatique.

Le système CasSys a été adapté au corpus traité. Tout d’abord, le corpus a été segmenté en tours de parole en fonction
des balises Transcriber13. Les cascades de CasSys ont été ensuite enrichies de nouvelles grammaires locales avec des
dictionnaires et des graphes spécifiques pour reconnaître dans les transcriptions de l’oral en plus des entités nommées
11
Pour une présentation de la procédure : Baude et Dugua Guide d’Anonymisation (en ligne http://eslo.huma-
num.fr/index.php/pagemethodologie?id=69)

12
Nom anonymisé

13
Méthode recommandée par (Dister, 2007)

ESHKOL-TARAVELLA I., BAUDE O., KANAAN-CAILLOL L. ET MAUREL D.
d’autres indices. Enfin, en tenant compte de la nature du corpus, les différentes disfluences de l’oral ont été prises aussi
en compte comme par exemple dans je m’appelle euh Patrick Mallon14.

Nous avons procédé en deux étapes. Tout d’abord, nous lançons des cascades de transducteurs qui repèrent et annotent
les entités nommées (EN). Ensuite, une autre série de cascades appliquée à ce corpus annoté, identifie les indices-
candidats à l’anonymisation (DE15). Dans cet exemple, l’entité nommée Pithiviers a été reconnue au cours de la
première étape, cette entité devient identifiante au cours de la deuxième étape car elle se trouve dans le contexte
indiquant son lien avec le locuteur moi je suis native de Pithiviers :

 1ère étape : <EN type="loc.admi">Pithiviers</EN>
 2ème étape : <DE type="pers.speaker">moi je                     suis    <DE     type="identity.origin">native        de   <EN
type="loc.admi">Pithiviers</EN></DE></DE>

La Figure 1 présente le graphe permettant la reconnaissance d'une origine géographique : ce graphe appelle un sous-
graphe (NELoc) qui reconnaît un toponyme identifié par la cascade des entités nommées.

Suite à l’analyse manuelle du corpus et à partir de la typologie de la campagne d’évaluation Ester2 (campagne
d’évaluation des systèmes de transcription enrichie d’émissions radiophoniques)16 nous avons élaboré le jeu d’étiquettes
pour annoter des indices. L'enquête correspond essentiellement à des questions concernant la personne interrogée et sa
famille : origine, âge, naissance, arrivée à Orléans, travail et même syndicat. Pour cela nous avons défini une typologie
avec trois types principaux, personne, identité et travail, eux même divisés en sous-types, comme présenté dans la Figure
3. Le sujet sur qui porte l’information est annoté en premier lieu. Nous distinguons entre le locuteur (pers.speaker) et les
autres membres de sa famille (pers.spouse, pers.parent, pers.child). Nous précisons ensuite la nature de cette
information : l’identité, le travail, les études, l’engagement associatif ou syndicale, les vacances :

 il est parti à Paris =>
<DE           type="pers.child">il       est       parti    <DE     type="work.location">à        <ENT
type="loc.admi">Paris</ENT></DE> il travaille dans les <Sync time="1526.195"/> <DE
type="work.field">dans les assurances</DE></DE>
 alors je suis monsieur Gabrion je suis ingénieur chimiste=>
alors <DE type="pers.speaker"><DE type="identity.name">je suis <ENT type="pers.hum">monsieur
Gabrion</ENT></DE></DE> <DE type="pers.speaker">je suis <DE type="work.occupation">ingénieur
chimiste</DE></DE>
 je peux vous demander quel est votre syndicat ? </Turn> <Turn speaker="spk5" startTime="5071.106"
endTime="5072.466"> <Sync time="5071.106"/> <Sync time="5071.22"/> oui c'est la <DE
type="pers.speaker"> <DE type="involvement.tradeunion"> <ENT type="org"> CGT </ENT></DE></DE>
 de ce fait <DE type="pers.speaker">nous sommes allés euh <ENT type="time.date.rel"> trois jours </ENT>
<DE type="trip.work"> à <ENT type="loc.admi"> Londres </ENT> </DE> <ENT type="time.date.rel">
trois jours </ENT> <DE type="trip.work"> à <ENT type="loc.admi"> Vienne </ENT> </DE> nous avons
été <ENT type="time.date.rel"> trois jours </ENT> <DE type="trip.work"> en <ENT type="loc.admi">
Hongrie </ENT></DE>

On voit dans ces exemples, que l’entité nommée monsieur Gabrion est bien annotée en tant qu’indice car elle se trouve
dans un contexte qui concerne le locuteur je suis monsieur Gabrion. C’est le cas pour une autre entité nommé, le nom du
syndicat CGT, car elle se trouve dans la réponse à la question concernant le locuteur. Les cascades annotent également
les syntagmes fondés sur les noms communs comme le métier ingénieur chimiste ou le domaine d'activités
professionnelles dans les assurances. A cela s’ajoute l’annotation d’autres indices comme les vacances nous sommes
allés trois jours à Londres ou des autres actions il est parti de Paris.

La reconnaissance des indices est fondée sur le contexte qui joue un rôle primordial dans le processus d’identification
car il permet de réduire le champ d’application de ces éléments à un seul individu, de le distinguer des autres référents
possibles. En premier lieu, on peut mentionner le contexte immédiat (gauche et/ou droite) d’indice. Le nom de lieu
14
L’annotation automatique des entités nommées et dénommantes a été décrite dans (Maurel et al., 2011, Eshkol et al.,
2012).
15
Le terme que nous avons utilisé à l’époque de cette expérimentation pour désigner les indices annotés est celui d’« entité
dénommante » (Eshkol, 2010). Le travail complémentaire entrepris depuis sur la définition de cette notion nous amène
maintenant à préférer la terminologie de « faisceau d’indices » telle que nous l’avons présentée dans la section 2.

16
http://www.afcp-parole.org/ester/docs/Conventions_EN_ESTER2_v01.pdf

22ème Traitement Automatique des Langues Naturelles, Caen, 2015
n’aura pas grand intérêt employé seul, mais employé avec des verbes comme venir de, travailler à ou avec des noms
comme collège, hôpital, etc. il devient identifiant du lieu de travail, d’études ou d’origine de la personne. L’indice
repéré doit être étiqueté aussi selon le contexte. Dans la phrase je travaille au collège de Saint-Jean-de-Braye, l’entité
nommée collège de Saint-Jean-de-Braye ne réfère plus seulement à un établissement scolaire en général, c’est une
référence à un lieu de travail du locuteur. Ce contexte peut être aussi défini par la question posée. On sort ce faisant des
limites de l’énoncé pour étudier un contexte plus large. Le nom de lieu, par exemple, n’est pas signifiant s’il est utilisé
pour répondre à la question : où parle-t-on le mieux le français ?, par contre il devient un indice dans les réponses aux
questions concernant les origines du locuteur, ou dans les énoncés décrivant l’emploi du locuteur, pour autant que celui-
ci indique le lieu de son travail. De la même manière, les réponses aux questions sur les émissions de télévision, par
exemple, n’apportent pas d’information personnelle et les noms de personnes qui apparaissent n’ont pas à être pris en
compte. Les questions posées peuvent donc jouer un rôle important dans la catégorisation adéquate d’un indice repéré.
Enfin, il est nécessaire de prendre en compte le contexte socioculturel de l’époque. Ainsi, les destinations de vacances
peuvent être prises en compte car en 1968 peu de gens à Orléans voyageaient à l’étranger, c’est le cas du dernier
exemple ci-dessus.

L’annotation a été réalisée sur 112 fichiers Transcriber (35,75 Mo). L’évaluation des résultats a été effectuée sur 9
fichiers (6 fichiers ont été réservés pour les tests). Les indices ont été reconnus avec la précision estimée à 94,2 % et le
rappel de 84,4 % (Maurel et al., 2009).
Figure 2 : Un graphe pour l'origine géographique
4.2     Difficultés rencontrées

Malgré ce succès, plusieurs difficultés ont été mises en évidence.

En premier lieu, la présence de multiples disfluences (hésitations, répétitions, reformulations, amorces, etc.) qui peuvent
intervenir à différents moments dans le discours comme dans je m’appelle euh Patrick Mallon rendent la tâche difficile.
Le graphe contenant la liste de disfluents possibles a été créé ce qui a permis de résoudre ce problème dans beaucoup de
cas.

Ensuite, dans le discours oral, les informations apparaissent d’une manière parfois aléatoire. Ainsi, des informations sur
le témoin ne se trouvent pas nécessairement dans la partie questionnaire, mais peuvent surgir à des endroits inattendus.
Par exemple, la description de la recette de l'omelette peut être l'occasion de glisser son origine géographique :

 enfin on assaisonne sel poivre euh <DE type="pers.speaker"> nous en <DE type="identity.origin"> <ENT
type="loc.admi"> Lorraine </ENT></DE></DE> on on découpe des petits des petits morceaux de lards qu'on
fait frire avant

ESHKOL-TARAVELLA I., BAUDE O., KANAAN-CAILLOL L. ET MAUREL D.
Certaines informations doivent être aussi parfois déduites du contexte comme dans l’exemple suivant:
BV: y a longtemps que vous êtes à Orléans ?
MS530:euh oui euh vingt-deux ans
BV: ça fait euh vous êtes née à Orléans
MS530: oui

Une autre difficulté provient de la variation linguistique. Les informations de nature personnelle varient d’une manière
non homogène dans le corpus. Chaque type d’information peut être présenté à travers un groupe nominal ainsi qu’avec
des expressions plus étendues. Ainsi, le locuteur peut décrire son métier de manières diverses :

 je suis enseignant dans l'école publique
 je suis maître auxiliaire
 j’enseigne des mathématiques modernes des mathématiques classiques de la chimie et de la technologie
la personne interrogée (+speaker)

son conjoint (+spouse)
Personne (+pers)
ses enfants (+child)

les autres membres de la famille (+parent)

le nom (+name)

l’adresse (+addr)

l’âge (+age)

le mariage (+wedding)
Identité (+identity)
l’origine (+origin)

la naissance (+birth)

l’arrivée à Orléans (+arrival)

le nombre d’enfants (+children)

métiers (+occupation)

secteur d’activité (+field)
Travail (+work)
lieu de travail (+location)

entreprise (+business)

association (+voluntary)
Engagement               militaire (+military)
(+involvement)
scolaire (+school)

syndical (+tradeunion)

études (+study)
Voyage (+trip)           vacances (+holiday)

professionnel (+work)

lieu (+location)
Etudes (+study)          diplôme (+degree)

établissement (+edu)

Figure 3 : Typologie des indices
On ne peut jamais atteindre une liste exhaustive de toutes les reformulations possibles.

22ème Traitement Automatique des Langues Naturelles, Caen, 2015
Enfin, le corpus peut comprendre des informations difficiles à catégoriser comme par exemple :

 mon père a fondé un le plus grand cabinet d'ophtalmologiste de la ville
 je suis scout de France le jeudi soir où j'anime un un atelier photos

Cette catégorie du faisceau d’indices comprenant les actions, les événements, les activités sociales du locuteur semble
« imprévisibles » en raison de son manque d’homogénéité.

Malgré toutes ces difficultés, les indices peuvent être reconnus automatiquement avec une bonne précision et un bon
rappel. La première catégorie, les entités nommées identifiantes, est bien reconnue par le module développé. La
deuxième indiquant les actions, événements, activités sociales du locuteur est identifiée mais pas d’une manière
exhaustive.

Cependant la multitude d’éléments personnels annotée dans le corpus soulève une autre question concernant leur
pertinence. Tous les éléments annotés ne nécessitent pas d’être anonymisés. Actuellement, la décision d’anonymiser un
tel ou tel indice, ne peut se faire que manuellement par un humain. C’est seulement l’humain qui peut décider
aujourd’hui, souvent d’une manière assez subjective, lequel des éléments personnels renvoie le plus vers le locuteur ou
ses proches et doit donc être masqué. Le principe respecté est de garder le maximum d’informations pour pouvoir
permettre l’analyse du corpus. Ainsi, dans les 112 fichiers contenant 1 038 indices annotés, seulement 168 ont été
remplacés par leur hyperonyme (159 NPERS et 9 NANOM17).
5    Conclusion et perspectives

Le travail effectué a montré que si l’on veut anonymiser un corpus d’enquêtes sociolinguistiques, il ne suffit pas de
reconnaître les noms propres et les autres entités nommées car d’une part, d’autres éléments peuvent aussi permettre
l’identification du locuteur ou de la personne mentionnée dans le discours notamment quand il existe une combinaison
de ces éléments au sein du corpus et, d’autre part, tous les entités nommées ne sont pas sensibles à l’anonymisation et
ont besoin d’un contexte pour devenir identifiantes.

Le module développé pour le repérage automatique des indices-candidats à l’identification potentielle de la personne
tient compte des spécificités de l’oral (la présence de disfluences, l’absence des signes de ponctuation dans les
transcriptions, la segmentation en tours de parole) et permet d’obtenir des résultats encourageants.

La difficulté majeure de l’anonymisation automatique des discours transcrits de l’oral est que toutes les informations
personnelles n’identifient pas la personne mais qu’en revanche une combinaison de certaines d’entre elles constituent un
faisceau qui dans un certain contexte, le plus souvent extralinguistique, contribuent à l’identification. Actuellement la
décision sur la pertinence de masquer certains éléments du faisceau ne peut se faire que par une intervention humaine.
Pour aider cette validation manuelle, la distinction pourrait se faire entre les éléments les plus sensibles à
l’anonymisation, c’est-à-dire ceux qui apportent une information plus importante et plus spécifique, et ceux qui sont plus
généraux. Ainsi, pour distinguer entre les noms de famille rares comme Eshkol ou Kanaan et très répandues Dupond ou
Durand, on pourrait s’appuyer, dans le cas du corpus des ESLO, sur une information concernant la fréquence d’un nom
propre, éventuellement pondérée par des critères géographiques. De la même manière, le locuteur peut désigner son
métier par un seul mot enseignant ou en précisant professeur de physique. Ce passage d’un seul nom à un groupe
nominal plus étendu grâce aux modifieurs « se manifeste par l’ajout de propriétés supplémentaires à la classe présentée
par le groupe nominal minimal, ce qui diminue l’extension de la classe et rapproche le groupe d’une référence plus
individualisante » (Eshkol, 2010 : 258). Ce processus concerne n’importe quelle caractéristique (maladie, loisir, etc.).
On pourrait ainsi attribuer plus de poids à ces éléments sensibles à l’anonymisation ce qui diminuerait le nombre des
indices candidats à l’anonymisation et de cette manière aiderait la validation manuelle.

Dans le faisceau d’indices, la deuxième catégorie comprenant les éléments ne faisant pas partie des entités nommées
comme actions, événements, activités sociales, doit être approfondie d’autant plus qu’elle permet d’apporter une
information sur le profil sociologique du locuteur. Le module développé tient compte de ces indices mais leur liste n’est
pas exhaustive. Pour un travail futur, nous envisageons d’étudier avec précision dans le corpus ESLO, tous les éléments
anonymisés par une procédure manuelle afin d’affiner la typologie du faisceau d’indices.
17
L’étiquette NANOM signifie le nom anonymisé.

ESHKOL-TARAVELLA I., BAUDE O., KANAAN-CAILLOL L. ET MAUREL D.
Références
AMBLARD M., FORT K. (2014). Étude quantitative des disfluences dans le discours de schizophrènes : automatiser pour
limiter les biais. Actes de TALN2014, Marseille, France.

BAUDE O. (2006). Corpus oraux : guide des bonnes pratiques. CNRS-Editions et Presses universitaires d’Orléans,
2006.

BAUDE O., DUGUA C. (2011). (Re)faire le corpus d’Orléans quarante ans après : quoi de neuf, linguiste ?, vol. 10,
Corpus, Varia.

CHARAUDEAU P., MAINGUENEAU D. (2002). Dictionnaire d'analyse du discours. Paris, Éditions du Seuil.

CHARHAD M., QUENOT G. (2005). Approche par patrons linguistiques pour la détection automatique du locuteur :
application à l'indexation par le contenu des journaux télévisés. Compression et Représentation des Signaux
Audiovisuels (CORESA'05), Rennes.

DUBOIS J. (1973). Dictionnaire de linguistique. Paris, Larousse.

EHRMANN M. (2008). Les entités nommées, de la linguistique au TAL : statut théorique et méthodes de
désambiguïsation. Thèse de doctorat, Université Paris 7 - Centre de recherche Xerox, Grenoble (XRCE).

ESHKOL I. (2010a). Entrer dans l’anonymat. Etude des « entités dénommantes » dans un corpus oral. Eigennamen in der
gesprochenen Sprache, 245-266.

ESHKOL I., MAUREL D., FRIBURGER N. (2010b). Eslo: from transcription to speakers' personal information
annotation. Actes de Seventh Language Resources and Evaluation Conference (LREC 2010), Malte.

ESHKOL-TARAVELLA I., BAUDE O., MAUREL D., HRIBA L., DUGUA C., TELLIER I., (2012). Un grand corpus oral
« disponible » : le corpus d’Orléans 1968-2012. Ressources linguistiques libres, TAL. 52 : 3, 17-46.

FRIBURGER N. (2002). Reconnaissance automatique des noms propres ; application à la classification automatique de
textes journalistiques. Thèse de doctorat d'informatique, Université François Rabelais Tours.

GROUIN C, ZWEIGENBAUM P. (2011). Une approche à plusieurs étapes pour anonymiser des documents médicaux. RSTI-
RIA, 25 :4, 525-549.

HAMON P. (1977). Pour un statut sémiologique du personnage. Poétique du récit. Barthes R. et alii, Points-Seuil, Paris.

MAUREL D., FRIBURGER N., ESHKOL I. (2009). Who are you, you who speak? Transducer cascades for information
retrieval. Actes de 4th Language & Technology Conference: Human Language Technologies as a Challenge for
Computer Science and Linguistics. Poznań, Poland, 220-223.

MAUREL D., FRIBURGER N., ANTOINE J.-Y., ESHKOL-TARAVELLA I., NOUVEL D., (2011). Cascades de transducteurs
autour de la reconnaissance des entités nommées. Varia TAL, 52 :1, 69-96.

MEYSTRE S., FRIEDLIN B S., SHUYING S., SAMORE M. (2010). Automatic de-identification of textual documents in the
electronic health record: a review of recent research. BMC Medical Research Methodology 10.70.

NADEAU N., SEKINE S. (2009). A survey of named entity recognition and classification, Satoshi Sekine and Elisabete
Ranchhod, ed., John Benjamins publishing company, 3-28.

PAUMIER S. (2003). De la Reconnaissance de Formes Linguistiques à l'Analyse Syntaxique. Thèse de Doctorat en
Informatique, Université de Marne-la-Vallée.

RAAJ N. (2012). Automated Tool for Anonymization of Patient Records. Report. MSc Computing and Management,
Imperiel College, London18.

18
http://www.comp.leeds.ac.uk/mscproj/reports/1112/raaj.pdf

22ème Traitement Automatique des Langues Naturelles, Caen, 2015
ROSSET S., GROUIN C., ZWEIGENBAUM P. (2011). Entitées nommées structurées : guide d’annotation Quaero. Notes et
documents LIMSI N˚2011-04.

TRAN M., MAUREL D. (2006). Prolexbase : Un dictionnaire relationnel multilingue de noms propres. TAL, 47 : 3, 115-
139.

TVEIT A., EDSBERG O., BROX RØST T., FAXVAAG A., NYTRØ Ø., NORDGÅRD T., THORSEN RANANG T., GRIMSMO A.,
(2004). Anonymization of General Practitioner Medical Records. Second HelsIT Conference at the Healthcare
Informatics, Trondheim.

UZUNER O., LUO Y., SZOLOVITS P., (2007). Evaluating the state-of-the-art in automatic de-identification. J Am Med
Inform Assoc 14, 550-63.
22ème Traitement Automatique des Langues Naturelles, Caen, 2015
Annotateurs volontaires investis et éthique de l’annotation de lettres de suicidés

K. Bretonnel Cohen1 John P. Pestian2 Karën Fort3
(1) University of Colorado, Denver
(2) Cincinnati Children’s Hospital Medical Center, University of Cincinnati, Cincinnati
(3) Université Paris-Sorbonne / STIH
kevin.cohen@gmail.com, john.pestian@cchmc.org, karen.fort@paris-sorbonne.fr

Résumé. Cet article présente une perspective éthique sur le projet décrit dans (Pestian et al., 2012b). La campagne
d’annotation en question a visé à produire un corpus de lettres de suicidés annotées en émotions. Les annotateurs étaient
soit des parents ou des amis de suicidés, soit des professionnels de la santé mentale. Nous appelons ces annotateurs
bénévoles, volontaires pour faire avancer la recherche, des volontaires investis. Ce projet soulève un certain nombre de
questions éthiques, notamment en ce qui concerne le rôle de l’empathie des annotateurs, les effets possibles sur ceux-ci et
les utilisations potentielles des résultats obtenus. Nous concluons par une analyse du corpus du point de vue de la Charte
Éthique et Big Data.
Abstract.
Annotating suicide notes : ethical issues at a glance.
According to the World Health Organization, 800,000 people die of suicide every year. About 20% of them leave a writ-
ten message. This paper discusses a corpus of such messages. The corpus was annotated with reference to the emotions
expressed in the notes. The annotators were family or friends of someone who had died by suicide, or mental health
professionals. We refer to these non-coercivally and altruistically motivated annotators as vested volunteers. A number of
ethical issues are explored with this task and group of annotators, including the role of empathy, possible effects on the
annotators, and the uses that might be made of the products of the annotation project. We conclude considering the project
from the point of view of the Ethics and Big Data Charter.
Mots-clés :         lettres de suicidés, éthique, annotation, myriadisation, corpus.

Keywords:           suicide notes, ethics, annotation, crowdsourcing, corpus.
1     Introduction
Selon l’Organisation Mondiale de la Santé (OMS), chaque année dans le monde, plus de 800 000 personnes se donnent
la mort 1 . Environ 20 % d’entre elles laissent un message écrit (on peut citer par exemple, (Volant, 1990) 664 sur 3 450
et (Fédération française de psychiatrie, 2001), 200 sur 621). On parle en anglais de suicide note. Nous utiliserons ici le
terme de lettre, reprenant à notre compte la remarque à ce sujet d’Éric Volant (Volant, 1990) :
Elles sont bien davantage qu’un simple mot qu’on laisse traîner [...]. Même s’il ne s’agit que d’un grif-
fonnage, il a un but défini pour son auteur, et mérite donc pleinement le statut de lettre.
Un certain nombre de chercheurs ont étudié différents aspects de ces lettres, dans le but d’aider à la prévention du suicide.
L’analyse que nous présentons ici concerne un projet de recherche de ce type, mené aux États-Unis, qui a donné lieu à
la constitution et à l’annotation en émotions d’un corpus de lettres de suicidés. Ce projet est détaillé dans (Pestian et al.,
2012b).
Les lettres de ce corpus ont été annotées par des volontaires qui ont eu une expérience concrète du suicide, soit parce qu’un
membre de leur famille ou un de leurs amis s’est suicidé, soit parce qu’ils sont des professionnels de la santé mentale.
Ceux-ci ont participé au projet bénévolement, sans contrainte, pour faire avancer les travaux des scientifiques dans ce do-
maine. Pour rendre compte à la fois de cette volonté et de leur rapport personnel au suicide, nous les appelons « volontaires
1. Voir http://www.who.int/mediacentre/news/releases/2014/suicide-prevention-report/fr/.

K. B RETONNEL C OHEN , J OHN P. P ESTIAN , K ARËN F ORT
investis » (vested volunteers).
Ce type de projet pose un certain nombre de questions éthiques, que l’appel à des volontaires investis rend encore plus
saillantes. Il en va ainsi du rôle de l’empathie de l’annotateur, des effets potentiels sur celui-ci, des sources de biais et des
utilisations possibles des résultats de l’étude.
1.1     Une foule (limitée) de volontaires

Le terme anglais crowdsourcing est défini par le dictionnaire en ligne Merriam-Webster comme « la pratique consistant
à obtenir le service désiré, qu’il s’agisse d’une idée ou de contenu, en sollicitant les contributions d’un grand groupe de
personnes, en particulier de la communauté des internautes plutôt que des employés ou fournisseurs traditionnels 2 ». Ce
mot valise très expressif, formé de foule (crowd) et de délocalisation (outsourcing), se délave à la traduction. Nous le
traduirons ici par myriadisation 3 .
Ces dernières années, la myriadisation est devenue une méthode populaire de construction de ressources langagières, en
particulier dans le cadre de la recherche en Traitement Automatique des Langues (TAL). Cet essor n’a pas été sans poser
des questions éthiques. Ainsi, (Fort et al., 2011) présente une critique détaillée de l’utilisation d’Amazon Mechanical
Turk, une plate-forme de travail parcellisé qui permet l’exploitation de travailleurs de pays en développement. Les
conditions de travail des Turkers ont d’ailleurs fait l’objet d’études sur le terrain qui confirment cette exploitation (Gupta
et al., 2014).
D’autres méthodes de myriadisation, plus satisfaisantes éthiquement, se sont développées en parallèle. Parmi celles-ci, les
jeux ayant un but (Games With A Purpose) proposent de produire des ressources en jouant. Le premier de ces jeux pour les
ressources langagières est à notre connaissance JeuxDeMots (Lafourcade & Joubert, 2008), qui crée un réseau lexical
en français. Phrase Detectives (Chamberlain et al., 2008) a quant à lui été utilisé pour annoter des anaphores dans
un corpus en anglais. Plus récemment, ZombiLingo (Fort et al., 2014) permet l’annotation en syntaxe de dépendances
d’un corpus en français.
Le point commun entre JeuxDeMots, ZombiLingo et d’autres formes de myriadisation comme Wikipédia ou le
Projet Gutenberg est le bénévolat. Les participants savent qu’ils ne seront pas rémunérés 4 . Ils sont motivés par
l’aspect ludique du jeu ou par l’envie de participer à un projet de bien commun ou de recherche. Dans ce dernier cas, on
parle alors de sciences participatives. Un exemple de projet de science participative sans aspect particulièrement ludique
est Vigie Nature développé au Muséum National d’Histoire Naturelle (Couvet et al., 2011).
À la différence de ces projets, la campagne d’annotation en discussion ici n’a rien de plaisant. La motivation des partici-
pants est donc simplement d’aider à faire avancer la recherche sur le sujet du suicide. Une autre différence est que l’appel
à participation a été limité à certaines communautés (voir section 2.2) et n’a pas été totalement ouvert.
1.2     Une annotation discriminante

Les lettres de suicidés ont fait l’objet d’un nombre de recherches considérable (Shneidman & Farberow, 1957b; Osgood
& Walker, 1959; Tuckman et al., 1959; Schneidman, 1981; Leenaars, 1988; Baume et al., 1997; Brevard et al., 1990; Ho
et al., 1998; O’Connor & Leenaars, 2004; Olson, 2005).
Le domaine tel que nous le connaissons aujourd’hui a vu le jour lorsque Edwin Shneidman a découvert un gisement de
lettres de suicidés dans les bureaux du Coroner du comté de Los Angeles. Sa découverte a été complétée par la création
d’un corpus témoin : il a demandé (pour des raisons pratiques) à des travailleurs syndiqués d’écrire la lettre qu’ils écriraient
s’ils voulaient se suicider (Shneidman & Farberow, 1957a). La notion de contrôle en suicidologie est complexe (Pestian,
2010), les participants « témoins » ont cependant été sélectionnés pour correspondre aux suicidés en termes d’âge, de
genre, de religion et de nationalité (tous américains).
Ces données ont servi dans de nombreuses expériences qui ont montré qu’« il est possible de distinguer entre de vraies
lettres de suicidés et des simulacres, et, encore plus important, que les lettres de suicidés se caractérisent généralement par
une logique dichotomique, davantage d’hostilité et d’auto-critique, de noms spécifiques et d’instructions aux survivants,
2. the practice of obtaining needed services, ideas, or content by soliciting contributions from a large group of people and especially from the online
community rather than from traditional employees or suppliers (Merriam-Webster, consulte en avril 2015)
3. Cette traduction a été proposée par Gilles Adda dans (Sagot et al., 2011).
4. Le cas de Phrase Detectives est un peu différent, car les joueurs peuvent gagner des bons d’achat.

22ème A NNOTATION DE LETTRES DE SUICIDÉS
Moy. des lettres (écart type)   Moy. des lettres simulées (écart type)   Signif.
Nb phrases                                                    9,242 (8.058)                            4,848 (3,308)     0,001
Fréquence max. d’un mot                                       7,909 (6,090)                            4,212 (3,525)     0,002
Nb caractères hors mot                                    13,182 (11,078)                              7,212 (7,039)     0,004
Niveau de lisibilité Flesch-Kinkaid                           4,719 (2,142)                            6,517 (2,994)     0,008

TABLE 1 – Quelques spécificités des lettres de suicidés par rapport à des lettres simulées.
moins de signes de réflexion sur ce que l’autre pense, et plus d’usage des différents sens du mot "amour" » (Shneidman,
1973).
Nous présentons ci-dessous deux lettres d’adieux au monde 5 écrites par des hommes américains :
Please call my children first Can’t Take much more Love you all to much gone to See
Mary I can’t live without Mary this pain + misery is to much Love all you kids forgive
me ain’t going to Die in Hosp. Like Mom Love Dad
(Leenaars, 1988)

I’m sorry, but somewhere I lost the road, and in my struggle to find it again,
I just got further and further away. There should be little sadness, and no searching
for who is at fault; for the act and the result are not sad, and no one is at fault.
My only sorrow is for my parents who will not easily be able to accept that this is
so much better for me. Please folks, it’s all right, really it is. 1 :30 p.m.-The
ultimate adventure begins! Car to Helen or Ray (needs a tune up). Money to Max and
Sylvia. Furniture to George. I wanted to be too many things, and greatness besides-it
was a hopeless task. I never managed to learn to really love another person-only to
make the sounds of it. I never could believe what my society taught me to believe,
yet I could never manage to quite find the truth. 2 :15 p.m.-:-I am about to will
myself to stop my heart beat and respiration. This is a very mystical experience.
I have no fear. That surprises me. I thought I would be terrified. Soon I will know
what death is like-how many people out there can say that?
(Shneidman, 1973)
La première de ces lettres est relativement typique, puisqu’elle contient des verbes à l’impératif et l’expression d’une
souffrance.
La seconde a été écrite par un psychiatre de 31 ans qui s’est donné la mort en prenant des barbituriques. Il a été trouvé
dans une forêt du mid-Ouest américain. Cette deuxième lettre est moins typique, car elle ne mentionne pas de souffrance
(du moins pas explicitement). Elle contient cependant elle-aussi des formes à l’impératif.
Le tableau 1 présente un certain nombre d’autres caractéristiques des véritables lettres de suicidés par rapport à des lettres
simulées. Nous nous limitons ici aux spécificités les plus significatives (p < 0,01 ou moins).
Il est à noter que les lettres prises en compte dans ces chiffres ont été écrites par des Américains et qu’il faut donc se
garder de généraliser à d’autres cultures, y compris d’autres pays anglo-saxons.
Le corpus de lettres dont il est question ici a été annoté en émotions, afin d’affiner l’analyse de ces lettres et aller au-delà
de la simple classification entre « vraie » et « fausse » lettre de suicidé.
2      Matériels et méthodes

2.1      Création du corpus

Les 1 319 lettres du corpus ont été collectées entre 1950 et 2012 par Edwin Shneidman (UCLA), et John Pestian, du
Centre médical des enfants de l’hôpital de Cincinnati (CCHMC). La mise en place de la base de données a commencé
5. Nous utilisons ici à dessein le terme employé par (Volant, 1990).

K. B RETONNEL C OHEN , J OHN P. P ESTIAN , K ARËN F ORT
en 2009 et a reçu l’aval des institutions (CCHMC’s Institutional Review Board). Chaque lettre a été scannée dans le
module spécialisé (Suicide Note Module) de la plate-forme de décision clinique CHRISTINE. Les lettres ont ensuite été
transcrites par un transcripteur professionnel et chacune a été relue par trois relecteurs en parallèle. Leurs instructions
étaient de corriger les erreurs de transcription mais de laisser les erreurs d’origine, telles que les fautes d’orthographe, de
grammaire, etc.
Afin de préserver la vie privée des suicidés, les lettres ont ensuite été anonymisées. Afin qu’elles restent utilisables par
des systèmes par apprentissage supervisé, les informations retirées ont été remplacées par des valeurs équivalentes qui
protègent la vie privée des individus. Ainsi, tous les prénoms de femmes ont été remplacés par « Jane » et tous les prénoms
d’hommes par « John ». Tous les noms de famille ont quant à eux été remplacés par « Johnson ». Les dates ont été
modifiées, tout en restant dans la même année. Par exemple, le « 18 novembre 2010 » a pu être changé en « 12 mai 2010 ».
Enfin, toutes les adresses sont devenues « 3333 Burnet Ave., Cincinnati, OH 45229 ».
Chaque lettre du corpus d’entraînement et de test a été annotée par au moins trois annotateurs. Il leur était demandé
d’identifier les émotions et expressions suivantes : maltraitance, colère, accusation, peur, culpabilité, désespoir, tristesse,
pardon, joie, paix, espoir, amour, fierté, reconnaissance, instructions et informations. Une interface Web conçue spéciale-
ment a été utilisée pour collecter, gérer et arbitrer l’activité des annotateurs. Cet outil permet des annotations au niveau
du mot et de la phrase. Il permet d’annoter un segment avec plusieurs concepts. Cette fonctionnalité a rendu impossible
l’utilisation d’un simple coefficient Kappa pour calculer l’accord inter-annotateurs. Celui-ci a donc été calculé à l’aide
de l’alpha de Krippendorff (Krippendorff, 1980, 2004), avec une distance de Dice. L’accord moyen mesuré a été de 0,54
(Pestian et al., 2012a).
2.2    Recrutement des annotateurs

Par manque de financement, les annotateurs du projet ont été recrutés par crowdsourcing bénévole. Le choix a cependant
été fait de limiter l’appel à participation à certaines personnes (il s’agit donc de crowdsourcing limité), dont le vécu pouvait
être vecteur de motivation.
Ainsi, approximativement 1 500 membres de plusieurs communautés en ligne ont reçu une information concernant l’étude,
soit directement par courriel, soit indirectement, via des pages Facebook de soutien. Les deux groupes les plus actifs
dans ces communautés ont été les groupes de Karyl Chastain Beal Families and Friends of Suicides (Familles et amis
de suicidés) et Parents of Suicides (Parents de suicidés), ainsi que Suicide Awareness Voices of Education (Voix pour
l’éducation et la sensibilisation au suicide), un groupe dirigé par Dan Reidenberg, un psychiatre.
Le message envoyé aux participants potentiels contenait des informations concernant l’étude, ses sources de financement
et ce qui était attendu des participants. Les volontaires ont été sélectionnés en deux étapes. La première a consisté à vérifier
qu’ils remplissaient les critères fixés : avoir au moins 21 ans (la majorité légale aux États-Unis), être de langue maternelle
anglaise et être prêt à lire et annoter 50 lettres de suicidés.
Dans un second temps, les participants ont reçu un courriel leur demandant de décrire leur relation à la personne suicidée,
le temps passé depuis cette mort et si le ou la suicidé(e) avait été diagnostiqué(e) comme souffrant d’une maladie mentale.
2.3    Formation des annotateurs

Les annotateurs ont été formés par le biais d’une interface Web. Ils ont annoté 10 lettres jusqu’à atteindre une qualité
minimale de 50 % d’accord observé avec la référence. Ils ont ensuite été invités à annoter 50 autres lettres.
Il est important de noter qu’ils ont été informés de la possibilité d’arrêter quand ils le souhaitaient, durant la formation ou
pendant l’annotation. Des possibilités de soutien leur ont été proposées. Un « filet de sécurité » psychologique a été mis en
place à deux niveaux : les organisateurs ont proposé un contact auprès de l’équivalent de SOS Suicide et les communautés
de provenance des annotateurs leur ont fourni un soutien complémentaire.
2.4    Profils des annotateurs

L’annotateur type, dans ce projet, est une femme d’âge moyen, ayant fait des études et dont un membre de la famille
proche s’est suicidé.

22ème A NNOTATION DE LETTRES DE SUICIDÉS
Le tableau 2 détaille les informations démographiques dont nous disposons sur les annotateurs du projet.

Genre                             Homme                          10 %
Femme                          90 %
Age                               Moyenne                       47,3 ans
Écart type                    11,2 ans
Extrêmes                     23–70 ans
Niveau d’études                   Bac                            26 %
Bac+2                          13 %
Bac+3                          23 %
Bac+5                          34 %
Doctorat                        4%
Connexion au suicide              Survivant d’une perte          70 %
Prof. de la santé mentale      18 %
Autre                          12 %
Temps passé depuis le suicide     0-2 ans                        27 %
3-5 ans                        25 %
6-10 ans                       14 %
11-15 ans                      13 %
16+ ans                        12 %
Relation au suicidé               Enfant                         31 %
Frère/soeur                    23 %
Époux ou partenaire            15 %
Parent                          8%
Autre parent                    9%
Ami(e)                          5%
TABLE 2 – Démographie des annotateurs
3     Résultats

3.1    Un corpus annoté unique, disponible pour la recherche

Le corpus est composé de 1 319 lettres, ce qui correspond à un total de 146 739 mots. La longueur moyenne d’une lettre
est de 102,4 mots et l’écart type est de 112,2 mots : certaines lettres sont très courtes, d’autres très longues. À notre
connaissance, il s’agit de loin du plus gros corpus existant de lettres de suicidés écrites en anglais. À titre de comparaison,
(Brevard et al., 1990) présente une étude de 20 lettres de suicidés et 20 lettres liées à une tentative de suicide, soit un
total de 40 lettres. Les auteurs de (Joiner et al., 2002) ont utilisé le même nombre de lettres, alors que ceux de (O’Connor
& Leenaars, 2004) ont eu accès à 30 lettres en provenance d’Irlande et 30 des États-Unis. En ce qui concerne l’anglais,
l’étude la plus large à notre connaissance a concerné 224 lettres de 154 sujets (Ho et al., 1998). En ce qui concerne le
français, 482 lettres en français ont été utilisées pour (Volant, 1990).
3.2    Des systèmes créés, identifiant les émotions

Une shared task a été organisée autour de ce corpus par le centre i2b2 (Informatics for Integrating Biology and the Bedside)
aux États unis en 2011. Les participants ont mis au point des méthodes de TAL pour annoter les émotions exprimées dans
les lettres. Cette shared task a regroupé 24 équipes, soit 106 scientifiques, en provenance d’Europe, d’Asie, et d’Amérique
du nord. Ceux-ci ont présenté leurs résultats lors d’un atelier à la conférence annuelle de l’American Medical Informatics
Association (Pestian et al., 2012b).
Les performances s’étagent de 0,30 à 0,61 de F-mesure. Les actes de cette shared task ont été cités près de 60 fois au mo-
ment où nous écrivons ces lignes (avril 2015). L’impact de ce corpus en termes d’applications, de produits commerciaux,
d’articles de recherche ou de brevets reste cependant inconnu à ce jour.

K. B RETONNEL C OHEN , J OHN P. P ESTIAN , K ARËN F ORT
4     Discussion

4.1    Des biais probables, difficiles à réduire

Nous avons identifié un certain nombre de biais potentiels dans ce projet. Certains viennent des annotateurs, d’autres des
chercheurs.
En ce qui concerne les annotateurs, il est possible que les survivants d’une perte et que les professionnels de la santé
mentale influent sur la tâche du fait de différents biais émotionnels et cognitifs.
Ainsi, les survivants pourraient être inconsciemment motivés à trouver moins de preuves de souffrance et plus de joie
dans les lettres de leurs parents. Les professionnels de la santé mentale pourraient eux être motivés à trouver davantage
ou moins de preuves de l’issue fatale d’une maladie.
Il a été noté dans (Olson, 2005) que les lettres de suicidés peuvent avoir pour but soit de réduire la souffrance des
survivants en allégeant leur culpabilité soit, au contraire, à l’alourdir en les accusant. Il est possible que les survivants et
les professionnels de la santé mentale annotent les lettres d’adieux à la vie de manière différente, du fait d’une tentative
inconsciente des premiers à alléger leur propre souffrance.
Nous n’avons malheureusement pas de réponse à ces questions, mais il est important d’avoir ces questions en tête lorsque
l’on conçoit un tel projet d’annotation et lorsque l’on utilise le produit de ce projet.
Ainsi, dans ce projet, des différences ont été observées entre les différents types d’annotateurs. Une analyse préliminaire
suggère en effet que les volontaires non professionnels identifient un variété moindre d’émotions que les professionnels
de la santé mentale.
“We conjecture that part of this difference is due to psychological phenomenology. That is, each annota-
tor has a psychological perspective that he/she brings to emotionally-charged data and this phenomenology
causes a natural variation. [...] Whether our use of vested volunteers biased the interoperation, we are not
sure. " (Pestian et al., 2012b)
Il est également possible que les chercheurs eux-mêmes aient pu inconsciemment biaiser les résultats. Ainsi, un des
membres du projet est lui-même un parent proche d’une personne qui s’est suicidée. Il est de ce fait difficile d’identifier
clairement les biais introduits dans la conception et l’analyse du projet, mais la question mérite d’être posée.
Quoi qu’il en soit, le travail en équipe participe à limiter ce type d’influence et à protéger le psychisme des uns et des
autres, en leur permettant d’échanger sur leur ressenti et d’identifier ainsi plus facilement les biais qui en découlent.
4.2    L’empathie en question

Lire et annoter des lettres de suicidés n’est pas une tâche facile. Il est probable que pour la mener à bien les annotateurs
doivent être amenés à faire appel à leur empathie, mais également à en réprimer une partie pour se protéger psychologi-
quement.
En théorie, cela ne devrait pas être différent d’autres situations où l’empathie est nécessaire, comme dans le cas de soins
psychiatriques. Ainsi, (Capuzzi & Golden, 2013) souligne la nécessité d’une relation empathique entre le thérapeute et
ses patients adolescents et (McLaughlin, 2007) insiste sur le rôle de l’empathie dans les réactions thérapeutiques face au
comportement suicidaire. Cependant, dans le cas de cette campagne les annotateurs n’étaient pas formés à cela, ce qui a pu
affecter leur capacité à gérer leurs sentiments et les effets secondaires de la campagne. Nous rappelons que les annotateurs
pouvaient arrêter n’importe quand s’ils avaient des difficultés (cette option leur était rappelée) et que différentes options
leur étaient proposées pour du soutien psychologique.
Les chercheurs qui ont organisé la campagne ont également été touchés, voire bouleversés par leur contact avec ce corpus.
De fait, les chercheurs les plus impliqués dans le projet ont l’obligation d’avoir un suivi psychiatrique ou religieux tous
les trimestres. Par ailleurs, une rotation régulière du personnel a lieu entre les différents projets.

22ème A NNOTATION DE LETTRES DE SUICIDÉS
4.3     Des utilisations potentiellement dangereuses des données

Les questions éthiques liées aux applications possibles d’un projet de recherche se posent sans doute dans tout le domaine
informatique (Ermann et al., 1997; Friedman, 1997; Martin & Weltz, 1999), et il n’y a aucune raison de penser que le
domaine du TAL fasse exception.
Cependant, ce projet est particulier en ce qu’il touche à la mort et au respect de la volonté.
Les conséquences de l’utilisation des technologies issues de ce corpus pourraient être particulièrement sévères, comme
l’hospitalisation injustifiée, l’emprisonnement, etc. Il est en effet assez facile d’imaginer l’utilisation qui pourrait être faite
de technologies issues de ce corpus par des gouvernements pour lesquels la psychiatrie serait un outil d’oppression des
dissidents comme, à une certaine époque, l’Union soviétique, où l’abus de diagnostics psychiatriques était massif (British
Medical Association, 1992).
Les buts recherchés eux-mêmes posent question. En effet, est-ce qu’une intervention pour éviter un suicide ne porte pas
atteinte au droit de mourir ? C’est une question connue des scientifiques travaillant (ou ayant travaillé) sur le sujet. Ainsi,
Joiner remarque que les psychiatres avec qui il a travaillé « respectaient l’autonomie finale des personnes, y compris leur
liberté de se donner la mort si c’était ce qu’elles souhaitaient vraiment » 6 (Joiner, 2009). L’un des chercheurs les plus
connus du domaine (Edwin Shneidman) dit également «. . .vous me demandez, eh bien, combien de suicides vous voulez,
je répondrais que je n’en veux aucun, mais je veux que la liberté de se suicider existe » 7 (Pestian, 2010).
5      Conclusion
Si nous considérons le corpus par rapport à la Charte Éthique et Big Data 8 (Couillault et al., 2014), nous constatons que
les conditions de collecte et de distribution des données pour l’annotation sont en accord avec les suggestions éthiques
sous-jacentes. La seule question en suspens est la section traitant des données liées aux contributeurs humains. Les trois
sous-sections concernées sont les suivantes :
— si un consentement a été demandé Dans notre cas, les annotateurs ont clairement consenti à ce que leurs annotations
soient distribuées, mais ce n’est évidemment pas le cas pour les auteurs des lettres.
— si une trace matérielle existe de ce consentement Le consentement des annotateurs a été obtenu par le biais d’une
interaction électronique, suivie, lorsque le besoin s’en est fait sentir, de conversations téléphoniques ou d’échange
de courriels.
— la nature de l’information fournie afin que le consentement soit éclairé Les annotateurs ont reçu une information
lors de la phase initiale de recrutement par courriel (pour ceux qui ont été recrutés directement), puis au cours de
la phase de formation (pour tous les annotateurs).
Nous avons présenté ici un certain nombre de questions éthiques liées à la construction d’un corpus de lettres de suicidés
annoté en émotions. D’autres points pourraient encore être analysés, notamment en ce qui concerne l’expérience vécue
par les annotateurs.
Nous pensons qu’une des leçons les plus importantes à retenir de ce projet est que les participants en contact avec ce type
de matériau doivent avoir accès à différents types de soutien. Ainsi, dans un travail mené actuellement sur les adolescents
suicidaires, les transcripteurs se voient proposé un suivi.
Enfin, la Charte Éthique et Big Data de ce corpus sera disponible prochainement.
6. “understood people’s ultimate autonomy, including their freedom to occasion their own death if they really were committed to doing so".
7. “. . .you say to me, well how many suicides do you want, and I say I don’t want any, but I want there to be the freedom to do it"
8. Voir : http://wiki.ethique-big-data.org.

K. B RETONNEL C OHEN , J OHN P. P ESTIAN , K ARËN F ORT
Références
BAUME P., C ANTOR C. H. & ROLFE A. (1997). Cybersuicide : the role of interactive suicide notes on the internet.
Crisis : The Journal of Crisis Intervention and Suicide Prevention, 18(2), 73.
B REVARD A., L ESTER D. & YANG B. (1990). A comparison of suicide notes written by suicide completers and suicide
attempters. Crisis : The Journal of Crisis Intervention and Suicide Prevention.
B RITISH M EDICAL A SSOCIATION (1992). Medicine betrayed : The participation of doctors in human rights abuses.
Zed books.
C APUZZI D. & G OLDEN L. (2013). Preventing adolescent suicide. Routledge.
C HAMBERLAIN J., P OESIO M. & K RUSCHWITZ U. (2008). Phrase Detectives : a web-based collaborative annotation
game. In Proceedings of the International Conference on Semantic Systems (I-Semantics’08).
C OUILLAULT A., F ORT K., A DDA G. & D E M AZANCOURT H. (2014). Evaluating Corpora Documentation with
regards to the Ethics and Big Data Charter. In International Conference on Language Resources and Evaluation (LREC),
Reykjavik, Islande.
C OUVET D., D EVICTOR V., J IGUET F. & J ULLIARD R. (2011). Scientific contributions of extensive biodiversity
monitoring. C. R. Biologies, 334, 370–377.
E RMANN M. D., W ILLIAMS M. B. & S HAUF M. S. (1997). Computers, ethics, and society. Oxford University Press.
F ÉDÉRATION FRANÇAISE DE PSYCHIATRIE (2001). La crise suicidaire : reconnaître et prendre en charge. John Libbey
Eurotext.
F ORT K., A DDA G. & C OHEN K. B. (2011). Amazon Mechanical Turk : Gold mine or coal mine ? Computational
Linguistics (editorial), 37(2), 413–420.
F ORT K., G UILLAUME B. & C HASTANT H. (2014). Creating Zombilingo, a Game With A Purpose for dependency
syntax annotation. In Gamification for Information Retrieval (GamifIR’14) Workshop, Amsterdam, Pays-Bas.
F RIEDMAN B. (1997). Human values and the design of computer technology. Number 72. Cambridge University Press.
G UPTA N., M ARTIN D., H ANRAHAN B. V. & O’N EILL J. (2014). Turk-life in india. In Proceedings of the 18th
International Conference on Supporting Group Work, GROUP ’14, p. 1–11, New York, NY, USA : ACM.
H O T., Y IP P. S., C HIU C. & H ALLIDAY P. (1998). Suicide notes : what do they tell us ? Acta Psychiatrica Scandina-
vica, 98(6), 467–473.
J OINER T. (2009). Why people die by suicide. Harvard University Press.
J OINER T. E., P ETTIT J. W., WALKER R. L., VOELZ Z. R., C RUZ J., RUDD M. D. & L ESTER D. (2002). Perceived
burdensomeness and suicidality : Two studies on the suicide notes of those attempting and those completing suicide.
Journal of Social and Clinical Psychology, 21(5), 531–545.
K RIPPENDORFF K. (1980). Content Analysis : An Introduction to Its Methodology, chapter 12. Sage : Beverly Hills,
CA., USA.
K RIPPENDORFF K. (2004). Content Analysis : An Introduction to Its Methodology, second edition, chapter 11. Sage :
Thousand Oaks, CA., USA.
L AFOURCADE M. & J OUBERT A. (2008). JeuxDeMots : un prototype ludique pour l’émergence de relations entre
termes. In Journées internationales d’Analyse statistique des Données Textuelles (JADT), Lyon, France.
L EENAARS A. A. (1988). Suicide notes : Predictive clues and patterns. New York : Human Sciences Press.
M ARTIN C. D. & W ELTZ E. Y. (1999). From awareness to action : Integrating ethics and social responsibility into the
computer science curriculum. ACM SIGCAS Computers and Society, 29(2), 6–14.
M C L AUGHLIN C. (2007). Suicide-related behaviour : Understanding, caring and therapeutic responses. John Wiley &
Sons.
M ERRIAM -W EBSTER (consulté en avril 2015). Définition du terme Crowdsourcing.
O LSON L. M. (2005). The use of suicide notes as an aid for understanding motive in completed suicides. PhD thesis,
Department of Health Promotion and Education, University of Utah.
O SGOOD C. E. & WALKER E. G. (1959). Motivation and language behavior : A content analysis of suicide notes. The
Journal of Abnormal and Social Psychology, 59(1), 58.
O’C ONNOR R. C. & L EENAARS A. A. (2004). A thematic comparison of suicide notes drawn from northern ireland
and the united states. Current Psychology, 22(4), 339–347.

22ème A NNOTATION DE LETTRES DE SUICIDÉS
P ESTIAN J. (2010). A conversation with edwin shneidman. Suicide and Life-Threatening Behavior, 40(5), 516–523G.
P ESTIAN J. P., M ATYKIEWICZ P. & L INN -G UST M. (2012a). What’s in a note : construction of a suicide note corpus.
Biomedical informatics insights, 5, 1.
P ESTIAN J. P., M ATYKIEWICZ P., L INN -G UST M., S OUTH B., U ZUNER O., W IEBE J., C OHEN K. B., H URDLE J.
& B REW C. (2012b). Sentiment analysis of suicide notes : A shared task. Biomedical Informatics Insights, 5, 3–16.
S AGOT B., F ORT K., A DDA G., M ARIANI J. & L ANG B. (2011). Un turc mécanique pour les ressources linguistiques :
critique de la myriadisation du travail parcellisé. In Actes de Traitement Automatique des Langues Naturelles (TALN),
Montpellier, France. 12 pages.
S CHNEIDMAN E. S. (1981). Suicide notes and tragic lives. Suicide and Life-Threatening Behavior.
S HNEIDMAN E. S. (1973). Suicide notes reconsidered. Psychiatry, 36(4), 379–394.
S HNEIDMAN E. S. & FARBEROW N. L. (1957a). Clues to suicide, volume 56981. McGraw-Hill Companies.
S HNEIDMAN E. S. & FARBEROW N. L. (1957b). Some comparisons between genuine and simulated suicide notes in
terms of mowrer’s concepts of discomfort and relief. The Journal of general psychology, 56(2), 251–256.
T UCKMAN J., K LEINER R. J. & L AVELL M. (1959). Emotional content of suicide notes. American Journal of Psy-
chiatry, 116(1), 59–63.
VOLANT É. (1990). Adieu, la vie... : étude des derniers messages laissés par des suicidés. Bellarmin.
Conférence TALN 1999, Cargèse, 12-17 juillet 1999
Une bibliothèque d’opérateurs linguistiques pour la
consultation de base de données en langue naturelle
Béatrice Bouchou, Denis Maurel

LI/E3i, Université François Rabelais
64, avenue Jean Portalis,
37200 Tours
(bouchou+maurel)@univ-tours.fr

Résumé
L’interrogation de bases de données en langue naturelle est une application directe du
traitement automatique des langues naturelles. Son utilité va en s’accroissant avec le
développement d’outils d’information accessibles au grand public à travers la Toile Internet.
L’approche que nous proposons s’appuie d’une part sur les fondations linguistiques établies
par la théorie de Z. S. Harris (dans l’élaboration du dictionnaire, et surtout dans la définition
des opérateurs linguistiques), et d’autre part sur un outil informatique précis (les
transducteurs). Elle représente une alternative aux traitements syntaxico-sémantiques
habituellement développés dans des formalismes logiques. Elle s’appuie sur la constitution
d’une bibliothèque d’opérateurs linguistiques pour les domaines d’application.

Mots clés
interrogation de base de données, langage naturel, opérateurs linguistiques, transducteur
(automate à nombre fini d’états)
1. Introduction
Utiliser la langue naturelle pour exploiter les ordinateurs est souhaitable pour des raisons
essentielles de confort d’utilisation d’une part, et d’ouverture générale vers un public peu
formé à l’informatique d’autre part. L’accès à l’information stockée dans les bases de données
est justement l’un des principaux axes grand public de l’informatique, c’est pourquoi le défi
de la langue naturelle est important dans ce domaine. Les nouvelles possibilités offertes par la
Toile Internet renforcent encore cette importance, car pour le commun des mortels la langue
naturelle présente bien des avantages par rapport aux actuels systèmes d’interrogation par
mots-clés, même sophistiqués.
Le but global est de traduire une question en langue naturelle vers une requête codée dans
un langage informatique d’interrogation de base de donnée, comme SQL par exemple. La
traduction doit associer aux mots de la question d’une part des éléments de la base considérée,
tables, attributs ou instances, d’autre part des opérations à réaliser sur ces éléments, à coder
dans les mots clés du langage d’interrogation.
Les premiers travaux sur la consultation de base de données utilisent tout naturellement un
lexique pour détecter des mots clés dans la question. Ainsi dans le système BASEBALL
[Green et al., 1960], l’information contenue dans la question est représentée par une liste de

Béatrice Bouchou, Denis Maurel

paires (attribut - valeur). Un certain nombre de fonctions sont appliquées sur ces éléments. La
connaissance nécessaire à la traduction est codée en partie dans la valeur liée à l’attribut, mais
l’essentiel se trouve dans les fonctions. La limite évidente de cette technique est qu’elle est
forcément étroitement liée au contexte. Dans ces conditions, on n’obtient de bons résultats
que pour des produits « ad hoc », conçus dans un but trop particulier [Sabah 1997].
Cependant la limite est-elle due à l’approche par mots-clés en soi, ou bien au défaut de
représentation adéquate de la connaissance nécessaire ? Il est à noter que dans BASEBALL,
le traitement syntaxique est effectué selon la théorie syntaxique de Z. S. Harris que nous
utilisons aussi pour définir nos opérateurs linguistiques.
Dans le but de généraliser l’outil d’interrogation (c’est-à-dire d’améliorer sa portabilité),
plusieurs travaux s’attachent à exploiter automatiquement la structure de la base et son
contenu pour alimenter le lexique [Kaplan 1984, Grosz et al., 1987, Clifford 88]. Dans son
article, Kaplan présente un système, CO-OP, qui est portable dans la mesure où ses sources de
connaissances sont uniquement un lexique et la base de données elle-même. De même, le
système TEAM développé par Grosz et ses collègues a pour objectif principal la portabilité :
il est conçu pour interagir avec deux types d’utilisateurs, un expert de la base de donnée d’une
part, l’utilisateur final d’autre part. Le système construit sa connaissance par interaction avec
l’expert de la base de données. L’essentiel de l’interprétation dépend des directives de cet
expert. Nous adoptons le même principe.
Le système TEAM est une excellente référence dans l’approche qui consiste en une
analyse syntaxique complète, suivie d’analyses sémantiques sur les arbres obtenus. Toutes ces
opérations sont réalisées dans un formalisme du type logique du 1er ordre. Or, une fois isolés
les constituants sémantiques pertinents dans la question, il apparaît que les trois quarts des
nœuds de l’arbre syntaxique ne représentent que la «glu syntaxique» (op. cit., p. 203) de la
phrase originelle. Ces nœuds sont pourtant pris en compte dans toute la première phase
d’analyse, ce qui nuit à l’efficacité. Un autre exemple axé sur la logique est décrit dans
[Péquegnat et al., 1988] : pas moins de cinq bases de connaissances y sont constituées,
représentations logiques des données linguistiques, conceptuelles (la base de données),
sémantiques. Un système complet de réécriture est nécessaire pour chaque passage d’une
représentation à une autre… Les traitements en sont alourdis d’autant. Nous ne passons pas
par une formalisation logique, bien que la formulation des opérateurs linguistiques s'en
rapproche quelque peu.
A la fin des années 80, les développements liés à l’interrogation de bases de données en
langue naturelle se sont, nous semble-t-il, scindés en deux : d’une part, l’approfondissement
d’outils d’analyse du langage naturel (pour répondre en particulier au problème de la
portabilité) et, d’autre part, l’étude du dialogue homme - machine. Nous revenons peut-être
sur des idées anciennes, mais c’est pour les aborder avec des outils différents, linguistiques et
informatiques.
Outils linguistiques
En ce qui concerne le traitement linguistique, le système est caractérisé par :
-   La construction interactive, avec l’administrateur de la base, d’un dictionnaire
électronique de mots clés, qui exploite de façon automatique son schéma et son
contenu. Celui-ci contiendra donc, entre autre, toutes les instances.
-   La définition et l’utilisation d’opérateurs linguistiques, plutôt que des opérateurs de
la logique : c’est là une application de la théorie proposée par Z. S. Harris [Harris,
1968, 1976] et décrite, pour le français, par de nombreux travaux [Gross, 1975],
[Boons et al., 1976]. Nous intégrons au lexique une description exhaustive des
opérateurs de la langue pour le domaine considéré, en associant aux arguments des

Consultation de Base de Données en langue naturelle

prédicats une information qui se rapproche du concept de classe d’objet [Le Pesant,
Mathieu-Colas, 1998].
-   Une interprétation de la question par analyse pré-syntaxique : pour cela, on extrait
de la question les informations linguistiques pertinentes, en s’aidant du dictionnaire
[Maurel, 1991, Maurel et Mohri, 1995].
Ces trois points correspondent à un système de mots clés complet, enrichi par le codage et
l’exploitation des opérateurs linguistiques du domaine.
Outils informatiques
Les avantages liés à la représentation des données par un transducteur, déjà recommandée
par [Gross et Perrin, 1989], ne sont plus une nouveauté dans le cadre du traitement du langage
naturel [Roche et Schabes, 1997]. C’est grâce aux progrès algorithmiques de la théorie des
automates à nombre fini d'états que nous pouvons utiliser un dictionnaire aussi complet, et
donc aussi volumineux. Notre dictionnaire sera, en fait, un transducteur minimal. Celui-ci sera
construit directement par l'algorithme de [Stoyan, 1999], adapté aux transducteurs, en suivant
le principe de placement des sorties défini par [Mohri, 1994] ; puis il sera compacté sous la
forme d'un tableau de taille à peu près égale au nombre de transitions [Liang, 1983].
En ce qui nous concerne, nous mettons en œuvre une « cascade de transducteurs » :
- le premier transducteur est notre dictionnaire : il extrait de la question les
informations pertinentes et les remplacent par ses propres codes
- un deuxième transducteur, qui part donc du résultat du premier, traite les opérateurs
linguistiques reconnus,
- puis des transducteurs « SQL » génèrent finalement une requête à partir des
informations produites par les deux premiers.
En conséquence, plutôt qu’une description formelle de la phrase à partir de règles
syntaxiques, puis sémantiques, nous associons aux mots l’information issue de la base de
données, avec les traitements afférents. Plus précisément, ces traitements sont associés aux
opérateurs linguistiques par l’administrateur de la base de données lors de l’installation du
système sur la base.
Dans la section 2 de cette communication, nous revenons sur le principe général du
système, ainsi que sur les transducteurs utilisés ; dans la section 3, nous précisons le contenu
du dictionnaire, et, en particulier, le codage des opérateurs linguistiques. Nous concluons en
section 4. Afin d’unifier la présentation, tous les exemples ont trait à la même base de
données, brièvement décrite en Annexe.

2. Le système : une cascade de transducteurs
2.1. Présentation générale
Nous donnons Figure 1 un aperçu de l’application, laquelle se divise en deux classes
d’outils : ceux qui « installent » le module d’interrogation sur la base de donnée (partie a de la
Figure 1) et ceux qui réalisent l’interrogation (partie b).
L’utilitaire pour la construction du dictionnaire exploite la structure et le contenu de la base
de données, ainsi que les bibliothèques d’opérateurs linguistiques ; il fait également appel à
des dictionnaires de synonymes. Le transducteur généré est ensuite compacté.
En phase de fonctionnement, tant que la session de questions n’est pas terminée, il y a :
- lecture de la question,
- exécution de la cascade de transducteurs,
- interrogation de la base et présentation du résultat.

Béatrice Bouchou, Denis Maurel

Les deux phases bien distinctes, le recours à un connaisseur de la base de données, tout
cela se rapproche du système TEAM, cependant nos questions à l’administrateur de la base de
données sont plus simples (plus techniques en ce qui concerne la base, mais moins
linguistiques) et les traitements sur la question sont différents.
B ib lio th èq u e d es o p érateu rs lin g u istiq u es

O p érateu rs           O p érateu rs                B ase d e d on n ées
d u d om ain e          g én éraux                   (S tru ctu re, in stan ces)

a ) C o n stru ctio n d u sy stèm e                                                                              S y n o n y m es
G én ératio n du
d ictio n n aire

A d m in istrateu r              In terface :
d e la b ase d e                 m en u s,
d o n n ées                      listes d e                   G én ératio n du sy stèm e
ch o ix                      d ’in terro g atio n , à b ase d e
tran sd u cteu rs
b ) L e sy stèm e en o p éra tio n

In terp rétatio n,
S aisie d e la                             g én ératio n d ’u n e
U tilisateu r
q u estio n                                 req u ête en S Q L
P résen tatio n d u
résu ltat                                          SG B D
Figure 1 : Présentation générale
2.2. Les transducteurs
Comme nous l'avons dit en introduction, le premier transducteur, qui opère à partir de la
question, est une représentation compacte de la base de données (les noms des tables et des
attributs, ainsi que les instances, le tout accompagné de synonymes éventuels) et des
opérateurs linguistiques que nous comptons mettre en œuvre.
Prenons pour exemple la base des meubles, dont une partie est donnée Figure 3 (en
annexe). La Figure 2 présente un petit extrait simplifié de dictionnaire. Les noms des tables
sont remplacés par les attributs qui représentent les tables elles-mêmes. Ces attributs
particuliers (soulignés, ici) seront indiqués au système par l’administrateur de cette base, lors
de l'installation du logiciel. Nous associons donc à salon un attribut (A.) de code
A.Canapés.Nom. De même, l’information associée à l’entrée Bahia se lit de la façon
suivante : il s’agit d’une instance (I.) de l’attribut Nom de la table Canapés et sa valeur est
Bahia.
La Figure 2 montre, dans sa partie inférieure, le transducteur correspondant : les ronds
représentent les états et les lettres étiquettent les transitions. Un mot est reconnu lorsque son
parcours correspond à un parcours des états du transducteur et se termine sur un état final
(rond noir). Le code lié au mot est généré par le transducteur au cours de la reconnaissance
(ce sont les sorties, en gras sur la figure).

Consultation de Base de Données en langue naturelle
Bahia →I.Canapés.Nom.Bahia                                       j       o       u
é                               r
Baie →I.Canapés.Couleur.Baie
s       a       l       o       n       s
salon →A.Canapés.Nom
A.Canapés.Nom
salons →A.Canapés.Nom                              B    a    h   i     a
séjour →A.Canapés.Nom                           I.
Canapés.Nom.Bahia
séjours →A.Canapés.Nom                Canapés.Couleur.Baie
i       e

Figure 2 : Un exemple de représentation d’un dictionnaire par un transducteur
Prenons comme exemple la question : Quels sont les salons de couleur Baie ?. Elle
contient un synonyme d'un nom de table, salons, un opérateur linguistique (voir § 3), la
préposition de, un nom d'attribut présent dans plusieurs tables, couleur et, enfin, une instance,
Bahia. Tous ces mots sont reconnus par le transducteur qui leur associe leur code :
salons → A.Canapés.Nom
de → DE
couleur → A.Canapés.Couleur | A.Lits.Couleur | A.Rideaux.Couleur | …
Baie → I.Canapés.Couleur.Baie | I.Rideaux.Couleur.Baie
Le premier transducteur génère un ensemble de codes à partir de la question. Chaque code
est la « définition » d’un mot reconnu. Nous avons donc un système de reconnaissance de
mots-clés, à ceci près que l’information associée à ces mots va permettre une analyse
linguistique, grâce à la reconnaissance d'opérateurs.
Le deuxième transducteur associe aux opérateurs linguistiques ses arguments et transforme
l'ensemble en un seul code (voir partie 3). Dans notre exemple, on obtiendrait seulement, au
final :
DE(A.Canapés.Nom, I.Canapés.Couleur.Baie)
Le travail de codage des opérateurs linguistiques trouve ici son application et permet de
générer des informations exploitables par les transducteurs de la dernière étape.
Les derniers transducteurs opèrent à partir du résultat du deuxième, générant chacun une
partie de la requête SQL : en général, il y a une partie SELECT, une partie FROM et une
partie WHERE [Bowman & al., 1996]. La partie SELECT comprend les attributs des tables
identifiés dans la question (certains sont ajoutés systématiquement selon le contexte, en
particulier le nom). La partie FROM comporte toutes les tables rencontrées dans la question.
La composante WHERE est construite sur la base des opérateurs. Pour l’exemple des canapés
de couleur Baie, on obtiendrait :
SELECT Produits.RéfProduit, Produits.Nom,
FROM       Canapés INNER JOIN Produits ON Canapés.RéfProduit =
Produits.RéfProduit
WHERE (((Canapés.Couleur) Like "Baie"))

3. La constitution du dictionnaire : les opérateurs linguistiques
La présentation rapide que nous venons de faire du système montre l’importance des codes
associés aux mots dans le dictionnaire : là se situe toute l’information nécessaire à la
génération d’une requête SQL pertinente. Nous indiquons dans un premier temps comment
sont représentés et utilisés les opérateurs linguistiques, puis nous décrivons le processus de
génération du dictionnaire.

Béatrice Bouchou, Denis Maurel

3.1. L'exemple de la préposition de
Dans l'exemple ci-dessus (Quels sont les salons de couleur Baie ?), la préposition de
correspond à l'opérateur que nous avons appelé DE. Cet opérateur DE possède deux
arguments, qui sont les attributs (éventuellement instanciés) d'une même tablei. L'un de ces
deux attributs doit correspondre à l'attribut représentant la table, ce qui est le cas ici.
Un seul de ces attributs soulignés étant présent ici, le deuxième transducteur choisit la table
Canapés et non les autres tables qui ne sont présentes que d'un seul côté de l'opérateur. On
obtient donc :
salons de couleur → DE(A.Canapés.Nom, A.Canapés.Couleur)
Baie → I.Canapés.Couleur.Baie | I.Rideaux.Couleur.Baie
Puis ce même transducteur reprend le résultat obtenu (dans la "cascade", le transducteur 2
est utilisé plusieurs fois) et instancie la couleur à Baie, tout en éliminant la référence à la table
des rideaux :
salons de couleur Baie → DE(A.Canapés.Nom, I.Canapés.Couleur.Baie)
Lors de l'installation, l'administrateur a entré la formule SQL correspondant à ce genre de
requête. C'est alors que les transducteurs SQL ont été construit. Ils vont pouvoir maintenant
fournir le résultat présenté à la fin du § 2.
3.2. Les opérateurs linguistiques
Les connaissances linguistiques sont codées dans des bibliothèques qui seront réalisées en
collaboration avec Barbara Kaltz, de l'équipe de recherche en linguistique de Tours, à la fois
en français et en allemand, ceci pour éviter des réponses trop ad hoc… Quelques exemples
détaillés sont donnés dans [Bouchou et al., 1999]. On distingue deux domaines :
-   le domaine général : les groupes numériques, les groupes nominaux, les verbes
“être”, “avoir”, les notions de comparaison (“plus”, “moins”), …
-    le domaine contextuel : ainsi pour la base de données sur le magasin de meubles, on
aura besoin d’opérateurs comme “ACHAT”, lié aux mots “achats”, “acheter”,
“achetés”, “acquisition”, “acquisitions”, “acquérir”, “approvisionner”, …
Les opérateurs représentent donc les différentes flexions des prédicats de la question
(verbes, noms, adjectifs). Leur recensement permet, lors de l’installation du module sur la
base, de poser à l’administrateur chargé de cette installation, qui connaît bien sa base de
données, des questions très précises. Il lui est demandé grosso modo d’indiquer la partie de la
base où se trouvent les informations associées aux opérandes, et leur méthode de calcul si
nécessaire.
Par exemple, prenons l’opérateur STOCK, qui n’a qu’un opérande dans notre contexte (le
stock courant). En français, il correspond à différents prédicats, des verbes :
Les salons de couleur Baie ont été stockés
Il reste des salons de couleur Baie
Ou des noms prédicatifs, associés à leurs verbes supports appropriés :
Les salons de couleur Baie sont en stock
Les salons de couleur Baie sont en réserve
Il reste en stock des salons de couleur Baie
Il reste en réserve des salons de couleur Baie
i
Signalons que la préposition de peut aussi sélectionner un autre opérateur, que nous avons appelé DANS et
qui est défini par des jointures entre tables, lors de l'installation. Par exemple ici, la question Quelles sont les
commandes de Dupont ?. Voir à ce propos [Bouchou et al., 1999].

Consultation de Base de Données en langue naturelle

Son code dans la bibliothèque est le suivant :
STOCK(quoi?)
Dans le contexte de notre base de meubles, l’interaction avec l’administrateur de la base
permet alors de construire l’association suivante (à l’intention du transducteur 2) :
quoi?≡ I.Produits.RéfProduits/Σ ([UnitésReçues])- Σ ([UnitésVendues])- Σ ([PerteUnités])
De cette manière on sait que le stock concerne une référence de produit, et qu’il est calculé
par la formule donnée.
Il peut arriver que la base ne permette pas de répondre à certaines des questions associées à
l’opérateur en bibliothèque. Par exemple, s’il n’y a pas de notion de date dans la base, on ne
peut pas interroger sur l’opérande quand? de l’opérateur COMMANDE. Dans ce cas
l’administrateur n’indique aucun élément (ni attribut, ni formule de calcul), et si cette
« dimension » est questionnée au cours de l’utilisation, le système répondra qu’aucune
réponse n’est disponible.
Dans le même ordre d’idées, l’opérateur COMMANDE représente un exemple d’ambiguïté
dans le contexte. Lorsqu’on demande : a-t-on commandé des canapés bleus ?, il est possible
que l’on veuille savoir si des canapés bleus sont en cours de commande et vont arriver. Il est
également possible que l’on veuille un bilan des commandes de canapés bleus, réceptionnées
ou non. De tels cas se repèrent dans la phase de constitution des bibliothèques, grâce aux
lexiques linguistiques du domaine. Le système peut alors construire une réponse pour chaque
interprétation signalée.
3.3. Création du dictionnaire
Le dictionnaire électronique pour l’interprétation des questions posées à la base de données
est construit à partir de cinq sources :
- le contenu de la base : les enregistrements
- le schéma (structure) de la base : tables, attributs, etc.
- des dictionnaires de synonymes
- le codage précédemment décrit des opérateurs généraux et du domaine
- une connaissance pointue de la base de données et de son utilisation courante
(connaissance qu'a l'administrateur)
Dans un premier temps, chaque enregistrement de la base est copié dans le dictionnaire, le
code associé indiquant qu’il s’agit d’une instance de tel champ, de telle table (cf. “Bahia” et
“Baie” dans le petit dictionnaire de la Figure 2). A la fin de cette phase, le dictionnaire
renferme une copie de chaque donnée de la base.
Pour les autres sources, l’alimentation du dictionnaire passe par une interaction avec
l’administrateur de la base de données. Par exemple, en ce qui concerne la structure de la
base, nous développons un utilitaire qui extrait les données relatives à la structure de la base à
partir du catalogue et interroge l’administrateur pour leur associer les informations utiles pour
l'interprétation de la question. Cet utilitaire ajoute également des synonymes (après avis de
l'administrateur).
Nous l’avons vu, les codes distinguent les tables, les attributs, les instances, les chiffres, les
opérateurs, etc.
Par exemple, le fait qu’il existe une table désignée par Canapés dans la base sera traduit
par une entrée canapé dans le dictionnaire, associée au code A.Canapés.Nom, lequel
s’interprète de la façon suivante : « ce mot correspond au nom d’une table de la base de
donnée, la table Canapés ; celle-ci a pour attribut principal l'attribut Nom ». Si

Béatrice Bouchou, Denis Maurel

l’administrateur juge que le mot séjour est un synonyme de canapé dans le contexte
d’utilisation de sa base, alors séjour sera associé au même code.
Le principe est le même pour les opérateurs linguistiques liés au contexte. Pour un
opérateur donné (issu de la bibliothèque des opérateurs linguistiques), l’administrateur de la
base devra donner, lorsqu’elles existent, les formules de calcul de chaque opérande.
Cette étape est cruciale pour la qualité de l’interprétation des questions. L’administrateur
de la base de données doit utiliser sa connaissance de la base et de son contexte, la
connaissance qu’il a des besoins des utilisateurs (c’est important) et la connaissance normale
qu'un locuteur a de sa langue. Il n’a pas à être un expert des méthodes et outils de l’IA.

4. Conclusion
Pour se situer par rapport aux techniques logiques classiques, disons que le premier
transducteur effectue une reconnaissance qui se situe au niveau d’une « présyntaxe ». Il
détecte les éléments significatifs de la question. L’analyse porte alors sur cette sélection, et
non sur un arbre syntaxique complet de la phrase. Cette analyse s’appuie sur une définition
linguistique générale des « prédicats » de la question, définition qui a été particularisée, au
moment de l’installation sur la base de données, pour correspondre exactement au contexte, et
devenir ainsi complètement opératoire.
L’approche que nous développons évite une série d’opérations sur des arbres syntaxico-
sémantiques. On peut voir la syntaxe comme une définition en compréhension du langage,
alors que le transducteur offre une définition extensive des mots d’une question : tout ce qui
est nécessaire à l’interprétation doit y être. C’est là l’apport essentiel des transducteurs à
nombre fini d'états : représenter une masse importante d’information sous une forme
suffisamment condensée pour être exploitable efficacement.
La souplesse, et en particulier la « portabilité » du système (c’est-à-dire son adaptabilité à
toute base de données) est satisfaisante dans la mesure où tout se passe à la construction du
dictionnaire. A l’intérieur d’un même domaine, le passage d’une base de données à l’autre ne
pose pas de difficulté, puisque la structure et le contenu de la nouvelle base permettent
d’alimenter la partie à modifier dans le dictionnaire. De même, les évolutions de la base de
données sont prises en compte par reconstruction de la partie du dictionnaire concernée, puis
par régénération des transducteurs.
L’application que nous proposons reste néanmoins relative à un domaine (par exemple le
tourisme, la vente par correspondance, la gestion de stocks, …). Le passage d’un domaine à
l’autre nécessite la constitution de la bibliothèque d’opérateurs linguistiques liés au domaine,
lorsqu’elle n’existe pas encore. Mais une fois construite, cette bibliothèque sert pour toutes les
bases de données du domaine considéré.
Le premier développement lié aux idées présentées ici avait un but spécifique :
l’interrogation en langue naturelle d’une base de données spatiales sur des informations
touristiques Tourangelles [Thomas et al. 1997]. Depuis, une partie des résultats a été reprise et
généralisée pour créer un prototype dont nous testons actuellement la portabilité sur plusieurs
exemples de bases de données.
Les développements actuels sont réalisés en Java. Nos efforts portent principalement sur
l’enrichissement de la bibliothèque des opérateurs linguistiques (travail linguistique),
l’amélioration de l’interface de construction du dictionnaire, l’optimisation des transducteurs.

Références

Consultation de Base de Données en langue naturelle

Boons J. P., Guillet A., Leclère C. (1976a), La structure des phrases simples en français. I.
Constructions intransitives, Genève, Droz.
Boons J. P., Guillet A., Leclère C. (1976b), La structure des phrases simples en français. II.
Constructions transitives, Paris, Rapport de recherche du LADL n°6.
Bouchou B., Maurel D., Kaltz B. (1999), Prédicats logiques / prédicats linguistiques
pour la consultation de base de données en langue naturelle, Revue Informatique et
Statistique dans les Sciences Humaines, à paraître.
Bowman J., Emerson S., Darnowsky M., The practical Sql handbook : using standard
query language, Addison Wesley, 1996.
Clifford J. (1988), Natural Language Querying of Historical Databases, Computational
Linguistics, Vol. 14, n° 4, pp. 10-34.
Green B.F., Wolf A.K., Chomsky C., Laughery K. (1960), Baseball : an automatic
question-answerer, Computers and tought, Mc Graw Hil, New-York, pp. 207-216.
Gross G. (1998), Pour une véritable fonction synonymie dans un traitement de texte,
Langages, 1998, n°131.
Gross M., Perrin D. (1989), Electronic Dictionaries and Automata in Computational
Linguistics, Lecture Notes in Computer Science, 377, Springer.
Gross M. (1975), Méthodes en syntaxe, Hermann, Paris.
Grosz B., Appelt D., Martin P., Peirera F.(1987), TEAM : an experiment in the design of
transportable natural language interfaces, Artificial Intelligence 32, pp 173-243.
Harris Z. S. (1968), Mathematical Structures of Language, Interscience Publishers.
Harris Z.S. (1976), Notes du cours de syntaxe, Paris, Le Seuil.
Kaplan S. J. (1984), Designing a Portable Natural Language Database Query System, ACM
Transactions on Database Systems, vol. 9, n°1, march 1984, pp. 1-19.
Le Pesant D., Mathieu-Colas M. (1998), Introduction aux classes d’objets, Langages,
1998, n°131.
Liang F. M. (1983), Word Hyphenation by Computer, PhD Thesis, Computer Science
Departement, Standford University, Research Report STAN-CS-83-977.
Maurel D., Mohri M. (1995), Computation of French Temporal Expressions to query
database, First International Workshop on Applications of Natural Language to Data
Bases (NLDB'95) (Actes p. 71-80).
Maurel D. (1991), Préanalyse des adverbes de date du français, TA information,
volume 32, n°2, p. 5-17.
Mohri M. (1994), Minimization of Sequential Transducers, Theorical Computer
Science.
Péquegnat C., Aguirre J.L. (1988) , Interface en langue naturelle pour l’interrogation d’une
base de données relationnelle, Systèmes experts et applications, pp 441-460.
Roche E., Schabes Y. ed. (1997), Finite state language Processing, Cambridge,
Mass./London, England: MIT Press.
Sabah G. (1997), Le sens dans le traitement automatique des langues, T.A.L., 1997, vol. 38,
n°2, pp. 91-133.

Béatrice Bouchou, Denis Maurel

Stoyan M. (1999), Direct Construction of Minimal Acyclic Finite States Automata,
Annuaire de l'Université de Sofia St. Kl. Ohridski, Faculté de Mathématiques et
Informatique, volume 92, livre 2.
Thomas N., Maurel D., Trépied C. (1997), Interrogation en langage naturel d'une base de
données spatiales, Rapport interne du Laboratoire d'Informatique de l'Université de Tours,
n°190, 10 p.

Annexe
La base de données dont sont issus les exemples présentés au fil de cette communication a
pour objet la gestion d’un magasin de meubles. Les principales tables, leurs attributs et leurs
jointures sont dans la Figure 3 ci-dessous.
Figure 3 : extrait des tables et jointures de la base des meubles
Les questions typiquement posées à cette base concernent les caractéristiques des meubles
(questions dites « de catalogue ») et le stock ou les commandes en cours. Elles sont émises
par les vendeurs pour répondre aux clients du magasin. Par exemple,
“qu’avons-nous comme canapés bleus ?”,
“qui a commandé le produit de référence XXX ?”, etc.
Plus épisodiquement, des opérations de marketing (table des clients), des calculs de primes
salariales (au pourcentage des ventes), diverses analyses et autres bilans peuvent justifier
d’autres types d’interrogation, également pris en compte dans le système d’interface en
langue naturelle.

L'interrogation de bases de données
comme application des classes d'objets

Béatrice Bouchou, Julien Lerat, Denis Maurel

LI, Université François Rabelais
E3i, 64 avenue Jean Portalis, 37200 Tours
(bouchou+maurel)@univ-tours.fr
Résumé – Abstract
En travaillant sur l'interrogation de bases de données en langue naturelle, nous sommes
amenés à exploiter les propositions du Laboratoire de Linguistique Informatique (LLI) en
matière de représentation de la langue : les classes d'objets. Un outil d'interrogation définit
une application du langage vers le modèle de l'information stockée. Ici les classes d'objets et
leurs prédicats appropriés modélisent le langage source, tandis que le modèle relationnel sert
pour les données interrogées. Nous présentons d'abord ce contexte d'application, puis
comment nous utilisons les classes d'objets et prédicats appropriés dans ce cadre.

We investigate how to use natural language to query a database from both the linguistic and
database points of view (but without AI considerations). In order to achieve this goal, we need
a natural language model which we can map on to a relational database model. We have
chosen to use the word classification called « classes d'objets » as proposed by the
Laboratoire de Linguistique Informatique (LLI). We present here the first results of this work.

Mots clés – keywords interrogation de BD en langage naturel, modèle relationnel,
classes d'objets – natural language database query, relational model, « classes d'objets »
1.    Introduction
Notre système est destiné à interpréter la langue naturelle dans le cadre précis d'une
interrogation de bases de données. Ses grandes lignes sont présentées entre autres dans
(Bouchou, Maurel, 1999). En phase opérationnelle, l'utilisateur écrit sa question en langage
naturel, puis le système lui fournit une réponse issue de la base de données. Avant d'être
opérationnel, le système est "installé" sur la base de données cible. Le lien entre le sens de la
question et le sens des données stockées est établi au cours de cette installation, sous la forme
d'un dictionnaire électronique de mots clés.

La phase de configuration du système d'interrogation pour une base de données précise est
l'une des principales pierres d'achoppement des systèmes existants (Kaplan, 1984), (Sabah,
1997) : il faut qu'elle soit rapide, et surtout qu'elle ne nécessite pas l'intervention d'un expert
en linguistique, ni en intelligence artificielle. C'est ici que nous faisons intervenir les classes

Béatrice Bouchou, Julien Lerat, et Denis Maurel
d'objets et prédicats appropriés, concepts mis au point par l'équipe de Gaston Gross au LLI
(Gross, 1994), (Gross, 1998), (Le Pesant, Mathieu-Colas, 1998), (Le Pesant, 2000). Les
classes d'objets découlent de la théorie des opérateurs linguistiques, développée par Z.S.
Harris (Harris, 1976) et décrite, pour le français, par les nombreux travaux qui ont suivi
(Gross, 1975). Ces classements définis en fonction des opérateurs linguistiques spécifient les
conditions (syntaxiques) que ces mots doivent remplir pour faire sens avec les autres mots de
la question. Notre système s'attache à associer ainsi à la question le sens qu'impose la base.

La phase d'installation consiste à construire un dictionnaire de mots clefs :
• En entrée sont les mots susceptibles d'être reconnus dans une question sur la base : ces
mots sont rattachés à des classes d'objets et/ou à des prédicats (ou classes de prédicats).
• En sortie sont des codes qui indiquent :
• d'une part les éléments de la base auxquels il est fait référence dans la question,
• et d'autre part les liens entre ces éléments dans la base, établis à partir des liens
reconnus entre les mots dans la question.

Nous disposons alors d'un système de mots clés, combiné à la connaissance apportée par les
opérateurs linguistiques. Pour ce dictionnaire nous travaillons sur des transducteurs à nombre
fini d'états, minimaux, compactés (Revuz, 1991), (Mihov, 2000). Au contraire d'un système
développé avec ILLICO (Pasero, 1999), nous n'utilisons pas de logique : d'une part, la
connaissance de la langue (syntaxe/sémantique) se trouve dans les classes d'objets, et d'autre
part les modèles conceptuel et contextuel sont présents dans la base interrogée.

Dans la section 2 de cette communication nous exposons ce que sont les liens entre données
dans une base de données relationnelle. Cela permet, dans la section 3, de préciser ce qui est
recherché dans la base lors d'une interrogation. Après cette spécification de la cible, la section
4 revient sur la théorie utilisée pour représenter la source (la question en langue naturelle),
puis présente la correspondance établie entre les deux. Enfin, nous rapportons dans la section
5 des exemples de l'utilisation des classes d'objets lors de la construction du dictionnaire.
2.       Le "sens" des données stockées
Une base de données représente un système d’information. Le sens des données de la base est
fondamental pour concevoir un système d'interrogation : c’est de cela que parlera la question.
2.1       Le modèle conceptuel d'une base de données
Rappelons comment est conçue une base : la première étape consiste à modéliser le système
d'information, c'est le modèle conceptuel (MCD). La figure 1 donne un exemple de résultat
avec le formalisme E-A (« Entités-Associations ») : on a 3 entités, Station, Skieur et
Compétition, avec leurs attributs, ainsi que 2 associations, est classé et A lieu à, la première
ayant pour attribut le rang de classement.

Le système d'information ne saurait être réduit aux seules entités et associations. L’ensemble
des contraintes est également une composante essentielle de sa description. Les principaux
types de contraintes conceptuelles sont :

•   Comment un sous-ensemble d'attributs détermine les valeurs d'une occurrence d'entité,
ce qui se traduit par la notion d'identifiant (la "clef" dans les BD relationnelles).

L'interrogation de bases de données comme application des classes d'objets
•   Comment certains attributs d'une entité A sont liés à certains attributs d'une entité B, ce
qui se traduit par la notion de cardinalité des associations.

Skieur      Est classé           Compétition        A lieu à          Station
réfSkieur      rang                réfComp                              nomStat
nomSkieur                          nomComp                              paysStat
(0,n)           (1,n)                  (1,1)           (0,n)
spécialité                         dateComp                             capacité

Figure 1. Un schéma E-A.

La cardinalité consiste en un couple d'informations : le nombre minimum d'occurrences de
l'entité dans l'association (choix restreint à 0 ou 1), et le nombre maximum d'occurrences de
l'entité dans l'association (choix restreint à 1 ou "plusieurs", le second cas étant noté "n").

Revenons à l'exemple de la figure 1 (notez que les identifiants des entités ont été soulignés) :
le couple (0,n) du côté Skieur indique qu'un skieur participe à éventuellement 0 et en général
plusieurs compétitions, tandis que (1,n) côté Compétition dénote qu'une compétition accueille
au moins 1 et en général plusieurs skieurs. La cardinalité (1,1) côté Compétition exprime
qu'une compétition a lieu dans au moins une station et au plus une station.

Une fois spécifié le schéma conceptuel du système d'information, il faut le traduire dans une
représentation opérationnelle du point de vue informatique : à ce stade on peut utiliser soit un
modèle orienté objet, soit un modèle hiérarchique, soit encore un modèle relationnel. Dans le
cas du modèle relationnel, les tables de la figure 2 sont dérivées du schéma de la figure 1. On
voit qu'un certain nombre de règles président à la traduction :

•     Une entité se traduit par une table, chacun de ses attributs devenant une colonne de la
table, et son identifiant devenant la clef (primaire) de la table.

•     Une association avec un maximum égal à n des deux côtés se traduit par une table, dont la
clef est formée des clefs de chaque table correspondant aux entités associées. Chacun de
ses attributs devient une colonne de la table, cf. la table Classement.

•     Une association avec maximum égal à 1 d'un côté se traduit par l'ajout, dans la table
"dépendante", de la clef de l’autre table, cf. nomStat dans Compétition.

Compétition(réfComp, nomComp, dateComp, nomStat)
Skieur(réfSkieur, nomSkieur, spécialité)
Station(nomStat, paysStat, capacité)
Classement(réfComp, réfSkieur, rang)

Figure 2 : Tables dérivées du schéma E-A.
2.2      Retrouver le sens des données d'une base en exploitation
Le modèle relationnel est le modèle le plus largement répandu dans les bases de données
actuelles. En relationnel « pur », on ne dispose que des relations (les tables avec leurs
colonnes) : cela seul ne permet pas de retrouver le « sens » des données, c’est-à-dire les liens
qui existent entre elles. Mais il a très vite été adjoint à la théorie relationnelle de quoi

Béatrice Bouchou, Julien Lerat, et Denis Maurel
exprimer ce sens : ce sont les contraintes (Abiteboul et al., 1995). Dans notre problématique,
les contraintes exprimant le modèle conceptuel à l'origine de la base, nous nous en servons
pour déterminer l’image dans la base de données des liens sémantiques trouvés entre les mots
de la question (liens exprimés par les prédicats).

Il est possible de retrouver automatiquement les contraintes d’une base de données en
exploitation : soit dans son catalogue (lequel contient les « méta-données ») si le concepteur a
explicitement posé des contraintes sur la base, soit par des calculs sur le contenu des tables
(Novelli et al., 2000). C'est le cas en particulier pour les dépendances fonctionnelles.

Dépendance fonctionnelle : Il y a une dépendance fonctionnelle lorsque la valeur d’un
attribut (ou d'
un groupe d' attributs) détermine de façon unique celle d’autres attributs. Nous
appelons df un tel lien entre un attribut et les autres attributs de la même table. Dans notre
exemple, la référence de la compétition en détermine la date et la station. De même la
référence du skieur en détermine le nom et la spécialité, etc.

Dépendance d'inclusion : Il y a une dépendance d'inclusion lorsque les valeurs d'un attribut
d'une table doivent appartenir à l'ensemble des valeurs d'un attribut d’une autre table. Nous
appelons di un lien de dépendance d'inclusion. Par exemple, le nom de station de la
compétition (dans Compétition) doit apparaître dans la colonne nomStat de Station. Cette
information permet de déduire que, la référence de la compétition déterminant la station (dans
Compétition), elle en détermine également le pays et la capacité d'accueil (dans Station).

Les contraintes n'expriment pas à elles seules tous les liens entre les données de la base : un
certain nombre de ces liens ne sont pas stockés (statiques), mais calculés (dynamiques). Ainsi
il existe dans tout SGBD des fonctions prédéfinies, appelées fonctions d'agrégat, pour
calculer une somme, une moyenne, un maximum ou un minimum, ou encore compter un
ensemble de valeurs. Elles vont servir par exemple à trouver la station offrant la plus grande
capacité d’accueil, à déterminer la capacité moyenne des stations françaises, etc.

Les fonctions d'agrégat correspondent à des prédicats généraux de la langue : nous avons
développé à ce sujet un certain nombre d'exemples dans (Bouchou et al., 1999). Il y a
également des fonctions programmées par le concepteur de la base : par exemple dans une
base d’inventaire de marchandise, il pourra y avoir une fonction pour calculer le stock
(produits reçus - produits vendus) (Bouchou, Maurel, 1999). Qu’elles soient prédéfinies ou
programmées, on peut trouver les fonctions d’une base dans de ses « méta-informations ».

En résumé, les informations que nous récupérons dans la base sont :

•   les données : le contenu des tables,
•   la structure générale : les noms des tables, les noms des colonnes,
•   les liens entre les données : les dépendances (df et di), ainsi que les fonctions de calcul.
3.       Le "sens" des données pour l'utilisateur qui interroge la base

3.1       Questions sur les « entités »
C’est un fait établi en IHM que tout utilisateur se forge un modèle mental de l’application
informatique qu’il utilise (cf. par exemple l'
ouvrage ( Norman, 1986)). Ainsi l’utilisateur de la

L'interrogation de bases de données comme application des classes d'objets
base a son propre modèle du système d'information qu'elle représente lorsqu’il l’interroge
(plus ou moins clair, complet, correct vis-à-vis de l'implantation effective de la base…).

En particulier, il en imagine les « entités » : c’est à leur propos qu’il va poser ses questions,
lesquelles vont donc contenir des références à ces entités. Nous devons donc savoir comment
l'utilisateur va faire référence à l’entité « skieur ». Il va peut être utiliser le mot skieur :

Quels skieurs participèrent au Championnat d'Europe 2000 ?

Pour le même genre de question, il peut aussi utiliser un terme représentant des compétiteurs,
comme concurrent par exemple. Enfin, pour parler d'une occurrence particulière de skieur, il
utilisera essentiellement les noms et prénoms de la personne :

Quelles compétitions ont été remportées par Franck Picard ?

Nous posons une définition pour prendre en compte cette dernière possibilité : nous appelons
l'ensemble des valeurs caractéristiques utilisées dans la question pour parler d'une occurrence
précise d'une entité le « représentant » de l’entité. Cela se traduit dans la base par un attribut,
ou un ensemble d’attributs d’une table. Seules les tables qui correspondent à des entités ont
un représentant1. Dans notre exemple, une station est représentée par son nom, une
compétition par son nom et sa date, un skieur par son nom (qui regroupe nom et prénom).

Cette notion de représentant est nécessaire pour repérer dans la question toute référence à une
table, soit directe, par ce qu'elle représente (« la station qui… » fait référence à la table
Station), soit par une valeur de son représentant (« la capacité d’accueil de St Moritz » fait
également référence à la table Station).
3.2      Questions sur les caractéristiques des entités
L’interrogation porte donc sur les tables qui correspondent aux entités que peut imaginer
l'
utilisateur. Plus précisément, elle porte sur l’une ou l’autre des caractéristiques des entités.

•     Dans notre exemple, on interrogera sur le pays d'une station, sa capacité, la spécialité d'un
skieur, la date d'une compétition, etc. On en déduit ainsi que chaque attribut d’une table T
correspond à une des caractéristiques possibles de l’entité représentée par T.

Formellement, il y a dépendance fonctionnelle (df) entre chaque attribut non-clef et la clef de
la table ; nous identifions la table (et son représentant) à sa clef. Ainsi, connaissant les df,
nous connaissons une première partie des caractéristiques de l'entité qui pourront être
interrogées.

•   Reprenons encore notre exemple : d’autres caractéristiques de l’entité sont stockées en
dehors de la table : le pays qui accueille la compétition, ou la performance (le rang)
d’un skieur dans une compétition.

Ces caractéristiques-là sont "rapportées" par les dépendances d'inclusion (di).
1
Parenthèse technique : cet ensemble d’attributs est souvent « clef » pour la table, mais c’est rarement la clef
primaire effective. Pour l'
instant, ce représentant doit encore être identifié par l’installateur.

Béatrice Bouchou, Julien Lerat, et Denis Maurel
•   Enfin, il y a les caractéristiques qui ne sont pas stockées, mais calculées, à savoir les
fonctions : le gagnant d'une compétition, par exemple, s'obtient par un MIN sur le rang.

Ainsi les images dans la base des caractéristiques interrogées s’expriment en termes de df, di
et fonctions, lesquelles mettent en relation tables, attributs, et valeurs.
4.         Projection de la question sur la base

4.1         Classes d'objets et prédicats appropriés pour représenter la question
Nous sélectionnons dans la question un certain nombre de mots qui fournissent assez
d’informations pour construire une requête SQL pertinente vis-à-vis de la base. Pour cela, il
faut non seulement les mots, mais leurs liens, soit encore l'     ensemble des conditions qu'    ils
doivent remplir pour faire sens les uns avec les autres dans la question, ceci dans le contexte
de la base. Ces liens, les travaux de Zellig Harris, puis de Maurice Gross (LADL) et, enfin, de
Gaston Gross (LLI) montrent qu'    ils sont donnés par les prédicats linguistiques : des opérateurs
sur les phrases simples, dotés d'arguments (le sujet, les compléments)...

Les classes d’objets sont présentées par leurs auteurs comme « des classes sémantiques
construites à partir de critères syntaxiques »2 (Gross, 1998), (Le Pesant, Mathieu-Colas,1998),
(Le Pesant, 2000). Il n'est pas dans notre propos de détailler leur cadre théorique, élaboré au
LLI depuis une dizaine d'années, aussi nous ne faisons que rappeler leur définition. Une
classe d’objets est définie comme représentant le « type » d'un argument de prédicat : en
d'autres termes, le prédicat sélectionne son argument dans telle ou telle classe d'objets. Ainsi,
on peut décrire le sens d'un mot prédicatif (verbe, nom, adjectif, ou adverbe) en indiquant les
classes d'objets qu'il sélectionne. Et inversement, une classe d'objets est définie (en partie) par
les prédicats qui lui sont spécifiques : ce sont ses prédicats appropriés. Cela permet de décrire
le sens d'un mot non prédicatif par l'énoncé des classes d'objets auxquelles il appartient.

Par exemple, compétition appartient à la classe des <événements> : ce mot est sélectionné par
« se dérouler », « être organisé », « avoir lieu », etc.

(i)       N0 a eu lieu à N1 le N2         ici N1 est de la classe des <toponymes> (auquel
appartiennent les stations de ski, mais également les pays), et N2 de la classe des dates.

(ii)       De même, le mot skieur, de la classe des <humains> (à ce titre il a un nom, donc il
peut être sélectionné par « être nommé », « avoir pour nom »,…) est aussi de la (sous-)
classe des <sportifs>, ce qui le rend sélectionnable par :

(iii)       N0 a pour spécialité N1 ; la spécialité de N0 est N1.

(iv)       Il est également dans la classe des <compétiteurs>, qualifiée par « être vainqueur »,
« perdre », « se classer », « avoir pour rang », etc.

(v)        Le mot station relève (entre autres) de la classe des <villes>, sélectionnées en
particulier par : N0 est dans <pays>, N0 en <pays>, N0 de <pays>. Il appartient
2
C'est en ceci nous semble-t-il que la démarche du LLI diffère de celle du DEC de Mel'cuk (Mel'cuk, 1995),
dans lequel la plupart des propriétés lexicographiques d'une lexie découlent des définitions sémantiques.

L'interrogation de bases de données comme application des classes d'objets
également aux lieux géographiques, caractérisés entre autres par leur altitude : N0 se situe
à <altitude>.

Dans les faits, les classes d'objets sont définies en extension, par l'ensemble des mots qui les
composent, et l'ensemble de leurs prédicats appropriés (verbes, adjectifs, noms ou adverbe).
Chaque mot est lui-même associé à des caractéristiques lexico-sémantiques : genre, trait,
domaine, etc. Les prédicats sont décrits également par leurs caractéristiques : catégorie
grammaticale, classes des arguments , trait, domaine, etc.
4.2      L'image dans la base des classes d'objets et prédicats appropriés
Les classes d'objets nous servent à construire le dictionnaire, en même temps que les
informations issues de la base de données : ce dictionnaire met en relation des mots avec des
ensembles de codes, comme le schématise la figure 3.

mot                  DICTIONNAIRE                    {codes}

Figure 3 : À un mot correspond une liste de codes

Quels sont les mots en entrée du dictionnaire, mots qu'il faudra reconnaître dans la question ?
C'est la base de données qui détermine ce vocabulaire : on doit pouvoir reconnaître :

•   Toute référence aux entités (tables).

•   Toute référence aux instances (valeurs stockées).

•   Toute référence aux caractéristiques de telle ou telle entité (relations entre attributs
d'une table –df–, ou relations entre attributs de différentes tables –di–, ou fonctions).

Reprenons les exemples (i) à (v) précédents :

•   Les mots qui parlent d'une table appartiennent à une (ou plusieurs3) classe(s) d'objets
linguistiques. De même, les valeurs (textuelles) prises par un attribut appartiennent à
une (ou plusieurs) classe(s) d'objets : St Moritz, Genève ou Lausanne sont des éléments
de la classe des noms de villes.

•   Considérons maintenant les attributs d'une table T : chacun correspond à une
caractéristique de l'entité représentée (même partiellement) par T. Chacun représente
donc un mot prédicatif, qui sélectionne au moins deux arguments : le premier dans la
classe d'objets de la table, le deuxième dans la classe d'objets des valeurs prises par
l'attribut. Voyez par exemple (i) : N0 a lieu à N1, ou encore (iii) et (iv).

•   Lorsqu'une entité est représentée par un ensemble de tables, les attributs d'une table T1
qui dépendent des valeurs d'une table T2 correspondent également à des mots
prédicatifs, lesquels sélectionnent un argument dans la classe d'objets de la table T2, et
les autres dans les classes des valeurs prises par l'attribut de T1. Ainsi la relation entre
3
Du fait de la hiérarchie entre classes d'objets, un mot peut apparaître dans une classe et une sous-classe.

Béatrice Bouchou, Julien Lerat, et Denis Maurel
l’attribut pays de la table Station avec la table Compétition représente la forme N0 a
lieu en N1 : le championnat d’Europe a lieu en Autriche.

•   Par ailleurs les fonctions (nommées selon le « sens » de leur résultat : minimum,
somme, stock, vainqueur…, et caractérisées par leurs arguments et leur type) sont, elles
aussi, référencées dans la langue par des mots prédicatifs. Par exemple la fonction qui
calcule un minimum représente la forme superlative le moins, ou le plus petit :
l'altitude la moins élevée, la plus petite capacité d'accueil.

Ainsi les informations (valeurs stockées) et les méta-informations (tables, attributs,
contraintes et fonctions) de la base de données conditionnent l'ensemble des mots à
reconnaître dans la question. À partir de ces conditions posées par la base, notre démarche
consiste ensuite à consulter la langue, structurée en classes d'objets et de prédicats, pour
déterminer les mots qui seront utilisés pour l'interrogation.

Spécifions maintenant les codes à associer aux mots du dictionnaire. Un mot est en entrée du
dictionnaire en tant qu'élément d'une classe d'objet et/ou élément d'une classe de prédicats.

•   Si c'est un élément d'une classe d'objets, alors c'est une référence soit à une table, soit à
une valeur d'un attribut d'une table4. Donc au moment où on ajoute le mot au
dictionnaire, on connaît la table ou la valeur d'attribut en question : on associe alors à
ce mot une référence à cette table (code T.nom_interne_de_la_table), ou à cette valeur
(de cet attribut de cette table : code I.nom_de_la_table.nom_ de_l’attribut.valeur).

•   Si c'est un mot prédicatif, alors il est en entrée du dictionnaire du fait d'une df, d'une di,
ou d'une fonction. De la même façon on peut lui associer un code qui représente cette
information dans la base. Par exemple, « se déroule » est associé au code
df(T.Compétition, A.Compétition.nomStat) au moment où cette df est traitée, puis au
code di(T.Compétition, A.Station.paysStat) lorsque cette di est traitée à son tour. Ce
terme, « se déroule », est donc en entrée du dictionnaire du fait des relations df et di,
ET grâce aux classes d'objets et de prédicats associés, qui fournissent les mots de la
langue5 qui expriment ces relations.

On voit donc qu'à chaque mot est associée une liste de codes, ne serait-ce que parce qu'il est
rare qu'un mot n'ait qu'un emploi, même dans un certain domaine. Le traitement d'une
question par le premier transducteur (qui applique le dictionnaire) génère une liste de listes de
codes. L'essentiel du traitement ultérieur consiste à mettre en regard les sous-listes : par
exemple, s'il y a le code df(T.Compétition, A.Compétition.nomStat) dans une sous-liste S1, et
s’il y a le code T.Compétition dans une sous-liste S2 et enfin le code A.Compétition.nomStat
dans une sous-liste S3, alors df(T.Compétition, A.Compétition.nomStat) est retenu, et ce code
n'est retenu qu'à cette condition : nous ne retenons que les dépendances et fonctions
« instanciées » par une partie ou une autre de la question.

Bien évidemment, les principes qui viennent d'être décrits ne sont applicables qu'en cas
d'attributs textuels : les booléens, les nombres et les dates ne sauraient se trouver en entrée du
dictionnaire, et portent peu de sémantique en eux-mêmes. Ces types d'attributs font l'objet
d'un traitement distinct que nous n'aborderons pas ici.
4
Table ou valeur qui sont à l'origine de la présence de ce mot en entrée du dictionnaire.
5
Et les règles qui régissent leur emploi, donc leur sens…

L'interrogation de bases de données comme application des classes d'objets
5.       Construction du dictionnaire avec les classes d’objets
Dans la phase d’installation, il faut que les choses soient simples. Dans (Bouchou et al.,
1999), il est montré que notre système fait grosso-modo ce que le système TEAM faisait déjà
en 1986 avec une approche de type intelligence artificielle (Grosz, 1986). Mais TEAM
demandait trop d’efforts et de compétences pour son adaptation à chaque nouvelle base. Pour
nous, compte-tenu de ce que nous extraions de la base elle-même (dont les dépendances et les
fonctions), et de la connaissance de la langue fournie par les classes d’objets, il faut peu
d’efforts et peu de temps à l’installateur. Voici quelques points qui tendent vers cet objectif :

Au départ, l'
installateur donne :

•   un mot de la langue courante pour désigner chaque table (le nom interne des tables est
rarement parlant),

•   le « représentant » des tables qui correspondent à des entités.

Pour chaque nom de table donné, T, les classes d’objets auxquelles il appartient sont
recherchées, et soumises à l’installateur pour qu’il désigne la plus pertinente, CoT6.

Par ailleurs, les classes d’objets auxquelles correspondent les valeurs de chaque attribut –
textuel– de chaque table sont également recherchées. Chaque attribut Ai de T correspond à une
colonne de valeurs : on recherche la plus petite classe d’objets qui contienne toutes ces
valeurs. Nous avons ici besoin de la hiérarchie des classes : si au plus petit niveau (le plus
précis) aucune classe ne contient toutes les valeurs de la colonne, alors il faut sélectionner une
classe d'un niveau supérieur. Soit CoT.Ai la classe ainsi déterminée pour l' attribut Ai de T.

Des techniques similaires sont utilisées pour les df, di et les fonctions : ainsi pour les df, on
recherche la classe de prédicats à laquelle correspond un attribut Ai de la table T. Il s'agit de
l’ensemble des prédicats qui sélectionnent à la fois un élément de la classe CoT et un élément
de la classe CoT.Ai. On note cet ensemble : CpT.Ai = {P/(CoT, CoT.Ai , P)}.

Grâce à de tels calculs, opérés automatiquement sur les fichiers qui décrivent les classes
d'objets et les prédicats, l'installateur, après avoir « amorcé » l'installation avec un nom pour
chaque table, n'a plus qu'un simple rôle de sélection ou de validation.
6.       Bilan et perspectives
Nous ne pensons pas avoir épuisé les ressources offertes par les classes d'objets et prédicats
associés. Les techniques qui viennent d'être présentées sont implantées dans notre prototype,
développé en Java. Celui-ci est testé sur plusieurs bases, de domaines différents (la gestion de
stock, les vins, …), et l'expérience mérite d'être prolongée pour intégrer des propositions
concernant l'organisation de la classification, comme l'usage des domaines et sous-domaines.

Une perspective importante ouverte par ce travail, pour l'instant focalisé sur les bases de
données relationnelles, consiste à transposer les réflexions qui le sous-tendent vers des
6
Les classes sont "soumises" à travers un échantillon de leurs prédicats appropriés.

Béatrice Bouchou, Julien Lerat, et Denis Maurel
collections de données semi structurées, par exemple en XML, et à ouvrir notre application,
de ce fait, à la recherche d'informations sur le Web.

Remerciements
Les auteurs tiennent à remercier M. Gaston Gross pour leur avoir permis de tester les idées
présentées ici sur des fichiers de classes d'objets et de prédicats développés au LLI.
Références
Abiteboul S., Hull V., Vianu V. (1995), Foundations of Databases, Addison-Wesley.
Bouchou B., Maurel D. (1999), Une bibliothèque d’opérateurs linguistiques pour la
consultation de base de données en langue naturelle, 6ème Conférence sur le Traitement
Automatique des Langues naturelles (TALN 1999), Cargèse, actes pp 65-74.
Bouchou B., Maurel D., Kaltz B. (1999), Prédicats logiques / prédicats linguistiques pour la
consultation de base de données en langue naturelle, RISSH, U. de Liège, Vol.35, pp 127-149.
Gross G. (1994), Classes d’objets et description des verbes, Langages 115, Larousse.
Gross G. (1998), Pour une véritable fonction synonymie dans un traitement de texte,
Langages 131, Larousse.
Gross M. (1975), Méthodes en syntaxe, Hermann, Paris.
Grosz B., Appelt D., Martin P., Peirera F.(1987), TEAM : an experiment in the design of
transportable natural language interfaces, Artificial Intelligence 32, pp 173-243.
Harris Z.S. (1976), Notes du cours de syntaxe, Paris, Le Seuil.
Kaplan S. J. (1984), Designing a Portable Natural Language Database Query System, ACM
Transactions on Database Systems, vol. 9, n°1, march 1984, pp. 1-19.
Le Pesant D., Mathieu-Colas M. (1998), Introduction aux classes d’objets, Langages 131,
Larousse.
Le Pesant D. (2000), Introduction aux classes d’objets, Thèse d'habilitation à diriger des
recherches, Université de Villetaneuse, décembre 2000.
Mel'cuk I. A., Clas A., Polguere A. (1995), Introduction à la lexicologie explicative et
combinatoire, "champs linguistiques", Louvain La Neuve, Duculot.
Mihov S., Maurel D. (2000), Direct Construction of Minimal Acyclic Subsequential
Transducers, First Conference on Implementing and Application of Automata (CIAA’2000).
Norman, D. A., Draper, S. (Eds.), (1986). User Centered System Design: New Perspectives
on Human-Computer Interaction. Hillsdale, NJ: Lawrence Erlbaum Associates.
Novelli N., Cicchetti R. (2000), Mining functional and embedded dependencies using free
sets, 16ème Conférence Bases de Données Avancées (BDA 2000), Blois.
Pasero R., Sabatier P. (1999), ILLICO : un système générique pour la compréhension d'un
sous-ensemble du français, rapport Laboratoire d'Informatique de Marseille, 1999.
Revuz D. (1991), Dictionnaires et lexiques - Méthodes et algorithmes, Thèse de Doctorat en
Informatique (Université Paris7).
Sabah G. (1997), Le sens dans le traitement automatique des langues, T.A.L. 38-2, pp. 91-133.

Elaboration d'une cascade dc transducteurs
pour l'extraction des noms dc personnes
dans les textes

Nathalie Friburger, Denis Maurel

Laboratoire d'Informatique de Tours
64 avenue Jean Portalis, 37000 Tours
{friburger, maurel} @univ-tours.fr

Résumé — Abstract

Cet article décrit une cascade de transducteurs pour l'extraction de noms propres dans des
textes. Apres une phase de pré-traitement (découpage du texte en phrases, étiquetage a l'aide
de dictionnaires), une série de transducteurs sont appliqués les uns apres les autres sur le texte
et permettent de repérer, dans les contextes gauches et droits des éléments "déclencheurs" qui
signalent la presence d'un nom de personne. Une evaluation sur un corpus journalistique
(journal Le Monde) fait apparaitre un taux de precision de 98,7% pour un taux de rappel de
91,9%.

This article describes a finite-state cascade for proper nouns extraction in texts. After a
preprocessing (division of the text in sentences, tagging with dictionaries, etc.), a series of
finite state transducers cascade is applied one after the other to the text and locate left and
right contexts which indicate presence of a person name. An evaluation on a journalistic
corpus (Le Monde) gives a rate of precision of 98,7% for a rate of recall of 91,9 %.

Mots clefs — keywords
Transducteur, noms propres, extraction de motifs

Transducer, proper nouns, pattern extraction

1 Introduction

Les automates a nombre fmi d'états, et tout particulierement les transducteurs, sont de plus en
plus utilises pour le traitement automatique des langues [Roche, Schabes, 1997]. Dans cet
article, nous proposons d'utiliser des transducteurs en cascade pour localiser les noms propres
[Coates-Stephens, 1993] dans les textes joumalistiques. Ce type de textes a déja été étudié
dans de nombreux travaux depuis 1e systeme Frump [Dejong, 1982] jusqu'aux programmes
américains Tipster et MUC d'évaluation des systemes d'extraction d'information, mais 1e

Nathalie Friburger, Denis Maurel

probleme de la détection des noms propres reste largement non résolu. Les inforrnations
extraites s'intégrerons dans un travail de classiﬁcation de textes a partir de noms propres.

Nous décrirons tout d'abord une phase de pré-traitement nécessaire des textes (découpage du
texte en phrases, étiquetage a l'aide de dictionnaires), puis nous décrivons en détail les
transducteurs utilisés. Enﬁn, nous présenterons les résultats d'une evaluation sur un corpus
d'environ 165 000 mots. Nous discuterons des principales difﬁcultés rencontrées et des
problemes qui restent a résoudre.

2 Pré-traitements des textes

Avant d'appliquer la cascade de transducteurs sur un texte, nous le soumettons a un certain
nombre de pré-traitements aﬁn d'améliorer les résultats ultérieurs. Le texte est tout d'abord
découpé en phrases [Friburger et al. , 2000] (ce qui permet d'éliminer l'ambigu'l'té du point de
fin de phrase avec les noms propres en contenant), puis étiqueté du point de vue morpho-
syntaxique. Nous pourrions utiliser un étiqueteur tel que celui de [Brill, 1992] qui donne des
résultats corrects a plus de 95% ou le systeme Cordiall qui offre des résultats encore
meilleurs. Nous avons préféré utiliser l'étiquetage par les dictionnaires du systeme Intex
[Silberztein, 1993]. Les mots sont étiquetés par toutes leurs formes présentes dans tous les
dictionnaires utilisés sans désambiguation. Parmi les différentes étiquettes d'un méme mot, la
bonne etiquette est la plupart du temps présente ce qui évite un mauvais étiquetage. Ce
mauvais étiquetage peut en effet géner et diminuer les performances d'un systeme [Morin,
1999].

Nous utilisons des dictionnaires qui contiennent les mots (lemmes et formes ﬂéchies), ainsi
que des informations grammaticales (nom, verbe, etc.) et sémantiques (humain, concret,
toponymes, prénoms, sigles, etc.).

—Delas : dictionnaire des mots simples et de leurs formes ﬂéchies [Courtois, Silberztein,
1990]

—Prolintex : dictionnaire de toponymes réalisés dans le cadre du projet Prolex [Piton,
Maurel, 1997]

—Prenom-prolex : Dictionnaire de prénoms (plus de 6500 entrées) et Sigles-prolex :
Dictionnaire de sigles avec leurs extensions (environ 3300 entrées)2

—Dictionnaire des professions3

L'avantage de ces dictionnaires est double :

— Chaque mot est donné avec sa forme lemmatisée, ce qui permet de ne pas avoir a
décrire toutes les ﬂexions des mots dans les transducteurs pour les détecter.
1 Propose’ par Synapse De’veloppement

2 Ces dictionnaires ont été cre’e au Laboratoire d'Informatique de Tours dans le cadre du projet Prolex de
dictionnaire des noms propres.

3 Dictionnaire e’labore’ par Ce’dric Fairon, LADL [Fairon, 2000].

Elaboration d'une cascade de transducteurs pour l'extraction de motifs

— Les dictionnaires utilisés contiennent des informations syntaxiques (Nom, verbe,
pronom, etc.) et sémantiques (humain, prénom, toponyme, etc.) qui peuvent aider a
la localisation des motifs de noms propres.

3 Une cascade de transducteurs pour l'extraction des noms de
personnes

3.1 Les transducteurs

Les transducteurs sont des automates qui possedent un alphabet d'entrée et un alphabet de
sortie : c'est cette propriété que nous pouvons utiliser pour extraire des motifs et les
catégoriser. L'alphabet d'entrée contient les motifs que l'on souhaite repérer dans les textes
tandis que l'alphabet de sortie contient, dans notre cas, des informations balisés dans un
langage inspiré de XML. Les motifs que nous recherchons sont les noms propres ainsi qu'une
partie de leurs contextes lorsqu'ils sont exploitables et repérables. Voici un exemple de nom
de personne trouVé dans un texte et balisé par le transducteur qui l'a repéré :

Le Juge Renaud Van Ruymbeke 9 <profession> juge <\profession> <person> <prenom>
Renaud <\prenom> <nom> Van Ruymbeke<\nom> <\person>.

Le systeme Intex nous permet d'exploiter les transducteurs sur des textes. Nous avons réalisé
un programme qui complete les possibilités d'Intex et permet de générer une cascade de
transducteurs.

Dans de nombreuses applications, les transducteurs sont utilisés en cascade. Cette technique
peut étre utilisée pour réaliser l'étiquetage syntaxique d'un texte (comme le font Xerox ou le
systeme Fastus [Hobbs et al., 1996]). La cascade est basée sur une idée simple : passer les
transducteurs sur le texte dans un ordre précis pour le transformer ou en extraire des motifs.
Chaque motif que nous découvrons est remplacé dans le texte par une etiquette qui nous
permet de le retrouver dans un index. Nous éliminons les motifs découverts du texte pour
éViter qu'un transducteur passé ultérieurement ne les extraie a nouveau.

3.2 Etude des formes présentes dans les contextes droit et gauche des noms
de personnes

Avant de créer la cascade, nous avons fait un inventaire des formes existant en contexte droit
et gauche des noms de personnes dans les articles de journaux. En effet, ce sont les contextes
qui aident a repérer les noms propres et en particulier une grande majorité de noms de
personnes. Nous avons remarqué que le contexte gauche permet de détecter plus de 90% des
noms de personnes dans les textes de style journalistique : ceci est certainement dﬁ a des
impératifs stylistiques propres a ce type de textes qui se veulent la plupart du temps obj ectif et
qui doivent décrire au mieux les faits. Une étude sur un texte tirés du journal Le Monde
d'enViron 165000 mots nous a permis de déterminer les categories de contextes les plus
frequents.

Nathalie Friburger, Denis Maurel

Cas l : 25,9% des noms de personne sont précédés d'un contexte contenant un titre ou un
nom de profession suiVis d'un prénom et d'un patronyme.
Ex : M Jean-Pierre Soisson déclarait .'  Ce regain de violence a coincide avec la visite
oﬁ‘icielle du présidentpe’ruvien Alberto F ujimori en Equateur, qui a pris ﬁn samedi.

Cas 2 : l9,l% des noms de personnes sont précédés d'un contexte déclencheur contenant
un titre ou un nom de profession suiVi, soit d'un patronyme seul, soit d'un prénom inconnu
(i.e. absent de notre dictionnaire des prénoms) et d'un patronyme. Ex :  et qui qualiﬁait
la démission du president Chadli d'" événement important et loura’ de consequences ",

Cas 3 : C'est le cas le plus fréquent. 43,4% des noms de personnes n'ont pas de contextes
descriptibles mais sont composes du prénom de la personne (connu de notre dictionnaire)
et suiVi du nom de la personne. Ex : Pierre Bourdieu est sans conteste l'une des ﬁgures
majeures de la sociologie c0ntemp0raine.

Cas 4 : 5,2% des formes sont repérables grace a la présence d'un verbe utilisé pour
designer une action mettant en jeu une personne (dire, expliquer, etc.) comme dans
"Wieviorka est décéa’é, le 28 décembre, a Paris " ou "Jelev a a’itll ou par la présence d'un
titre ou d'une profession en apposition (contexte droit) ex : "... Jospin, premier Ministre
". Cependant des verbe comme dire, expliquer  peuvent étre employés avec un sujet
non humain, des indices tels que ceux ci sont a utiliser avec une tres grande prudence.

Cas 5 : Les 6,4% de noms de personnes restants n'ont aucun contextes, méme complexes
qui puissent les distinguer a coup sﬁr d'autres noms propres. Ces noms de personnes sans
contextes (pas méme un prénom qui pourrait les distinguer d'autres noms propres) sont
principalement des noms de personnes tres connues pour lesquels l'auteur du texte estime
qu'il n'est pas nécessaire de préciser le prénom ni le titre ou la profession, ex : Picasso
n'est pas le premier a passer a la posterité commerciale. Cependant nous avons remarqué
que 49% de ces noms de personnes restants sont détectables en réalisant une seconde
passe dans laquelle on recherche les patronymes qui ont été découvert un autre endroit du
texte par un des transducteurs. Ce qui réduit a 33% 1e nombre de formes indétectables. Ce
pourcentage peut étre encore réduit en créant un dictionnaire des noms de personnes
celebres.

La méme étude a été menée sur des articles du journal Ouest France (67 000 mots environ).

Les résultats sont présentés dans le Tableau l.
Cas l : 17,1%
Cas 2 : 16,3%
Cas3:59%
Cas 4 : 2,2%
cas 5 : 5,4%
Tableau l : Proportion des différents cas sur le journal Ouest France

On remarque que le cas 3 est le plus commun et est bien plus fréquent dans Ouest France que
dans le journal Le Monde. Par contre, on peut remarquer que le cas l (nom précédé d'un titre
ou d'une profession) chute de 259% dans Le Monde a 17,1% dans Ouest France :

certainement dﬁ a un souci de rigueur du journal Le Monde.
ceci est

Elaboration d'une cascade de transducteurs pour l'extraction de motifs

3.3 Etude combinatoire des différentes formes de noms de personnes

Nous avons aussi étudié quelles formes pouvaient prendre les noms de personnes. On trouve
en majorité des prénoms suiVis d'un patronyme ou des noms seuls. Comme l'ont déja
remarqué aussi [Kim, Evens, 1996], l'auteur d'un article de journal donne en général une
premiere fois la forme complete du nom de personne, puis des formes abrégées ; c'est

pourquoi la majorité des noms de personnes trouvés le sont assez souvent avec leur prénom et
leur nom.

Les formes de prénoms a reconnaitre sont les suivantes :

— prénoms simples, ex : Jean,

— prénoms composes, ex : Jean-Pierre, Charles Edouard, ou en partie abrégés ex :
Pierre-J

— prénoms abrégés simples, ex : J pour Jean, 771. pour leierry,

— prénoms composes abrégés, ex : JR, J.-P., J-P, J-P.

— prénoms composes en partie inconnus : prénom compose dont une des parties est dans
le dictionnaire des prénoms et l'autre est inconnue.

La reconnaissance des prénoms se fait sur la base d'indices morphologiques ainsi que sur le
dictionnaire des prénoms. Les prénoms totalement inconnus du dictionnaire ne sont pas
reconnus, ils font alors partie intégrante du nom de personne. Nous avons fait l'hypothese que
les personnes sont plus souvent citées par les journalistes en donnant d'abord le prénom puis
le nom. Cette regle n'est éVidemment pas absolue, puisque, si dans le corpus étudié du journal
Le Monde aucun forme nom-prénom n'a été détectée, nous avons observe l'ordre nom-prénom
dans 3% des noms de personnes des articles étudiés dans le journal Ouest France.

Les différentes formes de patronymes sont les suivantes :

— Patronymes "composes" (surtout des noms d'origine étrangere),
ex :Mac Donald, Mac Donnell-Douglas, O'Ryan, L'Huillier, Le Falch'un, von Bulow, El
Amra, Da Silva

— Patronymes "simples" (composes d'un ou plusieurs mots commencant par une majuscule),
ex : Dupont, Durand-Pérec

— Patronymes francais a particules, ex : Dupont de Nemours, de Neuville, de la Fontaine

Nous avons distingué les patronymes francais a particules des patronymes composés car la
particule francaise (de, du) est tres ambigué dans les textes avec la préposition de ce qui n'est
pas le cas pour les particules étrangeres. Nous devrons donc tenir compte de ces differences
dans l'ordre de passage des transducteurs.

Les contextes déclencheurs qui décrivent le majorité des contextes gauches sont simplement
les civilités (ex: Mme, Monsieur, etc.), les titres de toutes sortes : politiques (ex : ministre,
député, etc.), militaires (ex : general, lieutenant, etc.) religieux (ex : cardinal, évéque, etc.),
administratifs (ex .' inspecteur, agent, etc.)  ainsi que les noms de professions (ex : le juge,
l'architecte, etc.). Les noms de professions sont les termes déclencheurs les moins frequents.
Le dictionnaire des toponymes permet de repérer les adjectifs de nationalités dans des
expressions telles que "le president américain Clinton", "l'allemandHelmut Kohl ".

Nathalie Friburger, Denis Maurel

3.4 Description de la cascade de transducteurs
Ordre Recherche
de passage de
des contextes Pre’noms Patronymes Exemples
transducteurs gauches
de’clencheurs
(oui/non)
l Oui Pre’noms compose’s et abre’ge’s Palronymes compose’s Ex : le président Richard
Pre’noms simples Palronymes a parlicules von Weizsécker
2 Oui Pre’noms compose’s et abre’ge’s Palronymes simples Ex : M J.—P. de F onsac
Palronymes a parlicules
3 Oui Pre’noms compose’s en partie Palronymes compose’s Pas d'exemple
inconnus
4 Oui Pre’noms compose’s en partie Palronymes simples Ex :Roger—Pol Droit (Pol
inconnus n 'était pas dans notre
dictionnaire de prénoms)
5 Oui Pre'noms simples Patronymes simples Ex :M Guy Fleury
6 Oui Patronymes compose’s Ex :M Strauss—Kuhn
7 Oui sans les Palronymes a parlicules Ex : M de Neuville
professions et
les titres.
84 Oui <CNP>5 <CNP> - <CNP>6 Ex : M Amnon Lipkin—
<CNP> <CNP> <CNP> Shahak
9 Oui <CNP> <CNP> Ex : le général Veljko
Kadijevic
10 Oui Patronymes simples Ex :Mme Bouchardeau
ll Interdit les Pre’noms compose’s et abre’ge’s Palronymes compose’s Ex Philippine Leroy—
de’terminants Pre’noms simples Beaulieu
avant le nom Ex .' Maria da Graca
de personne Meneghel
12 Interdit les Pre’noms compose’s et abre’ge’s Palronymes simples Ex : P. Bourdieu
determinants Pre’noms simples
avant le nom
de personne
l3 Interdit les Pre’noms compose’s en partie Palronymes compose’s Pas d'exemple
determinants inconnus
avant le nom
de personne
l4 Interdit les Pre’noms compose’s en partie Palronymes simples Pas d'exemple
determinants inconnus
avant le nom
de personne
Tableau 2 : Description d'une partie des transducteurs reconnaissant les noms de personnes
(cas l, 2 et 3)
4 Les transducteurs 8 et 9 permettent de reconnaitre des formes compose’es de mots commencant par une
majuscule sans pre’nom connu et pre'ce’de’es de de’clencheurs de la pre’sence de noms de personne.

5 <CNP> est l'e’tiquette qui de’signe un candidat nom propre, i.e. un mot qui commence par une lettre majuscule.

6 <CNP>-<CNP> de’signe 2 candidats noms propres se’pare’s d'un tiret.
Elaboration d'une cascade de transducteurs pour l'extraction de motifs

D'apres les différentes constatations faites lors de notre étude des formes des noms de
personnes et de leur contexte, nous avons défini une cascade de transducteurs dans laquelle
nous prenons en compte les impératifs liés aux transducteurs eux mémes.

Nous avons donné priorité aux motifs les plus longs afin de repérer les noms entiers. Par
exemple, si nous passons un transducteur qui reconnait M suiVi d'un mot commencant par
une majuscule avant le transducteur qui reconnaitM suiVi d'un prénom puis d'un nom, et que
l'on a un texte contenant le motif M Jean Dupont, on découvre le motif <person> <nom>
Jean Dupont </nom> </person> au lieu du motif <person> <prénom> Jean <prénom>
<nom> Dupont </nom> </person>.

Le Tableau 2 décrit la partie la plus importante de la cascade de transducteurs : c'est-a-dire la
description des cas l, 2 et 3 présentés dans la section 3.2. Chaque transducteur est numéroté
dans son ordre de passage.

Exemple de lecture du tableau .'
Le transducteur l reconnait des noms de personnes composés par:
— un prénom simple, composé ou abrégé,
— puis un patronyme composé ou a particule (ex : 0’Reilly, de La Fontaine),
— et précédé d'un mot déclencheur.

Avant de passer les transducteurs qui reperent les noms de personnes sans mots déclencheurs
(juste avec les prénoms), il faut passer les transducteurs qui détectent les noms d'association
afin d'éViter des problemes tels que ceux présentés dans la partie 4, car les transducteurs qui
reconnaissent des noms de personnes sont ambigus avec d'autres transducteurs. Par exemple,
le motif "L’association Hugues Aircraft" risque d'étre découvert comme un nom de personne a
cause de la présence de Hugues dans le dictionnaire des prénoms : le graphe l4 reconnait ce
motif.

3.5 Evaluation

Voici maintenant un exemple de résultats obtenus sur un extrait d'article du journal Le Monde
n°l6l9. Chaque motif trouVé est remplacé par une etiquette qui porte le nom du transducteur
qui l'a trouVé et contient la position du motif repéré dans un fichier dans lequel il est indexé.

Le texte initial est le suivant7 :

Invite a un séminaire sur la crise des fusées de 1962 organise a La Havane, M
Robert McNamara, qui fut le secrétaire a la defense du president Kennedy, a
estimé de son cote que les deux pays devaient normaliser leurs relations, minées
depuis trente ans par " la peur et l'hostilité ". {S} Au cours de ce séminaire, le
president cubain Fidel Castro a révélé que l'URSS avait déployé en 1962 trente-
six ogives nucléaires a Cuba, dont neufavaient été installées sur des missiles. {S}

Les amorces de noms propres sont ensuite repérées par la cascade de transducteurs. Le texte
deVient alors :
7 Les symboles {S}, pre’sents dans le texte, signalent les marques de separation de phrases
Nathalie Friburger, Denis Maurel

Invite a un séminaire sur la crise des fusées de 1962 organise a La Havane,
<$titCiv.'1358$> Robert McNamara, qui fut le secrétaire a la defense du
<$titPolit:7035$> Kennedy, a estimé de son co‘té que les deux pays devaient
normaliser leurs relations, minées depuis trente ans par " la peur et l'hostilité
". {S} Au cours de ce séminaire, <$titPolit.'1375$> Fidel Castro a révélé que
l'URSS avait déployé en 1962 trente-six ogives nucléaires a Cuba, dont neuf
avaient été installées sur des missiles. {S}

On lance ensuite les transducteurs qui reperent des noms de personnes

Invite a un séminaire sur la crise des ﬁrsées de 1962 organise a La Havane,
<$person6:12663$>, quifut le secrétaire a la defense du <$person11:33051$>, a
estimé de son co‘té que les deux pays devaient normaliser leurs relations, minées
depuis trente ans par " la peur et l'hostilité ". {S} Au cours de ce séminaire,
<$person11a:26686$> a révélé que l'URSS avait déployé en 1962 trente-six
ogives nucléaires a Cuba, dont neufavaient été installées sur des missiles.

Nous obtenons ﬁnalement un ﬁchier dans lequel nous avons un index de tous les motifs
trouvés :

<Civ:ms> M. <\Civ>
<titPolit> président <\titPolit>
<titPolit> président <nation> cubain <\nation> <\titPolit>

<Person> <$titCiv:1358$> <prénom> Robert <\prénom> <Nom> McNamara <\Nom> <\Person>
<Person> <$titPolit:7035$> <Nom> Kennedy <\Nom> <\Person>
<Person> <$titPolit: 1375 $> <Prénom> Fidel <\Prénom> <Nom> Castro <\Nom> <\Person>

Aﬁn de connaitre les résultats obtenus apres cette cascade de transducteurs étaient corrects,
nous avons Vériﬁé une partie (80 000 mots environ) de notre corpus du journal Le Monde
(Tableau 3). Nous avons utilisé les mesures classiques de rappel et de précision. Le rappel
est le nombre de noms de personnes correctement trouvés par la cascade de transducteurs
diVisé par le nombre de noms de personnes réellement presents dans le texte; le rappel
calcule donc la proportion de noms de personnes trouvés. La precision représente le nombre
de noms de personnes correctement trouvés diVisé par le nombre de noms de personnes
correctes et incorrectes trouvés par la cascade.
Cas 1 Cas 2 Cas 3 Cas 4 Cas 5 Totaux
Nombre d'occ. de noms de 253 187 424 50 64 977
personnes présents
Nombre d'occ. de noms de 245 187 413 32 32 909
personnes trouvés
Nombre d'occ. de noms de 242 186 410 30 31 899
personnes correctement trouvés
Rappel 95,7% 99,5% 96,7% 60,0% 48,4% 91,9%
Précision 98,8% 99,5% 99,3% 93,8% 96,9% 98,7%
Tableau 3 : Résultats obtenus sur un extrait du journal Le Monde

Nous avons dénombré 977 noms de personnes dans ce corpus de 80 000 mots. Dans le tableau
3, nous indiquons le nombre d'occurrences de noms de personnes dans cette portion de textes

Elaboration d'une cascade de transducteurs pour l'extraction de motifs

et leur nombre de noms de personnes trouvés par notre systeme. Puis nous indiquons les
pourcentages de rappel et de précision pour chaque cas.

Les résultats obtenus sur les trois premiers cas sont tres bons ; nous obtenons 97% de rappel
et 99,2% de précision sur les noms de personnes qui sont précédés d'un déclencheur et/ou d'un
prénom. Nous remarquons que le rappel est nettement moins bon dans les cas 4 et 5. Comme
nous l'avons expliqué en 3.2, 1e cas 4 représente les noms de personnes que l'on peut détecter
par des indices parfois trop ambigus : on ne peut donc pas tous les repérer. Le cas 5 est
constitué de noms de personnes sans aucun indice : seul 1e sens de la phrase ou la
connaissance du monde d'un lecteur humain permet de determiner qu'ils sont un nom de
personne ; 48,4% des noms de personnes du cas 5 peuvent étre trouvés, grace a leur présence
sous une forme detectable ailleurs dans le texte.

Les résultats obtenus lors de la recherche des noms de personnes pourront certainement étre
améliorés lors de la recherche des autres noms propres.

4 Problémes et solutions

Voici les problemes principaux que nous avons repérés et essayés d'éViter au cours de ce
travail :

— Ex : L'orchestre a joué le Carmen de Bizet : Carmen est un prénom connu du dictionnaire
et si, dans notre cas, on n'interdit pas les determinants, on obtient l'interprétation erronée
suivante : le <person> <prenom> Carmen <\prenom> <nom> de Bizet <\nom>
<\person>. Nous observons 1e méme probleme sur le motif "la France de Vichy ". Nous
avons donc interdit la présence de determinants ou de noms communs devant un prénom
ou un nom, ce qui éVite ces erreurs. Un travail sur les noms propres déterminés a été
réalisé par [Garric, Maurel, 2000]

— Dans les expressions "la Duchesse de Windsor", "le maire de Paris", de Paris et de
Windsor ne sont pas des noms de personnes a particules. Nous interdisons donc les noms
de personnes a particules trouvés derriere un titre ou un nom de profession car ce sont
rarement des noms de personnes. Par contreM de Neuville n'est certainement pas ambigu.

— On constate également qu'avec nos outils, la phrase :"Bull a négocié un ensemble de
credits bilatéraux avec des banques étrange‘res (l'allemana’e Commerzbank, la japonaise
T okai, )" donne comme résultat <person> Commerzbank </person> , <person> T okai
</Person>.

5 Conclusion

Le principe de la cascade de transducteurs est assez simple et efficace ; par contre, la
description des motifs a trouver s'avere fastidieuse si on veut obtenir 1e meilleur résultat
possible. Les combinaisons et interactions possibles sont complexes. Mais les résultats
obtenus sont prometteurs. Les autres noms propres (toponymes, noms d'organisations) sont
plus difficiles a repérer car leurs contextes sont beaucoup plus variés.

Les motifs découverts peuvent, au dela du sujet de l'extraction de motifs et de la classification
de textes, serVir dans de nombreux domaines. On peut ainsi imaginer la création d'un systeme

Nathalie Friburger, Denis Maurel

d'écriture de ﬁchiers XML semi-automatisé ou encore prévoir un enrichissement semi-
automatique de dictionnaires.

Références

Brill E. (1992), A Simple Rule-Based Part of Speech Tagger, Proceedings of the third
conference on Applied Natural Languages Processing, Trento, Italy, ACL, pp. 152-155.

Coates-Stephens, S. (1993), The Analysis and Acquisition of Proper Names for the
Understanding of Free Text, Computers and the Humanities, 26 (5-6), pp. 441-456.

Courtois B., Silberztein M. (1990), Dictionnaire électronique des mots simples du francais,
Paris, Larousse.

Dejong G. (1982), An Overview of the frump System, dans W.B. Lehnert etMH. Ringle éd.,
Strategies for Natural Language Processing, ErlBaum, pp. 149-176.

Fairon C. (2000), Structures non-connexes. Grammaire des incises en francais .' description
linguistique et outils informatiques, These de doctorat en informatique, Université Paris 7.

Friburger N., Dister A., Maurel D. (2000), Améliorer le découpage des phrases sous INTEX,
Actes desjournées Intex 2000, RISSH, Lieges, Belgique, a paraitre.

Garric N., Maurel D. (2000), Désambigui'sation des noms propres déterminés par l'utilisation
de grammaires locales, colloque AFLA 2000 (Association francaise de linguistique
appliquée), Paris, 6-8 juillet.

Hobbs, J.R., Appelt D.E., Bear 1., Israel D., Kameyama M., Stickel M., Tyson M. (1996),
FASTUS: A cascaded ﬁnite-state transducer for extracting information from natural-language

text. Dans Finite State Devices for Natural Language Processing. MIT Press, Cambridge,
MA.

Kim J.S., Evens M.W. (1996), Efﬁcient Coreference Resolution for Proper Names in the Wall
Street Journal Text, dans Online Proceedings of Annual M41CS Conference deest
Artiﬁcial Intelligence and cognitive Science Conference), Bloomington, USA.

Morin E. (1999), Acquisition de patrons lexico-syntaxiques caractéristiques d'une relation
sémantique, dans t.a.l. Traitement Automatique des Langues, 40(1), pp. 143-166

Piton 0., Maurel D. (1997), Le traitement informatique de la géographie politique
internationale, Colloque Franche-Comté T raitement automatique des langues (FRACTAL 97),
Besancon, 10-12 décembre, Bulag, numéro spécial, pp. 321-328.

Roche E., Schabes Y. (1997), Finite State Language Processing, Cambridge, Massachussets,
MIT Press.

Silberztein M. (1993), Dictionnaire électroniques et analyse automatique de textes - Le
syste‘me INTEX, Paris, Masson.
Les transducteurs à sorties variables

Denis Maurel1, Jan Daciuk2
1
Université François-Rabelais de Tours – Laboratoire d’Informatique
denis.maurel@univ-tours.fr

2
Université Polytechnique de Gdańsk
jandac@eti.pg.gda.pl
Résumé
Dans le traitement automatique du langage naturel, les dictionnaires électroniques associent à chaque mot de
l’information. La représentation informatique la plus efficace de ces dictionnaires utilise des machines à nombre
fini d’états (automates ou transducteurs). Dans cet article, nous nous inspirons des algorithmes de construction
directe d’un automate déterministe minimal pour proposer une nouvelle forme de transducteur. Cette nouvelle
forme permet un calcul rapide des sorties associées aux mots, tout en étant plus compacte quant au nombre de
transitions et de sorties distinctes, comme le montrent nos expérimentations.
Mots-clés : automates à nombre fini d’états, transducteurs, dictionnaires électroniques.

Abstract
In natural language processing, dictionaries usually associate additional information with lexical entries. The
most effective representation of dictionaries makes use of finite-state machines – either automata (recognizers)
or transducers. In this paper, we draw our inspiration from algorithms to directly build the minimal deterministic
automaton and we propose a new form of a transducer. This new form outperforms existing transducers in terms
of speed while computing outputs and in terms of size calculated on the basis of the number of transitions and
different outputs, as shown in our experiments.
Keywords: finite state automata, transducers, electronic dictionaries.
1. Introduction
L’utilisation des automates et transducteurs à nombre fini d’états dans le cadre du Traitement
automatique des langues, déjà recommandée dans les années quatre-vingts par Maurice Gross
(Gross, 1989), est maintenant largement diffusée (Roche et Schabes, 1997). Les machines à
nombre fini d’états permettent un accès rapide à l’information, proportionnel à la longueur de
l’élément cherché. Elles sont utilisées dans différents systèmes, citons, parmi d’autres, Intex
(Silberztein, 1993), Unitex (Paumier, 2003), Nooj (Silberztein, 2004), Xerox Finite-State
Tool (Beesley et Karttunen, 2003)… et servent à toute sorte d’application : segmenteurs,
analyseurs morphologiques (Kaplan et Kay 1994), étiqueteurs (Roche et Schabes, 1995), etc.
Une des applications les plus importantes est la modélisation des dictionnaires électroniques.
Les dictionnaires peuvent être de simples listes de mots, mais fréquemment on veut associer à
chaque mot une information (une partie du discours, le lemme, le genre, le nombre, le cas…).
Cette information constitue ce qu’on appelle, dans le cadre des machines à nombre fini
d’états, une sortie du transducteur.
Plusieurs modèles de sorties sont proposés, depuis le hachage parfait aux différents types de
transducteurs. Nous présentons ici une idée originale : tenir compte de la construction de
l’automate à partir du dictionnaire pour placer les sorties sur les nouvelles transitions des états
divergents. Nous prendrons comme exemple la construction d’un automate représentant un
dictionnaire électronique.
Après avoir présenté brièvement trois modèles de machines finies dans la section 2, nous
définirons précisément ce que nous appelons un transducteur à sorties variables (section 3) et
nous comparerons les tailles de ces différents modèles à la section 4.
2. Trois modèles de machines finies
Dans cette partie, nous allons illustrer différents procédés existants pour générer l’information
à partir d’un dictionnaire électronique. Les figures ci-dessous représenteront un dictionnaire
de sept mots : rade (Nfs), rate (Nfs), ride (Nfs), rite (Nms), rude (Amfs), ruse (Nfs) et ruses
(Nfp). Les mots du dictionnaire seront systématiquement terminés par le caractère de fin de
mots, #, ce qui évite la présence sur l’automate d’états finaux intermédiaires.

2.1. Le transducteur de hachage
Introduit par Dominique Revuz (1991) et retrouvé ensuite indépendamment par Cláudio
Leonardo Lucchesi et Tomasz Kowaltowski (1993), le transducteur de hachage associe à
chaque mot sa position dans l’ordre « alphabétique » ... Ou plutôt dans un ordre prédéfini sur
les caractères, ce qui n’est pas exactement la même chose1. L’essentiel étant d’associer à un
mot une unique sortie dans un tableau numéroté de 0 à n-1. Pour cela, on associe à chaque
transition un nombre entier de manière à ce que la somme des sorties des transitions
parcourues nous donne la position du mot dans la liste triée. Le transducteur de hachage a
pour automate sous-jacent l’automate déterministe minimal.
Sur la Figure 1, le premier mot, rade, est associé à la valeur 0+0+0+0+0=0 et le mot rite à la
valeur 0+2+1+0+0=3.
i                   t
2                1
r           a                   d               e               #
1        0   2           0   3           0       4       0       5       0
u               d                           #
4           6       0                       0 #
s                                               0
e               s
1           7               8               9
0               1

Figure 1. Un transducteur de hachage

Cette représentation est idéale lorsque les sorties sont toutes très différentes et volumineuses.
L’inconvénient est la maintenance d’un tel automate. En effet, l’ajout ou la suppression d’un
mot est possible sans difficultés, mais elle change l’index ! Voir à ce sujet (Daciuk et al.,
2005).
1
Il ne s’agit pas, hélas, d’un tri alphabétique classique, mais d’un tri lexicographique (c’est-à-dire sur les
caractères). En français, un grand nombre de règles interviennent dans le tri alphabétique (en particulier,
l’interclassement de lettres majuscules et minuscules, de lettres accentuées ou non, de ligatures, etc.) (Labonté,
1998). Le tri se complique encore dans le cas des noms propres (Tran et al., 2005) !
2.2. Le transducteur à multiterminaux
Lui aussi proposé par Dominique Revuz dans sa thèse (1991), le transducteur à
multiterminaux (appelé aussi Machine de Moore) est utilisé par les logiciels Intex et Unitex.
Il s’agit de placer l’information sur des états terminaux. L’automate obtenu est déterministe,
mais la minimisation ne concerne qu’une partie des mots ayant la même sortie. Cette
représentation est, au contraire de la précédente, très intéressante pour un nombre d’étiquettes
très inférieur au nombre de mots. La Figure 2 présente un tel automate (rappelons que nous
avons remplacé les états terminaux par des transitions finales, mais le résultat est identique).
t                     e
i                                                                   #
6                        7                 8
d
t                                      Nms

r               a                    d                     e                    #
1           2                    3                        4                 5    Nfs
u                        d                 e                #
9                    10                11 Amfs
s
Nfs #
e                 s                    Nfp
12                13                   14
Figure 2. Un transducteur à multiterminaux

Remarquons sur cette figure la présence de deux transitions portant la même sortie (Nfs),
puisque le mot ruse est contenu dans le mot ruses.

2.3. Le transducteur
C’est Mehryar Mohri (1994) qui a proposé de minimiser les transducteurs en déplaçant les
sorties le plus à gauche possible, quitte à les scinder en morceaux. Les sorties sont donc à
concaténer pour obtenir la sortie finale (voir Erreur ! Référence non valide pour un signet.
où le N a été placé en amont du fs de ride et du ms de rite, tout comme le Nf en amont du s de
ruse et du p de ruses).
t
ms
i                    6
d
N
fs
t

r               a                        d                     e                        #
1           2       Nfs 3       4                                                   5
u      d
7 Amfs
s                                                  s        #
e                     s
Nf
8                 9        p               10
Figure 3. Un transducteur
3. Le transducteur à sorties variables
3.1. La construction directe de l’automate déterministe minimal
Dans les années quatre-vingt-dix, l’algorithme classique de construction d’un automate
déterministe minimal (Hopcroft, 1971) exigeait trop de place en mémoire pour permettre la
modélisation de vrais dictionnaires de langue. Les automates représentant des dictionnaires
étant en principe acycliques, plusieurs algorithmes spécifiques ont été proposés, en particulier
celui de Dominique Revuz (1992) qui utilise une pseudo-minimisation préliminaire. Un peu
plus tard, plusieurs personnes ont parallèlement implanté des algorithmes de construction
directe de l’automate déterministe minimal : citons Jan Daciuk (1998) et Stoyan Mihov
(1998, 1999), dans le cas d’une liste de mots triée ; Jan Daciuk (1998) ainsi que Bruce et
Richard Watson (Daciuk et al., 1998), J. Aoe, K. Morimoto et M. Hase (1992), Dominique
Revuz (2000), dans le cas d’une liste de mots non triée. L’algorithme pour une liste de mots
triée a été étendu à la construction d’un transducteur (Mihov et Maurel, 2000) et, plus
récemment, à un automate cyclique (Carrasco et Forcada, 2002).
D’autres algorithmes existent : citons celui de Bruce Watson, qui range préliminairement les
mots par longueur décroissante (Watson, 1998, 2003).
3.2. La construction d’un transducteur à sorties variables
Dans cet article, nous utilisons une version légèrement augmentée de l’algorithme de
construction à partir d’une liste de mots triée.
Illustrons notre algorithme à partir de notre exemple et de la Figure 4. Lors de la construction
du premier mot, rade (Nfs), nous plaçons la sortie sur la première transition. La construction
du deuxième mot, rate (Nfs), commence par une lecture du r et du a sur le premier automate,
puis vient la création d’une deuxième transition sur l’état 3 qui devient divergent. C’est cette
transition qui va porter la sortie. Et ainsi de suite… Nous suivons donc exactement
l’algorithme de construction, en ajoutant l’insertion de la sortie associée au mot sur la
première transition créée.
Dans la phase de minimisation partielle2, qui suit l’insertion d’un nouveau mot, celle-ci prend
en compte les étiquettes et les sorties ainsi placées. Ainsi, toujours sur la Figure 4, l’état 6 ne
sera pas fusionné avec l’état 3, à cause de la présence de sorties différentes.
Une fois la construction terminée, lors de la lecture d’un mot, la variable de sortie prendra
successivement toutes les valeurs de sortie rencontrées, la dernière étant la sortie associée au
mot. Par exemple, lisons le mot ruses (Nfp) sur la Figure 4 : la variable de sortie associée
prendra successivement les valeurs Nfs, Amfs , Nfs et Nfp, sa valeur finale.
2
Dont la profondeur dépend du mot suivant, ce qui justifie le fait que la liste soit triée au départ de
l’algorithme.
t
Nms
i             6
d
Nfs
t
Nfs
r           a               d                 e               #
1   Nfs     2                 3               4               5
u                d
Amfs          7
s
e               s
Nfs
8               9   Nfp         10
Figure 4. Un transducteur à sorties variables
3.3. Définition et algorithmes
Définition : Un transducteur à sorties variables est un transducteur p-sous-séquentiel à
nombre fini d’états qui associe à chaque mot reconnu la dernière sortie rencontrée lors du
parcours de l’automate.
C’est-à-dire un septuplet T=(Q, L, S, q0, q1, δ, λ) où Q est un ensemble fini non vide d’états, L
un ensemble fini non vide de lettres (l’alphabet d’entrée), S un ensemble fini non vide
d’entiers (les numéros de ligne du tableau des sorties), q0 est un élément de Q (l’état initial),
q1 est un élément de Q (l’état terminal), δ est une fonction définie de Q × (L ∪ {#}) dans Q (la
fonction de transition) et λ est une fonction définie de Q × (L ∪ {#}) dans S ∪ {ε} (la fonction
de sortie des transitions).
Un mot m=l1l2...ln est reconnu par T ssi ∃ p0, p1,..., pn ∈ Q, tels que :
∀ i=0, 1, ..., n-1, δ(pi, li+1)=pi+1 et ∀ i=0, 1, ..., n-1, λ(pi, li+1)=si+1 avec p0=q0 et pn=q1
et il a pour sortie s= sj avec j=Max{i=0, 1, ..., n-1 / λ(pi, li+1)≠ε}
Algorithme :
{      Créer_l’automate_avec_le_premier_mot3
suivant←Lire_un_mot_dans_le_dictionnaire
FAIRE
{      courant←suivant
suivant←Lire_un_mot_dans_le_dictionnaire
Lire_le_début_du_mot_courant_sur_l’automate
Complèter_l’automate_par_la_fin_du_mot_courant3
préfixe_plus_un←Calculer_la_fin_de_la_minimisation
Minimiser_l’automate_jusqu’au_préfixe_plus_un
TANT_QUE (le_mot_suivant_existe)
3
En plaçant la sortie sur la première transition.
4. Comparaison des résultats
Nous allons comparer les résultats obtenus sur deux dictionnaires français, la liste des noms
des villes françaises et le dictionnaire Delaf (Courtois et Silberztein, 1990), qui est sans doute
le dictionnaire électronique le plus complet pour les mots monolexicaux de la langue
française. Ces dictionnaires sont présentés sur la Figure 5 (nombre de mots et de caractères,
sans prendre en compte les sorties, mais avec le caractère de fin de mot). Nous ajoutons aussi
à ce tableau le nombre d’états et de transitions de l’automate déterministe minimal
correspondant.
Dictionnaire                               Automate
Mots             Caractères               États            Transitions
Villes               38 198               447 549               61 240               95 589
Delaf               682 418             7 238 120               67 995              177 465
Figure 5. Nos listes de mots et l’automate déterministe minimal correspondant

Nous associons aux noms des villes françaises, le numéro de département et aux mots du
dictionnaire Delaf, le lemme4, la catégorie syntaxique, la couche d’utilisation5 et le code
morphologique. Des exemples de sorties sont donnés sur la Figure 6. Le dictionnaire des
noms des villes françaises ainsi constitué possède 1 550 sorties distinctes6 et le dictionnaire
Delaf, 7 185.

Entrées                      Sorties

Villes                       Tours                         37
exemple                     0.N+z1:ms
Delaf
exemples                   -1.N+z1:mp
Figure 6. Exemples de données

Présentons maintenant les résultats concernant la construction d’un transducteur. La Figure 7
présente, pour chaque construction, le nombre d’états et de transitions obtenus, ainsi que le
nombre de transitions qui portent des sorties et le nombre de sorties distinctes présentes sur le
transducteur. Les sorties distinctes sont placées dans un tableau. Ce sont les numéros de ligne
de ce tableau qui sont réellement notée sur le transducteur. D’où l’importance de la taille de
ce tableau, c’est-à-dire du nombre de sorties distinctes sur le transducteur.
Ces exemples prouvent, s’il était besoin, l’importance de la représentation des données. Si
nous mémorisons nos automates sous la forme d’un tableau de transitions (Daciuk, 2000),
nous obtenons évidemment que le tableau minimal est celui du hachage parfait, puisqu’il

4
Le lemme est calculé : on indique le nombre de caractères à retirer au mot en entrée pour l’obtenir (avec,
éventuellement des caractères à ajouter).
5
Le vocabulaire du dictionnaire Delaf est divisé en trois couches : mots courants, précieux et techniques
(Garrigues, 1993).
6
Il ne faut pas s’étonner de trouver 1 550 sorties pour les numéros de département associés au noms de ville,
alors qu’il n’existe que 99 numéros ! Cela est dû à l’importante homographie des noms de ville française. Par
exemple, il existe 25 villes portant le nom de Neuilly ; celles-ci sont en fait distinguées par un nom polylexical
plus large (Neuilly-sur-Seine, Neuilly-sur-Marne, etc.), mais trois restent avec un nom monolexical associé à une
sortie triple : 27|58|89 (Belleil, Maurel, 1996).
correspond exactement à l’automate déterministe minimal. Cependant, celui-ci comprend un
nombre important de transitions avec sortie (sur le tableau de la Figure 7, le nombre de
transitions avec sortie des autres transducteurs représente respectivement pour les deux
dictionnaires 4,4 % et 0,1 % du nombre de sorties du transducteur de hachage).
Pour les trois autres constructions, le transducteur à sorties variables est celui qui possède le
moins de transitions pour les deux dictionnaires (pour les noms de ville, le transducteur à
multiterminaux a même un tiers de transitions en plus…). Paradoxalement peut-être, puisque
le principe du transducteur classique est de permettre des factorisations, c’est aussi celui des
deux transducteurs qui possède le moins de sorties distinctes. En fait, exactement le nombre
de sorties distinctes présentes dans le dictionnaire, comme évidemment le transducteur à
multiterminaux, bien moins performant en nombre de transitions comme cela a été déjà
remarqué.
Notre représentation des dictionnaires semble donc légèrement meilleure que la
représentation classique par transducteur. Elle évite en plus le calcul correspondant à la
concaténation des sorties, calcul non trivial lorsqu’une sortie est multiple7.
Transitions           Sorties
États          Transitions
avec sortie          distinctes
Transducteur
61 240              95 589              95 589             35 3828
de hachage
Transducteur à
94 870             130 199              3 792               1 550
multiterminaux
Villes
Transducteur
62 692              98 026              35 443              1 564

Transducteur à
62 395              97 523              35 129              1 550
sorties variables
Transducteur
67 995             177 465             177 465            637 2828
de hachage
Transducteur à
102 343            251 472              20 186              7 185
multiterminaux
Delaf
Transducteur
99 529             259 139              81 623              9 013

Transducteur à
92 946             240 534             147 589              7 185
sorties variables
Figure 7. Taille respective des transducteurs
7
Par exemple, le mot fort a pour catégorie adjectif, adverbe ou nom et est donc associé à une sortie triple :
0.A+z1:ms|0.ADV+z1|0.N+z1:ms pour laquelle on aurait peut-être pu factoriser 0. sur des transitions
précédentes.
8
Il ne s’agit pas véritablement ici du nombre de sorties différentes, mais de la plus grande sortie présente sur
le transducteur.
5. Conclusion
Dans cet article, nous avons défini un nouveau modèle de transduction à nombre fini d’états,
le transducteur à sorties variables. Après comparaison avec d’autres machines finies, nous
remarquons qu’il permet une représentation des dictionnaires légèrement plus compacte en
nombre de transitions et de sorties différentes. L’algorithme de construction du transducteur à
sorties variables est basé sur l’algorithme de construction à partir d’une liste de mots triée et a
donc une complexité similaire9. De plus, le calcul de la sortie associée à un mot reconnu est
trivial, ce qui n’est pas toujours le cas d’un transducteur classique. Ceci à la fois dans
l’algorithme de construction du transducteur, où il faut décomposer les sorties et dans
l’algorithme de lecture où il faut au contraire les concaténer.
6. Remerciements
Les auteurs remercient le Professeur Éric Laporte pour l’utilisation du dictionnaire Delaf. La
collaboration sur ce sujet entre les universités Politechnika Gdańska et François-Rabelais a
débuté alors que Jan Daciuk était professeur invité à Blois.
Références
AOE J., MORIMOTO K., HASE M. (1992). « An Algorithm for Compressing Common Suffixes
Used ». In Trie Structures. Trans. IEICE.
BEESLEY K.R., KARTTUNEN L. (2003). Finite-State Morphology. CSLI Publications.
BELLEIL C., MAUREL D. (1996). « Traitement informatique des ambiguïtés dans la reconnaissance
des noms propres liés à la géographie ». In Bulag 21 : 29-50.
CARRASCO R.C., FORCADA M.L. (2002). « Incremental construction and maintenance of minimal
finite-state automata ». In Computational Linguistics 28 (2) : 207-216.
COURTOIS B., SILBERZTEIN M. (1990). « Dictionnaires électroniques du français ». In Langue
française 87 : 11-22.
DACIUK J. (1998). Incremental Construction of Finite-State Automata and Transducers, and their
Use in the Natural Language Processing. Ph.D. dissertation, Technical University of Gdańsk.
DACIUK J. (2000). « Experiments with Automata Compression ». In CIAA 2000. LNCS 2088 : 105-
112.
DACIUK J., MAUREL D., SAVARY A. (2005). « Dynamic Perfect Hashing with Finite-State
Automata ». In Intelligent Information Systems 2005 (IIS 2005). Gdansk.
DACIUK J., MIHOV S., WATSON B.W., WATSON R.E. (2000). « Incremental construction of Minimal
Acyclic Finite-state Automata ». In Computational Linguistics 26 (1) : 3-16.
DACIUK J., WATSON B. W., WATSON R.E. (1998). « Incremental construction of Minimal Acyclic
Finite-state Automata and Transducers ». In Proceedings of FSMNLP : 48-56.
GARRIGUES M. (1993). Méthode de paramétrage des dictionnaires et grammaires électroniques :
Application à des systèmes interactifs en langue naturelle. Thèse de doctorat en Sciences du
langage, Université Paris VII.
GROSS M. (1989). « The Use of Finite Automata in the Lexical Representation of Natural
Language ». In Electronic Dictionaries and Automata in Computational Linguistics. LNCS 377 :
34-50.
HOPCROFT J. (1971). « An n log n algorithm for minimizing states in a finite automaton ». In
International Symposium on Theory of Machines and Computations. Haifa : 189-196.

9
O(n) en espace et O(mlog(n)) en temps, avec n le nombre d’états et m le nombre de mots (Mihov, 1999).
KAPLAN R.M., KAY M. (1994). « Regular Models of Phonological Rule Systems ». In Computational
Linguistics. 20 (3) : 331-378.
LABONTÉ A. (1998). Technique de réduction – Tris informatiques à quatre clés, Gouvernement du
Québec, publié sur la Toile à l’adresse : http://www.lirmm.fr/~lafourca/Souk/trif/techniques-de-
tri.html.
LUCCHESI C.L., KOWALTOWSKI T. (1993). « Applications of Finite Automata Representing Large
Vocabularies ». In Softw., Pract. Exper., 23 (1) : 15-30.
MIHOV S. (1998). « On-line algorithm for building minimal automaton Presenting Finite Language ».
In Annuaire de l’Université de Sofia St. Kl. Ohridski 91 (1). Faculté de Mathématiques et
Informatique.
MIHOV S. (1999). « Direct construction of minimal acyclic finite states automata ». In Annuaire de
l’Université de Sofia St. Kl. Ohridski. 92 (2). Faculté de Mathématiques et Informatique.
MIHOV S., MAUREL D. (2000). « Direct construction of minimal acyclic sub- sequential
transducers ». In CIAA’2000. LNCS 2088 : 217-229.
MOHRI M. (1994). « Minimization of Sequential Transducers ». In CPM’94. LNCS 807 : 151-163.
PAUMIER S. (2003). De la Reconnaissance de Formes Linguistiques à l’Analyse Syntaxique. Thèse de
Doctorat en Informatique, Université de Marne-la-Vallée.
REVUZ D. (1991). Dictionnaires et lexiques – Méthodes et algorithmes, Thèse de Doctorat en
Informatique, Université Paris 7.
REVUZ D. (1992). « Minimization of acyclic deterministic automata in linear time ». In Theoretical
Computer Science 92 : 181-189.
REVUZ D. (2000). « Dynamic acyclic minimal automaton ». In CIAA 2000. LNCS 2088:226-232.
ROCHE E., SCHABES Y. (éds) (1997). Finite state language Processing. MIT Press, Cambridge,
Mass./London,.
SILBERZTEIN M. (1993). Dictionnaires électroniques et analyse automatique de textes – Le système
INTEX. Masson, Paris.
SILBERZTEIN M. (2004). « Nooj: A cooperative Object Oriented Architecture for NLP ». In Cahiers
de la MSH Ledoux, Série Archive, Bases, Corpus 1 : 351-361.
TRAN M., MAUREL D., SAVARY A. (2005). « Implantation d’un tri lexical respectant la particularité
des noms propres ». In Lingvisticae Investigationes, XXVIII-2 (à paraître).
WATSON B.W. (1998). « A fast new semi-incremental algorithm for the Construction of Minimal
Acyclic DFAs ». In WIA’98, LNCS 1660 : 121-132.
WATSON B.W. (2003). « A new algorithm for the construction of minimal acyclic DFAs ». In Science
of Computer Programming, 48-2-3 : 81-97.

PrepLex : un lexique des prépositions du français pour
l’analyse syntaxique

Karën F ORT1 , Bruno G UILLAUME2
1
projets Calligramme et TALARIS, 2 projet Calligramme
LORIA/INRIA Lorraine, UMR 7503, Nancy
{Karen.Fort,Bruno.Guillaume}@loria.fr
Résumé. PrepLex est un lexique des prépositions du français. Il contient les informations
utiles à des systèmes d’analyse syntaxique. Il a été construit en comparant puis fusionnant
différentes sources d’informations lexicales disponibles. Ce lexique met également en évidence
les prépositions ou classes de prépositions qui apparaissent dans la définition des cadres de
sous-catégorisation des ressources lexicales qui décrivent la valence des verbes.
Abstract. PrepLex is a lexicon of French prepositions which provides all the information
needed for parsing. It was built by comparing and merging several authoritative lexical sources.
This lexicon also shows the prepositions or classes of prepositions that appear in verbs sub-
categorization frames.
Mots-clés :          prépositions, lexique, analyse syntaxique.

Keywords:            prepositions, lexicon, parsing.
1    Introduction

Lors de la définition de classes d’entrées lexicales en fonction de leur catégorie, il apparaît
naturellement une distinction entre deux types de classes. D’une part, les classes fermées dont
on peut énumérer tous les éléments de façon exhaustive ; c’est le cas par exemple des pronoms
ou des déterminants. D’autre part, les classes ouvertes pour lesquelles il n’est pas possible de
lister tous les éléments (ils peuvent dépendre d’un vocabulaire spécifique à un domaine par
exemple) ; les quatre grandes classes ouvertes sont les noms, les verbes, les adjectifs et les
adverbes. La méthodologie de construction d’un lexique doit nécessairement être adaptée en
fonction de cette notion de classe.
Le statut de la classe des prépositions est plus difficile à établir. A priori, l’ensemble des pré-
positions peut sembler une classe fermée dont on peut énumérer les éléments ; en pratique, la
comparaison de différentes sources disponibles montre qu’il n’est pas facile de déterminer de
façon exhaustive la liste des prépositions. Or, celles-ci représentent plus de 14% des lemmes du
français1 .
1
voir par exemple sur un corpus journalistique : https://www.kuleuven.be/ilt/blf/
rechbaselex_kul.php\#freq (Selva et al., 2002)

219

Karën F ORT, Bruno G UILLAUME
Dans un lexique complet, il est important d’avoir des informations de sous-catégorisation pour
les mots prédicatifs (Briscoe & Carroll, 1993; Carroll & Fang, 2004). Ces informations de
sous-catégorisation font souvent référence à des prépositions dans la description des argu-
ments. En effet, ces arguments sont souvent contraints à utiliser une préposition particulière (par
exemple compter sur) ou un ensemble de prépositions qui ont un aspect sémantique commun
(par exemple aller LOC, où LOC peut être remplacé par n’importe quelle préposition locative).
Pour une analyse syntaxique profonde, il est utile de distinguer les compléments indirects requis
par le verbe des autres compléments adjoints ne figurant pas dans la valence du verbe. Les
deux exemples (1a) et (1b) ont la même structure de surface et seule la sémantique permet de
distinguer les deux usages différents de la préposition sur : elle introduit un complément oblique
dans le premier cas et un complément adjoint dans le second. Ce problème peut être géré par
l’utilisation d’informations sémantiques plus fines.
1a.   Jean compte sur ses amis
1b.   Jean compte sur ses doigts
Cette distinction amène à distinguer deux usages différents des prépositions et est donc une
source d’ambiguïté lexicale. Pour limiter cette ambiguïté, il est important que le lexique repère
les prépositions qui peuvent jouer ces deux rôles (ce sont les prépositions argumentales).
Notre travail est destiné à fournir un lexique utilisable par un analyseur syntaxique. Nous
nous sommes restreints aux aspects purement syntaxiques et à quelques éléments séman-
tiques comme la définition des ensembles de prépositions ayant un aspect sémantique commun
(comme LOC). Le lexique produit est diffusé sous licence libre et a vocation à être intégré dans
des ressources plus larges, existantes ou à venir.
La section 2 décrit les sources utilisées et la méthodologie de comparaison, la section 3 décrit
les résultats de cette comparaison. La section 4 décrit comment le lexique est constitué à partir
des résultats précédents. Enfin, la section 5 présente un exemple d’utilisation de ce lexique en
analyse syntaxique.
2      Méthodologie

L’utilisation de prépositions dans le cadre d’une analyse syntaxique nécessite une liste large,
inventoriant à la fois les prépositions non argumentales et celles susceptibles d’apparaître dans
les cadres de sous-catégorisation des verbes.
2.1    Utilisation de lexiques syntaxiques

Bien entendu, il existe déjà des lexiques syntaxiques, qui proposent un ensemble intéressant de
prépositions. Ainsi, le Lefff (Sagot et al., 2006) fournit une liste conséquente de prépositions,
mais la partie syntaxique du lexique est encore en cours de développement, il présente donc peu
de prépositions dans les cadres de sous-catégorisation des verbes. En outre, certaines préposi-
tions du Lefff semblent obsolètes ou rares. Le dictionnaire français-UNL (Sérasset & Boitet,
2000) en propose également, mais sa couverture reste limitée et la qualité des entrées est encore
inégale. D’autres sources proposent des prépositions dans les cadres de sous-catégorisation des
verbes, mais les listes ne sont pas tout à fait cohérentes d’une source à l’autre.
220

Un lexique des prépositions
Nous avons donc effectué, dans un premier temps, un travail d’inventaire des prépositions pré-
sentes dans un certain nombre de ressources, des lexiques et/ou dictionnaires d’une part, pour
la liste générale, des lexiques syntaxiques d’autre part, pour la liste de prépositions argumen-
tales. Deux ressources se classent cependant dans les deux catégories, le Lefff et le dictionnaire
UNL :
– Le Lefff (Lexique des Formes Fléchies du Français (Sagot et al., 2006)) est un lexique mor-
phologique et syntaxique du français à large couverture (plus de 110 000 lemmes). Dans sa
version 2.2.1, ce lexique contient 48 prépositions simples et 164 prépositions complexes. Il
présente également des informations de sous-catégorisation des verbes, qui font apparaître
14 prépositions que nous qualifierons d’argumentales.
– UNL (Universal Networking Language (Sérasset & Boitet, 2000)), est un dictionnaire du
français vers un anglais désambiguïsé conçu pour la traduction automatique, qui comprend
des informations syntaxiques dans sa partie française. UNL n’a qu’une couverture assez
faible (moins de 27 000 lemmes), mais il propose dans sa partie anglaise des informations de
type sémantique que nous envisageons d’utiliser par la suite. UNL contient 48 prépositions
simples dont 10 apparaissant dans les cadres de sous-catégorisation des verbes.
2.2      Utilisation de sources de référence

Nous avons ensuite complété la liste de prépositions en utilisant différentes sources construites
manuellement, lexique ou dictionnaire, voire grammaire :
– Le Grevisse (Grevisse, 1997), dans sa version papier, nous a permis de vérifier certaines
intuitions concernant l’obsolescence ou l’usage de certaines prépositions.
– Le TLFi (Trésor de la langue française informatisé), que nous avons consulté via l’interface
du CNRTL2 , offre une liste de prépositions un peu différente des autres. Elle comporte no-
tamment les formes voici et voilà, rarement citées dans les autres sources à notre disposition.
– Enfin, la base de prépositions PrepNet (Saint-Dizier, 2006) nous a permis de vérifier à la fois
la complétude de notre liste et les informations sémantiques présentes dans certaines sources.
2.3      Utilisation de dictionnaires de valences verbales

Nous avons ensuite cherché à enrichir la liste des prépositions apparaissant dans les cadres de
sous-catégorisation des verbes de Lefff et UNL en nous référant à deux sources traitant plus
particulièrement des verbes :
– Le dictionnaire de valences des verbes du français DICOVALENCE, héritier de PROTON
(van den Eynde & Mertens, 2002), dont la démarche est fondée sur l’approche pronominale.
Dans sa version 1.1, ce dictionnaire donne les cadres valenciels de plus de 3700 verbes. Nous
avons extrait les prépositions simples et multi-mots qu’il contient (soit plus de 40), ainsi que
leurs traits sémantiques associés.
– Nous avons complété cette liste de prépositions argumentales par celle de SynLex (Gardent
et al., 2006), lexique syntaxique créé à partir des tables du lexique-grammaire du LADL
(Gross, 1975).

2
voir : http://www.cnrtl.fr

221

Karën F ORT, Bruno G UILLAUME
Lexiques                      Cadres de sous-catégorisation
Lefff   TLFi   Grevisse PrepNet       UNL     Lefff DVa SynLex UNL
à         X       X        X       loc                 319 895         887     246
après        X       X        X       loc          X       2      12       1
aussi                                             X
avec          X     X         X          X        X       35     193     611      49
chez          X     X         X         loc       X                9               1
comme           X                                   X       14      11      10       3
de           X     X         X        deloc      X      310     888     1980     282
depuis          X     X         X        deloc      X                2       1
derrière        X     X         X         loc       X                3
devers         X     X         X
dixit         X
emmi                 X
entre         X     X         X         loc       X              19        4
hormis          X     X         X          X        X
jusque          X     X         X                   X               7
lès          X     X         X
moyennant        X     X         X          X        X
par          X     X         X         loc       X       3      38       73        8
parmi          X     X         X         loc       X               7        7
passé                X                             X
selon          X     X         X          X        X               1        1
voici               X                             X
TAB . 1 – Quelques prépositions simples dans les différentes sources
a
DICOVALENCE
Nous avons, à partir de ces différentes sources, effectué une étude systématique de la présence
de chaque préposition, de leur appartenance éventuelle à des cadres valenciels, ainsi que de
certains traits sémantiques qui leur sont associés. Nous avons ensuite regroupé les prépositions
qui apparaissaient à la fois en tant qu’entrée lexicale et dans les cadres de sous-catégorisation
des verbes.
Il est à noter que nous avons dû restreindre notre analyse des cadres de sous-catégorisation à
ceux des verbes, du fait qu’il n’existe encore à notre connaissance aucun lexique présentant une
information syntaxique suffisamment riche sur les adjectifs ou les noms.
Les prépositions multi-mots présentant des caractéristiques (nombre) et des difficultés spéci-
fiques (segmentation), nous en avons fait un inventaire séparé selon les mêmes méthodes.
222

Un lexique des prépositions
Lexiques                    Cadres de sous-catégorisation
Lefff   TLFi   Grevisse PrepNet      UNL    Lefff DVa SynLex UNL
à cause de        X                X        X
à la faveur de                       X        X
à partir de       X                X      deloc                                1
afin de         X       X        X        X
au nord de                                 loc
au vu de         X
auprès de         X       X        X         loc                      27      35
comme de                                                               1
conformément à        X                             X
d’avec                          X                                   1       6
en faveur de       X                X            X                     13
il y a        X
jusqu’à         X                          loc       X              10
jusqu’en         X
jusqu’où         X
loin de         X                X         loc
par suite de                        X
pour comble de        X
près de         X                X         loc
quant à         X       X        X
tout au long de      X                             X
vis-à-vis de       X                X            X                              1
TAB . 2 – Quelques prépositions multi-mots dans les différentes sources
a
DICOVALENCE
3        Résultat de la comparaison des sources

3.1       Prépositions simples

Nous avons ainsi listé 85 prépositions simples, dont 24 apparaissent dans des cadres de sous-
catégorisation de verbes (cf. tableau 1).
Il est à noter que les 4 sources qui décrivent des cadres de sous-catégorisation utilisent des
formats très différents. Lefff propose une vision condensée des verbes, les cadres valenciels
étant regroupés dans une seule entrée ; à l’opposé, SynLex et DICOVALENCE décrivent de
nombreux cadres en distinguant par exemple systématiquement les différentes réalisations syn-
taxiques des arguments. Les valeurs relatives sur une ligne ne reflètent donc pas du tout la
couverture lexicale des sources.
3.2       Prépositions multi-mots

Nous avons obtenu une liste de 222 prépositions multi-mots, dont 18 apparaissent dans des
cadres de sous-catégorisation de verbes (cf. tableau 2). Il est intéressant de noter que seuls
223

Karën F ORT, Bruno G UILLAUME
DICOVALENCE et SynLex proposent des prépositions multi-mots dans leurs cadres de sous-
catégorisation. Le Lefff fournit quant à lui une liste impressionante de prépositions multi-mots
(plus de 150) qui représente une excellente base de travail.
4     Construction du lexique

Le premier critère de sélection que nous avons choisi d’appliquer pour construire notre lexique
est qu’une préposition doit être listée dans au moins une source parmi celles citées. Par ailleurs,
nous considérons qu’une préposition est argumentale si elle apparaît dans au moins un cadre de
sous-catégorisation de verbe.
4.1    Filtrage manuel

Nous avons ensuite trié ces prépositions en fonction de critères simples et avons identifié en
particulier celles qui étaient à éliminer parce que :
– visiblement erronées, c’est le cas par exemple de aussi, présent dans le dictionnaire UNL en
tant que préposition,
– obsolètes ou d’un emploi extrêmement rare, comme emmi (TLFi), devers (Lefff, TLFi, Gre-
visse) ou encore comme de (DICOVALENCE).
Nous avons également effectué un tri dans les traits sémantiques et avons supprimé les entrées
erronées, telles que avec comme locatif dans SynLex et dans DICOVALENCE.
4.2    Quelques remarques

Certaines sources font apparaître comme des prépositions des formes qui ne sont pas considé-
rées comme telles en linguistique. C’est le cas en particulier de :
– comme, qui n’est pas cité dans les trois sources de référence que sont le Grevisse, le TLFi et
PrepNet, il est en effet ambigu et peut également être considéré comme une conjonction,
– il y a ou y compris, qui ne sont citées que dans le Lefff,
– d’avec, qui, bien qu’il apparaisse dans le Grevisse, n’est présent que dans les cadres de sous-
catégorisation de DICOVALENCE et SynLex.
Nous avons choisi de conserver ces formes dans notre lexique, pour des raisons pratiques liées
à l’application visée, l’analyse syntaxique.
Par ailleurs, même si sa couverture est large, notre lexique n’est évidemment pas exhaustif. Il
serait intéressant d’y ajouter certaines formes manquantes, notamment :
– des prépositions présentes dans le DAFLES (Selva et al., 2002), comme par exemple la forme
au détriment de,
– des prépositions citées dans des grammaires de référence, comme question, dans la Gram-
maire méthodique du français (Riegel et al., 1997),
– les multiples prépositions locatives (et, par métonymie, temporelles) que peut préfixer la
forme jusqu’, par exemple jusqu’auprès de. Cette forme élidée de jusque pourrait sans doute
faire l’objet d’un traitement particulier en tant que modifieur de préposition. Il en va d’ailleurs
de même de dès, suivi d’un temporel (ou d’un locatif, par métonymie).
224

Un lexique des prépositions
Lexiques                                Cadres de sous-catégorisation
Lefff    TLFi   Grevisse PrepNet    UNL     PrepLex    Lefff    DV SynLex UNL PrepLex
44       69      55         36      46        49       14      24     18      11        23
TAB . 3 – Total des prépositions simples par source

Lexiques                                Cadres de sous-catégorisation
Lefff    TLFi   Grevisse PrepNet    UNL     PrepLex    Lefff    DV SynLex UNL PrepLex
166       11      77         89       2       206        0      16      4       0        15
TAB . 4 – Total des prépositions multi-mots par source

Ce tri a également permis de mettre en évidence certaines difficultés, en particulier les élisions
dans les formes multi-mots, telle afin de, afin d’, ou les contractions telles face à, face au ou à
partir de, à partir du, qui seront traitées lors de la segmentation.
D’autres, comme lès, qui n’est utilisé qu’en toponymie dans des formes avec tirets (comme
Bathelémont-lès-Bauzemont), seront traitées en amont, lors de la segmentation.
4.3      Résultats

Au final, nous obtenons une liste de 49 prépositions simples, dont 23 apparaissent dans des
cadres de sous-catégorisation des verbes, dans au moins une source, et sont donc considérées
comme argumentales (cf. tableau 3).
Nous obtenons également une liste de plus de 200 prépositions multi-mots, dont 15 apparaissent
dans des cadres de sous-catégorisation des verbes, dans au moins une source, et sont donc
considérées comme argumentales (cf. tableau 4).
Nous avons pour l’instant limité les informations sémantiques utilisées dans le lexique à loc (lo-
catif) et deloc (délocatif), mais nous avons l’intention d’étendre ces catégories à celles retenues
dans DICOVALENCE (temps, quantité, manière).
En outre, nous nous sommes également référés aux sources pour renseigner les catégories des
arguments introduits par les prépositions argumentales.
Il n’existe pas encore de format normalisé pour les lexiques syntaxiques, même si un effort
d’homogénéisation est en cours (Projet Lexsynt). Actuellement, PrepLex est donc présenté dans
un format texte qui permet à la fois l’édition manuelle et l’utilisation dans un analyseur ou
dans d’autres outils de traitement de la langue. Dans ce format, les informations syntaxiques
sont décrites à l’aide structures de traits récursives de profondeur 2. Le niveau externe décrit
la structure en termes d’“argument” : ce niveau contient toujours un trait “head” et un trait
pour chacun des “arguments” de l’entrée. Le niveau interne décrit alors plus finement chaque
argument. De plus, ce format permet de définir des informations syntaxiques de façon modulaire
en factorisant les parties redondantes. Dans le cas des prépositions, toutes les entrées partagent
le même squelette :

Prep : [
head [cat=prep, prep=#, funct=#]
comp [cat=#, cpl=@]
225

Karën F ORT, Bruno G UILLAUME
Lorsque ce squelette est instancié pour une préposition particulière, les 3 valeurs de traits (notées
“#”) doivent impérativement être renseignées et la valeur de trait notée “@” est optionnelle.
Ces traits sont repérés par leur nom (prep, funct) pour la tête, par une notation pointée
(comp.cat, comp.cpl) pour l’argument.
à             Prep     [prep=a|LOC; funct=aobj|loc|adj; comp.cat=np|sinf;
comp.cpl=void|ceque]
après     Prep         [prep=apres|LOC; funct=obl|loc|adj; comp.cat=np]
avec      Prep         [prep=avec; funct=obl|adj; comp.cat=np]
à_travers Prep         [prep=a_travers;funct=obl|adj; comp.cat=np]
Techniquement, la seule difficulté est de choisir comment représenter l’appartenance à une
classe sémantique de prépositions comme loc. Ici, nous avons choisi de définir comme va-
leurs atomiques possibles pour le trait “prep”, l’ensemble des prépositions argumentales et l’en-
semble des classes sémantiques (notées en majuscules). On utilise alors la disjonction a|LOC
pour indiquer que la préposition à peut être utilisée soit comme une préposition particulière,
soit comme une préposition locative.
Nous avons en outre décidé d’indiquer dans le lexique les sources dans lesquelles la préposition
apparaît, afin de permettre un éventuel filtrage pour des utilisations particulières. Dans le cas des
prépositions argumentales, nous avons ajouté un champ comportant la fréquence d’apparition
et un exemple pris dans l’une des sources.
5      Un exemple d’utilisation dans un système TAL

Nous exposons ici de manière succinte quelques problèmes posés par les prépositions lors de
l’analyse syntaxique.
5.1    Spécificités liées à la segmentation

La première difficulté pour l’intégration des prépositions dans un analyseur se situe au niveau de
la segmentation en lexèmes. Il faut gérer les phénomènes d’élision : au doit être traité comme à
le, le de finissant certaines prépositions multi-mots peut être élidé en d’. Cependant, ces phéno-
mènes ne sont pas spécifiques aux prépositions, il sont traités soit dans le lexique (par exemple
Lefff distingue les deux formes au cours de et au cours d’), soit lors de la segmentation. Nous
avons choisi de les traiter dans le segmenteur, afin de simplifier la maintenance du lexique.
Une difficulté plus directement liée aux prépositions multi-mots, est la possibilité d’avoir une
ambiguïté de segmentation. Par exemple, dans les deux phrases (2a) et (2b) la suite de mots
au cours de est une préposition multi-mots dans le premier cas, mais elle doit être décomposée
dans le deuxième. D’autres prépositions multi-mots ne nécessitent jamais de découpage, par
exemple y compris.
2a.   Il a beaucoup travaillé au cours de cette année
2b.   Il a beaucoup travaillé au cours de M. Durand
226

Un lexique des prépositions
5.2       Prépositions compléments vs prépositions argumentales

Lors de l’analyse syntaxique, on doit nécessairement distinguer l’usage d’une préposition
pour introduire un argument du verbe de celui d’une préposition introduisant un complément.
Comme on l’a vu (exemples (1a) et (1b)), cette distinction est souvent difficile à établir et re-
pose sur des considérations sémantiques. L’analyse syntaxique doit alors maintenir l’ambiguité
de rattachement. Le fait d’avoir des informations précises sur les prépositions argumentales
permet de contrôler ces ambiguïtés.
6        Conclusion

En comparant divers lexiques et dictionnaires, nous avons établi une liste de prépositions utiles
pour le TAL. Nous nous sommes concentrés surtout sur les aspects syntaxiques. Un tri manuel
a permis d’écarter des prépositions obsolètes ou très rares et quelques cas d’erreur. Le lexique
ainsi produit contient plus de 250 prépositions dont 49 sont des prépositions simples.
Dans les lexiques syntaxiques, les cadres de sous-catégorisation décrivent les prépositions intro-
duisant certains arguments. Des prépositions apparaissant dans les entrées verbales d’un lexique
syntaxique sont appelées argumentales. Nous avons identifié 40 prépositions argumentales.
Le lexique développé est librement disponible 3 . Ce lexique va nécessairement évoluer. D’autres
sources d’information auraient leur place dans ce travail, notamment les champs constructions
des verbes du TFLi qui font référence à des prépositions qui sont donc argumentales. Il est
prévu d’utiliser prochainement cette source pour faire évoluer le lexique.
La mise en place d’une base de donnée contenant ce lexique est en cours 3 . Cela devrait per-
mettre de faciliter la maintenance du lexique, mais aussi d’enrichir les données pour chaque
entrée, notamment avec des exemples d’usage ou des traits sémantiques plus variés (loc, de-
loc, mais aussi tim, man, qty). Nous envisageons d’y ajouter des informations de fréquence sur
corpus.
Une tâche de bien plus grande ampleur serait d’enrichir ce lexique avec des informations sé-
mantiques plus fines que la seule référence aux classes loc, deloc, . . . Il existe de nombreux
travaux de linguistique portant sur les prépositions. Cependant, la plupart s’attache à des des-
criptions sémantiques fines d’un petit nombre de prépositions ; une exception notable étant le
travail réalisé dans PrepNet (Saint-Dizier, 2006). Il conviendrait donc de transformer ces res-
sources pour les rendre directement utilisables par un système de traitement automatique des
langues.
Remerciements

Nous tenons à remercier chaleureusement M. Guy Perrier, Professeur à l’Université Nancy 2,
pour ses conseils patients et éclairés.

3
http://loriatal.loria.fr/Resources.html

227

Karën F ORT, Bruno G UILLAUME
Références
B RISCOE T. & C ARROLL J. A. (1993). Generalised probabilistic LR parsing for unification-
based grammars. Computational Linguistics.
C ARROLL J. A. & FANG A. C. (2004). The automatic acquisition of verb subcategorisations
and their impact on the performance of an HPSG parser. In Proceedings of the 1st International
Joint Conference on Natural Language Processing (IJCNLP), p. 107–114, Sanya City, China.
G ARDENT C., G UILLAUME B., P ERRIER G. & FALK I. (2006). Extraction d’information
de sous-catégorisation à partir des tables du LADL. In Proceedings of TALN 06, p. 139–148,
Leuven.
G REVISSE M. (1997). Le Bon Usage – Grammaire française, édition refondue par André
Goosse. Paris – Louvain-la-Neuve : DeBoeck-Duculot, 13e edition.
G ROSS M. (1975). Méthodes en syntaxe. Hermann.
R IEGEL M., P ELLAT J.-C. & R IOUL R. (1997). Grammaire méthodique du français. PUF,
3e edition.
S AGOT B., C LÉMENT L., V ILLEMONTE DE LA C LERGERIE E. & B OULLIER P. (2006). The
Lefff 2 syntactic lexicon for French : architecture, acquisition, use. In Actes de LREC 06,
Gênes, Italie.
S AINT-D IZIER P. (2006). PrepNet : a Multilingual Lexical Description of Prepositions. In
LREC, Gènes, 12/05/2006-14/05/2006, p. 877–885 : European Language Resources Associa-
tion (ELRA).
S ELVA T., V ERLINDE S. & B INON J. (2002). Le DAFLES, un nouveau dictionnaire pour
apprenants du français. In Actes du dixième congrès EURALEX’2002 (European Association
for Lexicography). Copenhague.
S ÉRASSET G. & B OITET C. (2000). On UNL as the future "html of the linguistic content"
and the reuse of existing NLP components in UNL-related applications with the example of a
UNL-French deconverter. In Proceedings of COLING 2000, Saarebruecken, Germany.
VAN DEN E YNDE K. & M ERTENS P. (2002). La valence : l’approche pronominale et son
application au lexique verbal, In Journal of French Language Studies, p. 63–104. Cambridge
University Press, 13e edition.
228

Sylva : plate-forme de validation multi-niveaux de lexiques

Karën Fort Bruno Guillaume
LORIA / INRIA Nancy Grand-Est
Karen.Fort@loria.fr, Bruno.Guillaume@loria.fr
Résumé.         La production de lexiques est une activité indispensable mais complexe, qui né-
cessite, quelle que soit la méthode de création utilisée (acquisition automatique ou manuelle),
une validation humaine. Nous proposons dans ce but une plate-forme Web librement dispo-
nible, appelée Sylva (Systematic lexicon validator). Cette plate-forme a pour caractéristiques
principales de permettre une validation multi-niveaux (par des validateurs, puis un expert) et
une traçabilité de la ressource. La tâche de l’expert(e) linguiste en est allégée puisqu’il ne lui
reste à considérer que les données sur lesquelles il n’y a pas d’accord inter-validateurs.
Abstract.         Lexicon production is essential but complex and all creation methods (auto-
matic acquisition or manual creation) require human validation. For this purpose, we propose a
freely available Web-based framework, named Sylva (Systematic lexicon validator). The main
point of our framework is that it handles multi-level validations and keeps track of the resour-
ce’s history. The expert linguist task is made easier : (s)he has only to consider data on which
validators disagree.
Mots-clés :         Lexiques, plate-forme de validation, cadres de sous-catégorisation.

Keywords:           Lexicons, validation framework, subcategorization frames.

Karën Fort, Bruno Guillaume
1        Introduction

De nombreux outils de traitement automatique des langues (TAL) nécessitent des ressources
linguistiques et notamment des lexiques. La construction de ce type de ressources présente de
nombreuses difficultés :
– différents types de lexiques sont nécessaires en fonction des applications ;
– le nombre d’entrées à décrire est souvent important (de l’ordre de 500 000 pour les formes
fléchies du français par exemple) ;
– suivant la tâche visée, des informations plus ou moins détaillées sont nécessaires ;
– la façon de coder le contenu linguistique doit parfois être adaptée aux outils ou au moins aux
théories mises en jeu dans ces outils.
Pour la langue française, les lexiques morphologiques sont moins problématiques : ce type de
lexique ne pose pas de problème majeur quant au contenu linguistique qu’il doit contenir. On
dispose ainsi de lexiques morphologiques de bonne qualité avec une couverture satisfaisante
(Morphalou (Romary et al., 2004), partie morphologie du Lefff (Sagot et al., 2006), . . .). Le
problème est plus aigu pour les lexiques syntaxiques sur lesquels nous nous concentrons ici.
Des acteurs du TAL francophones travaillent ensemble régulièrement depuis deux ans sur le
développement de ressources syntaxiques du français (projet LexSynt1 ), le travail présenté ici
pour la validation est en partie issu des reflexions menées dans le cadre de ce projet. Il est im-
portant de noter que la plate-forme présentée n’est pas spécifique à ces lexiques et pourrait être
utilisée avec des lexiques sémantiques pour lesquels les problèmes sont encore plus marqués.
La construction manuelle de ressources par des experts linguistes est a priori la plus satisfai-
sante, mais la quantité de travail nécessaire rend cette méthode irréaliste dans la plupart des cas.
Quelques exemples de ressources manuelles existent néammoins : pour le français, on peut citer
DICOVALENCE (van den Eynde & Mertens, 2003), les tables du lexique-grammaire (Gross,
1975) ou la base lexicale de Dubois (Dubois & Dubois-Charlier, 1997). À l’opposé, des mé-
thodes de construction automatique de lexiques à partir de corpus ont été proposées : acqui-
sition de cadres de sous-catégorisation à partir de corpus (Preiss et al., 2007) ou de corpus
arborés (Kupsc, 2007). L’inconvénient de cette dernière méthode est évidemment la qualité du
lexique produit. En effet, la pertinence de chacune des entrées ne peut être qu’estimée et la cou-
verture dépend de la taille des corpus utilisés et de leur représentativité pour l’aspect considéré
dans le lexique.
Dans la pratique, le plus souvent, la production de lexique utilise des méthodes mixtes qui
automatisent une partie du travail et qui minimisent l’intervention humaine. L’acquisition au-
tomatique se fait sur des ressources existantes : des corpus bruts ou arborés, des dictionnaires
électroniques (construction de Morphalou à partir du TLFi (Pierrel et al., 2004)), ou d’autres
ressources informatisées mais qu’il faut adapter (contruction de SynLex (Gardent et al., 2006)
à partir des tables du lexique-grammaire). La ressource produite dépend alors de la qualité de
la ressource de départ et de la méthode de conversion. Dans le cas de SynLex, la ressource
produite souffre de nombreuses imperfections (Falk et al., 2007) et l’intervention humaine est
nécessaire pour valider a posteriori toute ou partie de la ressource.
La validation multi-niveaux permet de reporter une partie du travail humain vers des linguistes
moins expérimentés. Le premier niveau de validation est ainsi réalisé par plusieurs validateurs
non-experts, le deuxième niveau permet ensuite au linguiste expert de ne considérer que les cas
où les différents validateurs ont des avis divergents.
1
http://lexsynt.inria.fr

Sylva : plate-forme de validation multi-niveaux de lexiques
Nous proposons ici une plate-forme pour gérer la validation multi-niveaux de lexiques. Cette
plate-forme permet la gestion des deux types d’utilisateurs et automatise le passage par les deux
niveaux de validation. Après avoir décrit les fonctionnalités et l’implantation de la plate-forme,
nous montrons comment elle peut s’instancier avec un lexique syntaxique (SynLex) produit
automatiquement à partir des tables du lexique-grammaire.
2     Choix méthodologiques
Notre objectif premier est de permettre la validation rapide et complète d’une ressource à large
couverture. C’est cet objectif qui a guidé certains choix pour cette première version de l’outil.
2.1    Interface simplifiée

Pour chaque entrée du lexique, le validateur a le choix entre trois jugements : accepté, refusé,
ne sait pas. Il n’a donc pas la possibilité d’éditer une entrée pour la modifier. En effet, le but
de cette interface est de permettre au validateur de se prononcer rapidement sur chacune des
entrées. Nous sommes bien conscients que cela n’est pas toujours satisfaisant et que certaines
entrées devraient être corrigées plutôt que rejetées. C’est pourquoi, il est demandé au validateur
de motiver ses choix par des commentaires guidés. Il sera ainsi possible d’envisager une phase
ultérieure de correction des entrées qui posent problème, guidée par les commentaires de la
première phase.
Une autre conséquence de cette interface simplifiée sans possibilité d’édition est que chaque
entrée doit contenir une information suffisamment atomique. Par exemple, il n’est pas possible
de présenter des valences (ou cadres de sous-catégorisation, CSC) différentes avec une notation
factorisée en une seule entrée.
2.2    Ajout d’entrées a posteriori

Il n’est pas prévu pour l’instant d’ajouter des informations dans la ressource. En effet, la notion
de paquet (décrite en section 4.2), qui permet d’obtenir une série de vues transversales sur la
ressource, interdit le regroupement des entrées par lemme. Par conséquent, le validateur ne
peut se faire une idée sur l’ensemble des cadres d’un lemme et il ne peut donc pas juger de la
couverture du lexique pour un lemme donné. Ainsi, si l’ajout d’entrées existait, il ne pourrait
pas être systématique et ne serait pas homogène.
Le problème de complétion par des entrées manquantes nécessite selon nous d’autres méthodes
comme l’intégration d’autres ressources (Dicovalence, Lefff, . . .) ou l’étude sur corpus (éven-
tuellement guidée par un ou plusieurs analyseurs syntaxiques).
2.3    Lien facultatif vers des exemples

Dans la version actuelle de l’outil, l’ajout d’un exemple est facultatif lorsque l’on accepte un
cadre. Des expériences sont prévues pour comparer la vitesse de validation avec ou sans ajout
d’exemple.

Karën Fort, Bruno Guillaume
Quoi qu’il en soit, nous pensons qu’une validation, même sans exemple, produira une ressource
avec une forte valeur ajoutée et qu’il est possible dans une deuxième phase de combiner l’amé-
lioration de la couverture et l’ajout d’exemples.
Dans cette seconde phase, il sera sans doute plus efficace de proposer des exemples et de deman-
der au validateur de choisir le CSC qui correspond (éventuellement guidé par un ou plusieurs
analyseur(s) syntaxique(s)), plutôt que de lui demander de fournir lui-même un exemple.
3       La plate-forme Sylva

3.1     Fonctionnalités

La plate-forme Sylva est bâtie autour d’une base de données, conçue pour gérer à la fois les
informations linguistiques du lexique et les informations périphériques (source, historique de
validation, gestion des paquets (décrits en section 4.2)).
Nous donnons ci-dessous, par ordre d’importance décroissant, les principales fonctionnalités de
Sylva2 :

Validation multi-niveaux. Comme on l’a vu, il est difficile de demander à des experts lin-
guistes de valider un lexique dans son intégralité. Nous proposons donc d’utiliser comme
validateurs de premier niveau des linguistes non-experts (des étudiant(e)s en Master Sciences
du Langage, par exemple). Ainsi, dans la version actuelle de Sylva, chaque entrée est va-
lidée par deux validateurs différents. Lorsque les deux rendent le même verdict (positif
ou négatif), le jugement est automatiquement appliqué à l’entrée. Lorsque les jugements
diffèrent ou lorsque les validateurs hésitent, Sylva demande à l’expert linguiste de tran-
cher3 .
Traçabilité. Nous avons également décidé de conserver le plus d’information possible pour
chaque entrée. Outre la description linguistique, nous conservons dans la base de don-
nées les informations concernant la source de la ressource initiale (d’où proviennent les
données utilisées pour peupler la base) ainsi que celles concernant les processus de vali-
dation (le nom des validateurs, leur jugement, le jugement de l’expert s’il y a lieu, . . .).
Vue transversale. Le nombre d’entrées à valider peut être relativement important : le premier
lexique que nous allons valider avec Sylva contient par exemple près de 30 000 entrées.
Une validation globale de la ressource serait problématique : par exemple, il serait dif-
ficile pour l’expert de répartir les tâches aux différents validateurs. Il est donc possible
de décomposer la ressource en ensembles plus petits, linguistiquement cohérents. Dans
Sylva, ces ensembles correspondent à la notion de paquet (voir section 4.2).
3.2     Acteurs

L’interface utilisateur est accessible sur le Web. Trois types d’acteurs existent : l’invité, le vali-
dateur et l’expert. Pour ces deux derniers, la connexion se fait par identifiant et mot de passe.
2
la version en cours de développement est disponible ici : http://sylva.loria.fr/
3
Sylva est prévu pour permettre la validation par plus de deux validateurs, il faut dans ce cas paramétrer les
règles de décisions pour l’accord (unanimité, majorité, . . .)

Sylva : plate-forme de validation multi-niveaux de lexiques
3.2.1    L’utilisateur invité

L’invité peut :
– voir la ressource dans son état actuel. Plusieurs types de vues sont disponibles, par lemme
ou par initiale, par exemple. Il sera également possible à terme de filtrer les entrées selon des
critères plus fins (verbes possédant tel type de valence, verbes dont le lemme correspond à
une expression rationnelle donnée, etc.).
– exporter le lexique dans un format texte ou XML respectant la norme Lexical Markup Fra-
mework (Francopoulo et al., 2006).
– poster des questions, des suggestions via une interface d’envoi de courriers électroniques.
3.2.2    Le validateur

Le rôle du validateur est de juger toutes les entrées des paquets qui lui sont assignés. Pour
chaque paquet, une page Web s’affiche. Les informations concernant une entrée sont présentées
de manière condensée sur une seule ligne. Le validateur peut :
– accepter l’entrée et éventuellement en donner un exemple d’utilisation,
– rejeter l’entrée en donnant une explication pour ce rejet (l’interface de validation suggère
certaines explications afin d’aider le validateur à formuler plus rapidement son jugement),
– exprimer un doute (Ne sait pas).
En outre, quelle que soit sa décision, le validateur peut toujours ajouter un commentaire.
3.2.3    L’expert

Les principaux rôles de l’expert sont de :
– créer et gérer les comptes des validateurs, et ce afin d’assurer la souplesse de validation,
– assigner les paquets aux validateurs,
– finaliser la validation d’un paquet une fois les validations de premier niveau terminées, c’est-
à-dire trancher les cas pour lesquels il n’y a pas d’accord inter-validateurs.
Il est important de noter que l’expert peut également valider directement les entrées, court-
circuitant ainsi les validateurs. Cette fonctionnalité a été prévue pour faire face à des cas ex-
ceptionnels, comme par exemple l’incapacité pour un validateur de terminer sa tâche ou pour
accélérer la validation.
3.3     Qualité

Sylva a été conçue et développée dans le respect des techniques de développement les plus ré-
centes. En particulier, cette plate-forme a fait l’objet de spécifications UML (Unified Modeling
Language) détaillées, validées par les utilisateurs. Nous avons par ailleurs utilisé un framework
de développement PHP robuste, bien documenté et très utilisé, symfony (Potencier & Zaninotto,
2007), qui offre :
–   séparation modèle/objet et Modèle/Vue/Contrôleur (Gamma et al., 1995),
–   indépendance à l’implémentation de la base de données (Object Relational Mapping),
–   le support du multilinguisme (encodage UTF-8) et de l’internationalisation,
–   une couche sécurité performante, permettant de protéger la base contre les intrusions,

Karën Fort, Bruno Guillaume
– des performances accrues grâce à l’utilisation d’AJAX (Asynchronous JavaScript And XML).
Cela facilite une conception propre et l’écriture d’un code lisible, rendant de ce fait la mainte-
nance, les modifications et la localisation plus faciles.
Par ailleurs, une documentation utilisateur détaillée et adaptée au statut de l’utilisateur est dis-
ponible en ligne, afin de permettre à la fois à l’expert linguiste et aux validateurs de bénéficier
au mieux des fonctionnalités de Sylva. Une documentation développeur sera également fournie
pour permettre d’adapter l’outil à d’autres types de ressources.
4     Un exemple de mise en œuvre de Sylva

Une instanciation de la plate-forme est une description précise des informations associées aux
entrées du lexique. À partir de cette description, nous devons adapter l’interface utilisateur
correspondante pour la validation. Il faut notamment adapter la partie de l’interface qui permet
au validateur de motiver son jugement en cas de refus.
4.1    La ressource utilisée

La validation à grande échelle d’une première ressource est en cours. Nous avons choisi de com-
mencer par le lexique syntaxique SynLex (Gardent et al., 2006). En effet, malgré des défauts
identifiés (Falk et al., 2007), SynLex est construit à partir d’une ressource de qualité (les tables
du lexique-grammaire français (Gross, 1975)) qui est l’une des seules à concilier couverture
large et informations détaillées. Il est à noter que SynLex ne fait pas de distinction de sens pour
un même lemme car il a été construit uniquement par extraction des informations syntaxiques
contenues dans les tables du lexique-grammaire.
La validation de SynLex va permettre d’en améliorer la précision ; l’amélioration de la couver-
ture se fera à l’aide d’autres méthodes (cf. section suivante).
Dans Synlex, une entrée se compose d’un verbe, d’une liste d’arguments syntaxiques ayant
un rôle sémantique (a0, a1, ...), d’une liste optionnelle d’associés (arguments régis par le verbe
mais ne remplissant pas de rôle sémantique, Ilimp pour « il impersonnel », par exemple) et d’une
liste de macros (informations supplémentaires sur les propriétés syntaxiques du verbe comme
l’auxiliaire ou la passivisation). Les associés et les macros sont des listes finies d’atomes.
Chaque argument est décrit par : fonction grammaticale (suj, objde, ...), marqueur optionnel
(de, pour, ...), catégorie syntaxique (scompl, sn, ...), restrictions morphologiques (pluriel obli-
gatoire, par exemple), restrictions sémantiques (humain, non humain, abstrait).
Construit à partir de 60% des tables, la version de SynLex utilisée pour peupler la base décrit
3 494 verbes différents et 26 714 entrées. La figure ci-dessous décrit quelques entrées associées
au lemme « abuser » (celles qui apparaissent dans l’interface de la figure 2), ainsi que quelques
entrées associées au lemme « épater » qui font apparaître d’autres éléments (marqueur de, macro
être).

Sylva : plate-forme de validation multi-niveaux de lexiques
abuser   <a0:suj:sinf:::>
abuser   <a0:suj:sn:::humain>
abuser   <a0:suj:sn:::non_humain>
abuser   <a0:suj:scompl:::>
abuser   <a0:suj:sn:::non_humain,a1:obj:sn:::humain>
abuser   <a0:suj:sinf:::,a1:obj:sn:::humain>
abuser   <a0:suj:sn:::humain,a1:obj:sn:::humain>
abuser   <a0:suj:scompl:::,a1:obj:sn:::humain>

épater <a1:suj:sn:::humain,a0:objde:de-scompl:::>; etre, participe-passe
épater <a1:suj:sn:::humain,a0:objde:de-sn:::abstrait>; etre, participe-passe
F IG . 1 – Exemples d’entrées de SynLex.
F IG . 2 – Interface du validateur pour SynLex (version béta).
4.2      Les paquets

Afin de rendre la phase de validation la plus efficace possible, la ressource est décomposée
en ensembles, linguistiquement cohérents et de taille raisonnable (entre 50 et 210 entrées), les
paquets. Etant donnée la taille de SynLex, ceux-ci ont été construits de manière automatique,
avant le peuplement de la base, notre but étant de réunir les entrées ayant un comportement
semblable, afin d’en simplifier la validation.
Une première étape de tri consiste à regrouper les entrées possédant le même CSC. Les groupes
suffisamment gros forment directement des paquets : 109 paquets sont produits ainsi. Les en-
trées restantes sont regroupées par table d’origine : 110 paquets. Enfin, le reste est arbitrairement
réparti dans les 9 derniers paquets. Au final, 228 paquets peuplent la base.
4.3      Des interfaces adaptées

La validation de SynLex a donné lieu au développement d’interfaces spécifiques, qui, grâce à la
séparation Modèle/Vue/Contrôleur sont facilement adaptables à d’autres lexiques. Des captures
d’écrans des interfaces de validation des validateurs et de l’expert pour la ressource actuelle
sont présentées respectivement en figures 2 et 3. L’interface de gestion de l’expert est quant à
elle présentée en figure 4.

Karën Fort, Bruno Guillaume
F IG . 3 – Interface de l’expert pour SynLex (version béta).
F IG . 4 – Interface générale de l’expert pour SynLex (version béta).

Sylva : plate-forme de validation multi-niveaux de lexiques
5     Développements futurs

5.1    Intégration d’autres ressources

SynLex a été contruit à partir d’une ressource de grande qualité, produite manuellement, mais il
reste limité à une partie des verbes (seules 60% des tables de lexique-grammaire des verbes sont
disponibles librement). Il est par conséquent nécessaire de le fusionner avec d’autres ressources
pour obtenir un lexique syntaxique du français générique.
Le Lefff (lexique des formes fléchies du français) est un lexique du français à large couverture
qui fournit des informations morphologiques et syntaxiques, non seulement pour les verbes,
mais aussi pour toutes les autres catégories morphosyntaxiques. Le Lefff surgénère souvent :
par défaut, à tout nom sont affectés tous les CSC possibles pour les noms. Cette ressource
est donc une bonne candidate pour une validation par filtrage des entrées. Le format du Lefff
étant très proche de celui de SynLex, il peut être entré facilement dans la plate-forme actuelle.
Cependant, les informations de sous-catégorisation sont très factorisées dans le Lefff, il faudra
en tenir compte lors de son intégration soit en dépliant les cadres factorisés, soit en permettant
au validateur de décrire plus finement les motivations de son choix.
Bien que nécessitant un travail linguistique conséquent, la traduction des informations de DICO-
VALENCE (van den Eynde & Mertens, 2003), lexique de grande qualité des valences verbales
du français, permettrait d’améliorer significativement la qualité du lexique envisagé.
5.2    Intégration d’autres fonctionnalités

Comme nous l’avons déjà évoqué, la première fonctionnalité que nous souhaitons ajouter à
Sylva est une interface Web permettant de relier des usages de verbes à des exemples. En-
core une fois, il s’agit là d’une tâche longue et pénible. Nous envisageons d’utiliser pour cela
des exemples extraits de corpus arborés, de corpus de textes ou de dictionnaires. Pour chaque
exemple, l’utilisateur devra sélectionner l’une des entrées du lexique. Grâce aux informations
fournies par le corpus arboré ou par le résultat d’une analyse syntaxique, on pourra réduire le
nombre de possibilités proposées au validateur et ainsi lui faciliter la tâche.
Une autre fonctionnalité importante serait la possibilité de modifier les entrées. En utilisant les
commentaires accompagnant les refus de la première phase, on pourra proposer au validateur
des hypothèses de corrections de l’entrée parmi lesquelles il pourra choisir.
On peut imaginer que des entrées manquantes pourraient être détectées, y compris après l’inté-
gration d’autres ressources, par exemple dans la phase de liage des exemples à partir de corpus.
Il faudrait donc prévoir une interface pour ajouter de nouvelles entrées, au moins pour l’utilisa-
teur expert.
Le développement de ces fonctionnalités nécessite un travail non négligeable, mais nous pen-
sons que notre plate-forme est suffisamment générique et adaptable pour en faciliter et en accé-
lérer l’implémentation.

Karën Fort, Bruno Guillaume
Remerciements
Ce travail a été réalisé grâce à des fonds provenant du CPER MISN de la Région Lorraine.
Les auteurs souhaitent remercier les autres membres du projet pour leur participation active aux
spécifications : Claire Gardent, Guy Perrier et Ingrid Falk. Nous remercions également Mathieu
Morey et François-Régis Chaumartin pour leur aide lors de la phase de développement.
Références
D UBOIS J. & D UBOIS -C HARLIER F. (1997). Les Verbes Français. Paris, France : Larousse-
Bordas.
FALK I., F RANCOPOULO G. & G ARDENT C. (2007). Évaluer SynLex. In Actes de TALN 07,
p. 335–344, Toulouse.
F RANCOPOULO G., G EORGE M., C ALZOLARI N., M ONACHINI M., B EL N., P ET M. &
S ORIA C. (2006). Lexical Markup Framework (LMF). In Actes de LREC 06, Gène, Italie.
G AMMA E., H ELM R., J OHNSON R. & V LISSIDES J. (1995). Design Patterns : Elements
of Reusable Object-Oriented Software. Boston, MA, USA : Addison-Wesley Longman Publi-
shing Co., Inc.
G ARDENT C., G UILLAUME B., P ERRIER G. & FALK I. (2006). Extraction d’information de
sous-catégorisation à partir des tables du LADL. In Actes de TALN 06, p. 139–148, Louvain.
G ROSS M. (1975). Méthodes en syntaxe. Hermann.
K UPSC A. (2007). Extraction automatique de cadres de sous-catégorisation verbale pour le
français à partir d’un corpus arboré. In Actes de TALN 07, Toulouse, France.
P IERREL J., D ENDIEN J. & B ERNARD P. (2004). Le TLFi ou Trésor de la Langue Française
informatisé. In Actes de EURALEX 04, Lorient, France.
P OTENCIER F. & Z ANINOTTO F. (2007). The Definitive Guide to symfony. Berkeley, CA,
USA : Apress.
P REISS J., B RISCOE T. & KORHONEN A. (2007). A system for large-scale acquisition of
verbal, nominal and adjectival subcategorization frames from corpora. In Actes de ACL 07,
Prague, République tchèque.
ROMARY L., S ALMON -A LT S. & F RANCOPOULO G. (2004). Standards going concrete :
from LMF to Morphalou. In M. Z OCK, Ed., COLING 2004 Enhancing and using electronic
dictionaries, p. 22–28, Genève, Suisse.
S AGOT B., C LÉMENT L., V ILLEMONTE DE LA C LERGERIE É. & B OULLIER P. (2006). The
Lefff 2 syntactic lexicon for French : architecture, acquisition, use. In Actes de LREC 06,
Gène, Italie.
VAN DEN E YNDE K. & M ERTENS P. (2003). La valence : l’approche pronominale et son
application au lexique verbal. Journal of French Language Studies, 13, 63–104.

ANNODIS: une approche outillée
de l'annotation de structures discursives

Marie-Paule Péry-Woodley (1), Nicholas Asher (2), Patrice Enjalbert (3),
Farah Benamara(2), Myriam Bras (1), Cécile Fabre (1), Stéphane Ferrari (3),
Lydia-Mai Ho-Dac (1), Anne Le Draoulec (1), Yann Mathet (3), Philippe
Muller (2), Laurent Prévot (4), Josette Rebeyrolle (1), Ludovic Tanguy (1),
Marianne Vergez-Couret (1), Laure Vieu (2), Antoine Widlöcher (3)

(1) CLLE-ERSS – Université de Toulouse UTM
{pery,bras,cecile.fabre,hodac,draoulec,rebeyrol,vergez}@univ-tlse2.fr
(2) IRIT – Université de Toulouse UPS
{asher,benamara,philippe.muller,vieu}@irit.fr
(3) GREYC – Université de Caen
{patrice.enjalbert,stephane.ferrari,mathet,awidloch}@info.unicaen.fr
(4) Laboratoire Parole et Langage – Université de Provence
laurent.prevot@lpl-aix.fr
Résumé.
Le projet ANNODIS vise la construction d’un corpus de textes annotés au niveau discursif
ainsi que le développement d'outils pour l’annotation et l’exploitation de corpus. Les
annotations adoptent deux points de vue complémentaires : une perspective ascendante part
d'unités de discours minimales pour construire des structures complexes via un jeu de
relations de discours ; une perspective descendante aborde le texte dans son entier et se base
sur des indices pré-identifiés pour détecter des structures discursives de haut niveau. La
construction du corpus est associée à la création de deux interfaces : la première assiste
l'annotation manuelle des relations et structures discursives en permettant une visualisation du
marquage issu des prétraitements ; une seconde sera destinée à l'exploitation des annotations.
Nous présentons les modèles et protocoles d'annotation élaborés pour mettre en œuvre, au
travers de l'interface dédiée, la campagne d'annotation.

Abstract
The ANNODIS project has two interconnected objectives: to produce a corpus of texts
annotated at discourse-level, and to develop tools for corpus annotation and exploitation. Two
sets of annotations are proposed, representing two complementary perspectives on discourse
organisation: a bottom-up approach starting from minimal discourse units and building
complex structures via a set of discourse relations; a top-down approach envisaging the text as
a whole and using pre-identified cues to detect discourse macro-structures. The construction
of the corpus goes hand in hand with the development of two interfaces: the first one supports
manual annotation of discourse structures, and allows different views of the texts using NLP-

M-P. Péry-Woodley, N. Asher et P. Enjalbert

based pre-processing; another interface will support the exploitation of the annotations. We
present the discourse models and annotation protocols, and the interface which embodies
them.
Mots-clés :
annotation de corpus, structures de discours, interface d'annotation
Keywords:
corpus annotation, discourse structures, annotation tools
1. ANNODIS : annotation discursive de corpus
Le projet ANNODIS1 vise la constitution d’un corpus de français écrit enrichi d’annotations
concernant le niveau discursif. Tout en se situant dans le sillage de projets d’annotation de
relations et structures de discours existants pour l’anglais – e.g. Penn Discourse Treebank
(Prasad et al., 2006) – il s’en démarque sur plusieurs points : sa principale originalité est
d’aborder l’organisation discursive à partir de deux approches complémentaires – ascendante
et descendante (macro-structures) ; deuxièmement, les annotations s’appliquent à un corpus
diversifié pour permettre la prise en compte de réalisations discursives variées ; ce corpus fait
l’objet de prétraitements automatiques pour guider les annotations ; enfin, le développement
d’outils d’aide à l’annotation et à la navigation constitue un objectif majeur du projet.

Les sections 2 et 3 présentent les deux points de vue théoriques et les choix méthodologiques
sur lesquels se fondent les annotations ascendantes et descendantes respectivement. La section
4 est consacrée à l’interface, au cœur du dispositif puisqu’elle donne corps aux deux modèles
d’annotation, et permettra leur mise en relation.
2. Annoter des relations de discours : l’approche ascendante
Le point de vue « ascendant » s’inscrit dans le cadre des théories du discours qui s'attachent à
construire une structure complète d'un discours, essentiellement vu comme un ensemble
d'unités élémentaires reliées par des relations de cohérence (dites rhétoriques). Cette vision
théorique du discours rassemble principalement les travaux de la RST (Mann et Thompson,
1987), la LDM (Polanyi et al., 2004 ; Wolf et Gibson, 2005), DLTAG (Forbes et al, 2003), le
PDTB cité ci-dessus, et la SDRT de (Asher et Lascarides, 2003), qui a servi de point de départ
à cette partie du projet. Les relations de cohérence considérées se divisent généralement en
plusieurs groupes (causalité, structuration, discours rapporté, élaboration, etc.). La plupart de
ces théories permettent de définir des structures hiérarchiques en créant des segments
complexes à partir de segments élémentaires de façon récursive. La SDRT permet de plus
d'avoir une structure de graphe plus expressive.
1
Projet financé pour 3 ans par l’ANR Programme Sciences Humaines et Sociales Appel 2007 Corpus et
outils de la recherche en sciences humaines et sociales.

ANNODIS : annotation de structures discursives

Le point de vue ascendant est focalisé sur la représentation précise et complète de la structure
d'un texte et est donc plus adapté sur des textes ou des unités textuelles de petite à moyenne
taille, et vise donc à s'articuler avec la représentation descendante présentée dans la section
suivante.

L’annotation dans le point de vue « ascendant » commence avec la segmentation d’un texte en
unités de discours élémentaires. Nous avons développé un manuel d'annotation qui indique
comment trouver ces unités élémentaires. Ces segments correspondent aux propositions, mais
également aux syntagmes prépositionnels, adverbiaux détachés à gauche (par ex. les
adverbiaux temporels et spatiaux) et incises. Sémantiquement, il est important que chaque
segment contienne la description d’une éventualité (évènement ou état). Pour les relations de
base, nous avons pris un ensemble restreint, c’est à dire les relations à peu près communes à
toutes les théories de discours. Nous avons utilisé les résultats de travaux antérieurs sur ces
relations et leurs déclencheurs, ainsi que sur les adverbiaux temporels, les temps verbaux, les
changements aspectuels, pour guider le choix de la relation et du point d’attachement, et
préciser les relations et leurs indices. Notre manuel d'annotation contient des descriptions pour
chacune de ces relations. Nous avons commencé une campagne préliminaire d'annotation avec
deux étudiants en linguistique de l'université de Toulouse pour raffiner notre manuel
d'annotation et faire des premiers tests d'accord inter-annotateurs.

Nous avons aussi commencé à développer un segmenteur automatique fondé sur des indices
lexicaux et syntaxiques, pour préparer le travail des annotateurs et à terme les soulager
complètement de cette partie de la tâche. D'un point de vue procédural, on peut séparer le
problème de l'annotation ascendante en trois tâches : le repérage des segments, la
détermination des paires de segments à relier, et le typage de la relation entre ces paires de
segments reliés (sous certaines contraintes pragmatiques globales).
3. Annoter des structures discursives : l’approche descendante
L'approche « descendante » vise l'annotation de structures discursives de haut niveau. Elle
s’ancre dans les recherches sur les indices de discontinuité qui délimitent des segments à
différents niveaux de grain, segments qui sont susceptibles d’être mis en relation avec une
organisation fonctionnelle. Les textes sont envisagés dans leur dimension de document (cf.
Péry-Woodley et Scott, 2006) et nous nous intéressons spécifiquement à leur mise en texte (y
compris dans ses aspects visuels, ou mise en espace (cf. Luc et Virbel, 2001)). La mise en
texte est premièrement abordée à partir d’une caractéristique incontournable : la séquentialité.
Du point de vue de la séquentialité, deux stratégies sont possibles à tout moment du texte
(pour le scripteur, et partant, pour le lecteur) : continuation avec ce qui précède ou
rupture/transition (shift). La continuation étant la stratégie par défaut, les ruptures/transitions
doivent faire l’objet d’une signalisation. Ainsi les indices typographiques et dispositionnels
mettent à part une citation, un élément de discours rapporté, un titre… Plus intégrés dans le
texte, les indices de discontinuité lexico-syntaxiques (marqueurs de segmentation (Bestgen,
2000) ; shift signals (Goutsos, 1996)) avertissent le lecteur d’un changement thématique ou
rhétorique : adverbiaux détachés à l’initiale (cf. notion de cadre de discours (Charolles et
Vigier, 2005)), redénominations (Schnedecker, 1997).

Le modèle d'annotation pour l’annotation descendante est centré essentiellement sur une méta-
structure : la structure énumérative. Stratégie de base de la mise en texte à différents niveaux

M-P. Péry-Woodley, N. Asher et P. Enjalbert

de grain, la structure énumérative ne peut se définir et s’interpréter qu’à partir des indices qui
la signalent (Luc et Virbel, 2001). Elle est pourtant loin d’être une simple question de
formatage, et la multiplicité de ses réalisations et de ses rôles fonctionnels en fait un bon point
d’entrée dans la complexité de l'organisation discursive2 : listes formatées, découpage en
sections, en paragraphes, listes « plates » à l'intérieur des paragraphes, autant de déclinaisons
d’une structure où chaque item (unité énumérée) est caractérisé par une continuité, et se
trouve en discontinuité par rapport à ses items voisins. A un niveau plus global, une structure
énumérative constitue un segment caractérisé par une continuité tant au plan de la mise en
texte qu’au plan thématique.

L’annotation manuelle se base sur une visualisation des textes enrichis d’indices potentiels
des structures recherchées : indices typographiques et dispositionnels (titres de sections, listes,
changement de paragraphe) ; indices lexico-syntaxiques (expressions co-référentielles,
adverbiaux ou connecteurs détachés à l’initiale des phrases, paragraphes ou sections) ;
redénominations démonstratives (y compris métadiscursives e.g. cette analyse) ; marqueurs
d’énumération ; parallélismes structurels. Ce marquage est fondé sur différentes procédures de
repérage automatique qui s’appuient sur la structure de document, ainsi que sur les sorties
d’un étiquetage morpho-syntaxique (TreeTagger) et d’une analyse syntaxique (SYNTEX
(Bourigault, 2007)). L’annotateur doit pouvoir dans un premier temps « scanner » le texte
dans une approche globale, à la recherche de zones de concentration d'indices de
discontinuité, pour, dans un second temps, délimiter précisément les segments par une lecture
plus ou moins précise.
4. L’interface d’annotation
Il découle de ce qui précède qu’un impératif du projet ANNODIS est de disposer d’un outil
d’annotation manuelle du discours permettant de travailler à différents niveaux de grain sur un
même document, et assez souple pour s’adapter à différentes campagnes d’annotation. Aucun
outil ne répondant à notre connaissance à ce cahier des charges, nous avons alloué une partie
de nos ressources à la conception et au développement d’un tel logiciel.

La plateforme Glozz qui en résulte est suffisamment générique pour s’adapter aux modèles
d’annotation utilisés dans les différentes campagnes d’ANNODIS, et pourra probablement
être utilisée à d’autres fins à l’issue de ce projet. Les quatre points suivants permettent d’en
comprendre les principes :

- Nous disposons simultanément de deux « vues » sur le texte de travail. La vue principale [1]
est la fenêtre de travail, dans laquelle les annotations sont saisies à la souris, tandis que la vue
[2], appelée « ruban », présente le texte 'vu de haut'. Dans ces deux vues, les indices pré-
définis sont associés à des styles. De tels styles sont également appliqués aux annotations
effectuées. Ainsi, la vue "ruban" permet une appréhension globale du marquage, et la
navigation rapide au sein des zones de concentration d’indices.

- Une boîte à outils [3] permet de choisir parmi les trois types d’objets d’annotation. Les
« unités » sont des blocs (apparaissant sous forme de cadres colorés dans [1] et [2]). Les
2
D’autres schémas structurels pourront être ultérieurement pris en compte.

ANNODIS : annotation de structures discursives

« relations » permettent de relier deux unités (apparaissant sous forme de lignes transverses
dans [1] et [2]), et les « schémas » permettent d’agglomérer unités et relations en une même
entité. Par exemple, pour représenter une chaîne référentielle, on pourrait créer autant d’unités
que de marques anaphoriques, puis mettre en relation les différentes marques, et enfin créer un
schéma intégrant toutes ces marques et leurs relations.

- Pour une campagne donnée, ou pour l’une de ses sous-tâches, un modèle d’annotation peut
être chargé afin de définir les catégories disponibles pour chacun des trois types d’objet (par
exemple des unités de catégorie « Amorce », des relations de catégorie « Elaboration », des
schémas de catégorie « Structure énumérative » etc.). Ces catégories apparaissent dans les
trois colonnes de [5], qui permettent d’assigner la catégorie souhaitée à chaque annotation. Le
modèle d’annotation prévoit aussi d’associer un jeu d’attributs/valeurs à chaque catégorie,
lequel peut être vu et modifié dans la fenêtre [4]. Des styles (notamment des couleurs) sont
associés aux différents types, et peuvent être modifiés grâce à l’interface [7].

- Le document traité peut être enrichi d’un marquage préalable, qui peut être issu d’une
segmentation ou d’un repérage automatique d’indices ou de structures comme exposé en 3 ;
ou résulter d’une précédente campagne d’annotation que l’on souhaite enrichir ou sur laquelle
on souhaite s’appuyer pour étudier de nouvelles structures.

Enfin, différents outils de recherche et de navigation [6] peuvent être ajoutés ou retirés à la
demande.                                                             4
4
Figure 1 : La plateforme d’annotation Glozz
5. Enjeux et retombées

La constitution d’un corpus de textes diversifiés, dont beaucoup sont longs et complexes sur
le plan de la structure de document, avec prise en compte de cette structure « logique » et

M-P. Péry-Woodley, N. Asher et P. Enjalbert

préservation de la mise en forme matérielle est en soi novateur. L’annotation systématique de
relations et de structures discursives dans ce corpus diversifié fournira des données précieuses
pour la description linguistique et la théorisation des fonctionnements discursifs. Déjà la
rédaction des guides d’annotation a conduit à un travail considérable d’affinement et de
précision des modèles. La construction d’outils logiciels satisfaisant aux exigences des deux
approches ascendante et descendante a exigé une modélisation qui en garantit la robustesse et
la généricité. Enfin, la confrontation/fusion des deux approches à travers cette double
annotation est un enjeu majeur du projet.
Références
ASHER, N., LASCARIDES, A. (2003). Logics of conversation. Cambridge; New York:
Cambridge University Press.

BESTGEN, Y., VONK, W. (2000). The role of temporal segmentation markers in discourse
processing. Disourse Processes 19, 385-406.

BOURIGAULT D. (2007) Un analyseur syntaxique opérationnel : SYNTEX. Mémoire d'HDR en
sciences du langage, Université de Toulouse 2, France.

CHAROLLES, M., & VIGIER, D. (2005). Les adverbiaux en position préverbale: portée cadrative
et organisation des discours. Langue Française 148, 9-30.

FORBES, K., MILTSAKAKI, E., PRASAD, R., SARKAR, A., JOSHI, A., WEBBER, B. (2003). D-
LTAG System: Discourse parsing with a lexicalized tree-adjoining grammar. Journal of
Logic, Language and Information, 12(3), 261-279.

GOUTSOS, D. (1996). A model of sequential relations in expository text. Text 16(4), 501-533.

LUC, C., VIRBEL, J. (2001). Le modèle d'architecture textuelle : fondements et
expérimentation. VERBUM 23(1), 103-123.

MANN, W., THOMPSON, S. (1987). Rhetorical Structure Theory: a theory of text organization.
Information Science Institute.

PÉRY-WOODLEY, M.-P., SCOTT, D. (2006). Computational Approaches to Discourse and
Document Processing. TAL 47(2), 7-19.

POLANYI, L., VAN DEN BERG, M., CULY, C., THIONE, G. L., AHN, D. (2004). A Rule Based
Approach to Discourse Parsing. Actes de SIGDIAL 2004. Boston.

PRASAD, R., MILTSAKAKI, E., DINESH, N., LEE, A. JOSHI, A. (2006). The Penn Discourse
TreeBank 1.0 Annotation Manual. http://www.seas.upenn.edu/~pdtb/papers/pdtb-1.0-
annotation-manual.pdf

SCHNEDECKER, C. (1997). Nom propre et chaînes de référence. Paris: Klincksieck.

WOLF, F., & GIBSON, E. (2005). Representing Discourse Coherence: A Corpus Based Study.
Computational Linguistics, 31(2), 249–287.

Détection des contradictions
dans les annotations sémantiques ∗

Yue Ma, Laurent Audibert
Laboratoire d’Informatique de l’université Paris-Nord (LIPN) - UMR 7030
Université Paris 13 - CNRS
99, avenue Jean-Baptiste Clément - F-93430 Villetaneuse, France
first-name.family-name@lipn.univ-paris13.fr

Résumé.         L’annotation sémantique a pour objectif d’apporter au texte une représentation
explicite de son interprétation sémantique. Dans un précédent article, nous avons proposé d’étendre
les ontologies par des règles d’annotation sémantique. Ces règles sont utilisées pour l’annotation
sémantique d’un texte au regard d’une ontologie dans le cadre d’une plate-forme d’annotation
linguistique automatique. Nous présentons dans cet article une mesure, basée sur la valeur de
Shapley, permettant d’identifier les règles qui sont sources de contradiction dans l’annotation
sémantique. Par rapport aux classiques mesures de précision et de rappel, l’intérêt de cette me-
sure est de ne pas nécessiter de corpus manuellement annoté, d’être entièrement automatisable
et de permettre l’identification des règles qui posent problème.
Abstract. The semantic annotation has the objective to bring to a text an explicit repre-
sentation of its semantic interpretation. In a preceding article, we suggested extending ontolo-
gies by semantic annotation rules. These rules are used for the semantic annotation of a text
with respect to an ontology within the framework of an automated linguistic annotation plat-
form. We present in this article a measure, based on the Shapley value, allowing to identify the
rules which are sources of contradictions in the semantic annotation. With regard to the clas-
sic measures, precision and recall, the interest of this measure is without the requirement of
manually annotated corpus, completely automated and its ability to identify rules which raise
problems.
Mots-clés :           Annotation sémantique, valeur de Shapley, plate-forme d’annotation.

Keywords:             Semantic annotation, Shapley value, annotation platform.
1        Introduction
La modélisation des connaissances d’un domaine de spécialité par le biais des ontologies et
l’annotation linguistique de documents textuels, constituent deux problématiques cruciales de
l’ingénieurie des connaissances et du traitement automatique des langues. Ces deux probléma-
tiques ont été largement étudiées indépendamment, mais la littérature concernant l’annotation
sémantique au regard d’une ontologie n’est pas très importante. Les travaux dans ce domaine
Ce travail a été réalisé dans le cadre du programme Quaero, financé par OSEO, agence nationale de valori-
sation de la recherche.

Y. MA, L. Audibert
s’appuient généralement sur des ontologies enrichies de connaissances linguistiques permettant
de mettre en correspondance des éléments de l’ontologie avec des fragments de textes.
Dans un précédent article (Ma et al., 2009), nous avons proposé d’étendre les ontologies par
des règles d’annotation sémantique plutôt que par l’ajout d’un niveau linguistique. Cette solu-
tion s’intègre mieux dans le cadre des plates-formes d’annotation puisqu’elle s’appuie sur les
étages inférieurs d’annotations linguistiques de la plate-forme. Cette solution présente égale-
ment l’avantage de distinguer clairement le processus d’analyse linguistique et la tâche d’inter-
prétation sémantique, qui seule met l’ontologie en jeu.
L’un des intérêts de cette extension à base de règles est que ces dernières sont inférables par un
ordinateur pour pouvoir être apprises (semi-)automatiquement à partir d’un corpus sémantique-
ment annoté selon l’ontologie à étendre. Cependant, un apprentissage automatique de ces règles
générera inévitablement du bruit qui se traduira par des erreurs d’annotation. L’objectif du pré-
sent article est justement de présenter une mesure, basée sur la valeur de Shapley, permettant
d’identifier automatiquement les règles qui sont sources de contradiction dans les annotations
sémantiques.
2     Extension à base de règles d’annotation

2.1   Différents types d’annotation
F IG . 1 – Exemple d’annotation de "Marie lit une pièce de théâtre de Molière"
Nous distinguons les types d’annotations sémantiques selon la nature de l’élément ontologique
auquel elles se rattachent.
Certains mots ou expressions renvoient à des instances de concepts. On les désigne traditionnel-
lement sous le terme d’entités nommées car ils renvoient à des entités référentielles de manière
autonome et conventionnelle, comme le mot Marie dans l’exemple de la figure 1. Le processus
d’annotation consiste à créer des instances de concepts et à leur rattacher ces entités nommées.
Certains mots ou expressions dénotent des concepts. Ils constituent généralement le vocabulaire
spécialisé du domaine considéré, i.e. la terminologie du domaine. Ces termes (par ex. pièce de

Détection des contradictions dans les annotations sémantiques
théâtre) sont souvent composés de plusieurs mots et ils sont importants à repérer parce qu’ils
sont sémantiquement plus pertinents que les mots qui les composent.
Certains mots ou expressions dénotent des rôles conceptuels. De même que les termes peuvent
être rattachés à des concepts dans le processus d’annotation, ils peuvent être rattachés à des
rôles si les notions sous-jacentes ont été modélisées sous la forme de rôles plutôt que comme
concepts. C’est le cas de lit dans l’exemple de la figure 1, mais le fragment de texte annoté est
souvent plus large, les rôles s’exprimant souvent par tournures de phrases qui ne se réduisent
pas à un mot-clef.
Certains fragments de texte renvoient à des relations entre instances : une pièce de théâtre de
Molière dans l’exemple de la figure 1. C’est souvent un large fragment qui est annoté comme
une relation entre instances.
Certains fragments textuels, enfin, expriment des axiomes ontologiques. Si on était capable
d’analyser une phrase comme les pièces de théâtre sont toujours écrites par quelqu’un, on
pourrait de la même manière l’associer à un axiome exprimant une restriction de cardinalité du
rôle a-pour-auteur.
2.2    Définition de l’extension

Soit O = C, R, I, RI, A une ontologie composée d’un ensemble de concepts (C), de rôles
(R), d’instances (I), de relations entre instances (RI) et d’axiomes (A) et R = RC , RR , RI ,
RRI , RA , un ensemble de règles permettant d’annoter des fragments de textes en les reliant
à des concepts (RC ), des rôles (RR ), des instances (RI ), des relations entre instances (RRI )
ou des axiomes (RA ). Une règle est la donnée d’un couple (P, C) où P (Prémisse) décrit les
conditions qu’un segment de texte doit vérifier pour être annoté et C (Conclusion) indique
comment annoter le segment. Nous disons qu’une ontologie OR est étendue si et seulement si :
– pour chaque concept c de C il existe un couple de règles (Cc , Ic ) concluant sur c et telles que
Cc ∈ RC et Ci ∈ RI ;
– pour chaque rôle r de R il existe un couple de règles (Rr , RIr ) concluant sur r et telles que
Rr ∈ RI et RIr ∈ RRI ;
– pour chaque axiome a de A il existe une règle Aa ∈ RA concluant sur a.
2.3    Expression des règles

La prémisse d’une règle peut être représentée par un ensemble de patrons qui s’appliquent sur un
corpus. S’il a été préalablement analysé par certains modules d’annotation (étiquetage morpho-
syntaxique, reconnaissance d’entités nommées, étiquetage terminologique, par exemple) celui-
ci porte déjà des annotations linguistiques. Un patron est une expression qui s’appuie sur ces
différents niveaux d’annotations. L’application d’un patron sur un corpus est une opération qui
retourne un ensemble de segments du corpus à annoter selon la conclusion de la règle.
A titre d’illustration, voici trois exemples distincts de patrons, écrits dans un pseudo langage
pour en faciliter la compréhension, pour repérer dans le texte des occurrences du concept infor-
matique Système d’exploitation :
1. [texte="système d’exploitation"]
2. [lemme="système"][lemme="de"][lemme="exploitation"]

Y. MA, L. Audibert
3. [terme="système d’exploitation"]
texte correspond à la forme brute du texte, lemme à la forme lemmatisée des mots et terme
aux annotations de l’extracteur de termes. Ces trois patrons montrent l’intérêt de l’utilisation des
différents types d’annotations de la plate-forme. En effet, le premier patron n’est pas générique
et ne peut pas reconnaître de simples variations comme Système d’exploitation ou systèmes
d’exploitation. Le second est plus générique car insensible à la casse et au nombre. Le dernier
est encore plus générique car, selon l’extracteur de termes utilisé, il peut reconnaître des chaînes
comme OS pour lesquelles l’extracteur proposera la forme canonique système d’exploitation.
3    Contradictions dans les annotations sémantiques
L’objectif d’une extension d’ontologie par des règles d’annotation, comme nous venons de le
décrire, est de réaliser de l’annotation sémantique automatique dans le cadre d’une plate-forme
d’annotation. Cette annotation permet de proposer une interprétation ontologique du texte an-
noté. Inévitablement, certaines annotations seront sources de contradictions au regard de l’on-
tologie. La cause réelle d’une contradiction peut venir du texte lui-même, de l’ontologie ou
encore des règles d’annotations.
Dans cet article, nous ne cherchons pas à remettre en question la cohérence du texte. D’autre
part, une ontologie est généralement constituée manuellement, ou partiellement manuellement
et fait l’objet d’une attention particulière (vérifications, évolutions. . .). Par contre, les règles
d’annotation ne feront probablement pas l’objet d’autant d’attention, entre autres parce qu’elles
sont bien plus nombreuses que les éléments constitutifs de l’ontologie, et que leur validation
doit se faire au regard de corpus de taille importante. Notre intention est d’ailleurs d’acquérir
ces règles par une procédure aussi automatisée que possible. Ainsi, lorsqu’une contradiction est
détectée, la cause la plus probable est bien entendu à chercher dans les règles.
Commençons par formaliser le résultat d’une annotation sémantique.

Definition 1 (Annotation) Une annotation est la donnée d’une paire dénotée Anno définie de
la manière suivante :
– si t est un segment de texte reconnu par une règle Cc ∈ RC concluant sur c ∈ C, alors
Anno = (t ≡ c, Cc ) ;
– si t est un segment de texte reconnu par une règle Ic ∈ RI concluant sur c ∈ C, alors
Anno = (c(t), Ic ) ;
– si t est un segment de texte reconnu par une règle Rr ∈ RR concluant sur r ∈ R, alors
Anno = (t ≡ r, Rr ) ;
– si t1 et t2 sont deux segments de texte reconnus par une règle RIr ∈ RRI concluant sur
r ∈ R, alors Anno = (r(t1 , t2 ), RIr ) ;
Le processus d’annotation sémantique produit donc un ensemble d’annotations ANNT défini
par :
ANNT = {Anno | Anno est une annotation}.

Soit Anno une annotation constituée de la paire (ax, ru), nous définissons deux fonctions de
projections p1 (.) et p2 (.) telles que p1 (Anno) = ax et p2 (Anno) = ru. Concrètement, p1 (.)
et p2 (.) permettent respectivement de faire référence au premier élément (i.e. l’annotation pro-
prement dite) ou au deuxième élément (i.e.. la règle d’annotation) d’une annotation. Par un

Détection des contradictions dans les annotations sémantiques
petit abus de notation, si ANNT désigne un ensemble d’annotations, nous définissons également
p1 (ANNT) = {p1 (Anno) | Anno ∈ ANNT} et p2 (ANNT) = {p2 (Anno) | Anno ∈ ANNT}.
Nous pouvons maintenant donner une définition formelle à la notion d’annotations sémantiques
contradictoires en nous appuyant sur la notion de cohérence des logiques de description. Nous
rappelons qu’une ontologie O est incohérente (ou insatisfiable) si elle ne possède pas de modèle
(Baader et al., 2003). Un exemple très simple d’ontologie incohérente est O = {A(a), ¬A(a)}
qui représente deux faits inconciliables qui sont qu’une instance a d’un concept A est également
une instance de sa négation.

Definition 2 (Annotations et Règles incohérentes) Soit ANNT un ensemble d’annotations sé-
mantiques d’un corpus T selon une ontologie étendue O. Si p1 (ANNT) ∪ O est incohérent,
nous dirons que l’ensemble d’annotations ANNT est incohérent et que l’ensemble de règles
p2 (ANNT) est incohérent au regard du corpus T . Dans le cas contraire, ANNT est dit cohérent
et p2 (ANNT) est dit cohérent au regard du corpus T .
IJU P E = ([texte="une jupe"], JU P E),
CHOM M E = ([terme="homme"], HOM M E),
IHOM M E = ({X | X est un homme}, HOM M E),
CF EM M E = ([terme="femme"], F EM M E),
IF EM M E = ({X | X . . . porte une jupe}, F EM M E)

Remarque : Ces règles sont écrites dans un pseudo langage
très simplifié et informel pour en faciliter la compréhension.
F IG . 2 – Exemple d’annotations sémantiques incohérentes.
La figure 2 illustre un exemple d’annotations sémantiques incohérentes. Dans cet exemple,
ANNT = { (JU P E(t1 ), IJU P E ), (HOM M E(t2 ), IHOM M E ), (F EM M E(t2 ), IF EM M E ), (t3 ≡
HOM M E, CHOM M E )} où t1 , t2 , t3 correspondent respectivement aux segments de texte "une
jupe", "Bob" et "homme". L’assertion que HOM M E et F EM M E sont des concepts dis-
joints dans l’ontologie O rend p1 (ANNT) ∪ O incohérent. Donc l’ensemble d’annotations ANNT
est incohérent et l’ensemble de règles p2 (ANNT)={IJU P E , IHOM M E , IF EM M E , CHOM M E } est
incohérent au regard de la phrase.
4       Détection des contradictions et des règles fautives
Par définition, un ensemble d’annotations ANNT est incohérent si et seulement si p1 (ANNT)∪O
est incohérent. Vérifier la cohérence de p1 (ANNT) ∪ O est une tâche classique à la portée des
raisonneurs proposés par la plupart des outils de manipulation d’ontologies1 .
Nous introduisons la mesure draconienne de cohérence d’un ensemble de règles p2 (ANNT) au
regard d’un corpus annoté sémantiquement (ANNT) de la manière suivante :
0 si p2 (ANNT) est cohérent au regard du corpus annoté sémantiquement,
I(p2 (ANNT)) =
1 si p2 (ANNT) est incohérent au regard du corpus annoté sémantiquement.
1
http://www.cs.man.ac.uk/~sattler/reasoners.html

Y. MA, L. Audibert
Nous aimerions maintenant identifier quelles sont les règles responsables de l’incohérence avec,
si possible, une mesure de la responsabilité de chacune. Pour cela, nous allons utiliser la mesure
de Shapley (Shapley, 1953; Hunter & Konieczny, 2006) empruntée à la théorie des jeux.

Definition 3 (Mesure de cohérence de Shapley d’une règle) Pour chaque règle r d’un ensemble
de règles p2 (ANNT) issu d’un ensemble d’annotations ANNT, nous définissons la mesure de co-
hérence de Shapley, notée SI (r), comme suit :
(card(R) − 1)!(card(p2 (ANNT)) − card(R))!
SI (r) =                                                            (I(R) − I(R \ r)). (1)
card(p2 (ANNT))!
R⊆p2 (ANNT)
Ainsi, au plus la mesure de cohérence de Shapley d’une règle est élevée, au plus l’implication
de la règle dans les incohérences générées est importante. Il faut donc revoir en premier lieu les
règles qui possèdent une mesure de cohérence de Shapley élevée.
En appliquant cette mesure sur l’exemple de la figure 2, nous obtenons :
1                      1
SI (IJU P E ) = 0,    SI (CHOM M E ) = 0,SI (IHOM M E ) = , SI (IF EM M E ) = .
2                      2
Nous pouvons en conclure que les règles IJU P E et CHOM M E ne posent pas de problème tan-
dis que les règles IHOM M E et IF EM M E sont toutes deux impliquées de manière égale dans le
problème d’incohérence. C’est donc du côté de ces deux règles qu’il faut chercher la source du
problème.
5      Conclusion et perspectives
Dans un précédent article, nous avons proposé d’étendre les ontologies par des règles d’anno-
tation sémantique. Dans cet article, en nous appuyant sur la théorie de la valeur de Shapley,
nous proposons un moyen de quantifier l’implication respectives des règles dans la produc-
tion d’annotations sémantiques incohérentes. L’avantage de cette mesure est d’être entièrement
automatisée et de permettre d’assister le processus de révision des règles, dans la perspective
d’améliorer la cohérence des annotations, en mettant en avant les règles à modifier en priorité.
Cette approche doit maintenant être testée et évaluée. Nous projetons de le faire dans la plate-
forme d’annotation sémantique qui doit être développée dans le cadre du programme Quaero.
Références
F. BAADER , D. C ALVANESE , D. L. M C G UINNESS , D. NARDI & P. F. PATEL -S CHNEIDER,
Eds. (2003). The Description Logic Handbook : Theory, Implementation, and Applications.
Cambridge University Press.
H UNTER A. & KONIECZNY S. (2006). Shapley inconsistency values. In P. D OHERTY, J.
M YLOPOULOS & C. A. W ELTY, Eds., KR, p. 249–259 : AAAI Press.
M A Y., AUDIBERT L. & NAZARENKO A. (2009). Ontologies étendues pour l’annotation
sémantique. 20èmes Journées Francophones d’Ingénierie des Connaissances.
S HAPLEY L. S. (1953). A value for n-person games. In H. K UHN & A. T UCKER, Eds.,
In Contributions to the Theory of Games (Annals of Mathematical Studies 28), p. 307–317 :
Princeton University Press.

Vers une méthodologie d’annotation des entités nommées en
corpus ?

Karën Fort1, 3 Maud Ehrmann2 Adeline Nazarenko3
(1) INIST, 2 allée du Parc de Brabois, 54500 Vandoeuvre-lès-Nancy
karen.fort@inist.fr
(2) XRCE, 6 Chemin de Maupertuis, 38240 Meylan
maud.ehrmann@xrce.xerox.com
(3) LIPN, Université Paris 13 & CNRS, 99 av. J.B. Clément, 93430
Villetaneuse
adeline.nazarenko@lipn.univ-paris13.fr

Résumé.           La tâche, aujourd’hui considérée comme fondamentale, de reconnaissance d’en-
tités nommées, présente des difficultés spécifiques en matière d’annotation. Nous les précisons
ici, en les illustrant par des expériences d’annotation manuelle dans le domaine de la microbi-
ologie. Ces problèmes nous amènent à reposer la question fondamentale de ce que les anno-
tateurs doivent annoter et surtout, pour quoi faire. Nous identifions pour cela les applications
nécessitant l’extraction d’entités nommées et, en fonction des besoins de ces applications, nous
proposons de définir sémantiquement les éléments à annoter. Nous présentons ensuite un cer-
tain nombre de recommandations méthodologiques permettant d’assurer un cadre d’annotation
cohérent et évaluable.
Abstract.         Today, the named entity recognition task is considered as fundamental, but it
involves some specific difficulties in terms of annotation. We list them here, with illustrations
taken from manual annotation experiments in microbiology. Those issues lead us to ask the fun-
damental question of what the annotators should annotate and, even more important, for which
purpose. We thus identify the applications using named entity recognition and, according to the
real needs of those applications, we propose to semantically define the elements to annotate.
Finally, we put forward a number of methodological recommendations to ensure a coherent and
reliable annotation scheme.
Mots-clés :             annotation, reconnaissance d’entités nommées.

Keywords :               annotation, named entities extraction.
1        Introduction
Si l’extraction d’entités nommées (EN), apparue au milieu des années 1990 à la faveur des
dernières conférences MUC1 , fait aujourd’hui figure d’incontournable en Traitement Automa-
tique des Langues (TAL), l’annotation de corpus qui la sous-tend est encore peu étudiée en tant
1
Message Understanding Conferences, (MUC, 1995), (MUC, 1998)

Karën Fort, Maud Ehrmann, Adeline Nazarenko
que telle. Les enjeux de l’annotation manuelle sont pourtant importants. Qu’il s’agisse de la
performance des systèmes mis au point (à partir d’un travail de modélisation ou d’apprentis-
sage automatique sur corpus), de l’évaluation de ces derniers, ou encore de la bonne réponse
apportée à des besoins applicatifs, l’annotation de corpus est une composante fondamentale.
Au cœur des divers processus constituant la reconnaissance d’entités nommées (REN), nous
souhaitons examiner de plus près la problématique de l’annotation manuelle, laquelle conduit à
s’interroger sur ce que sont les entités nommées et ce à quoi elles servent.
Nous présentons cette pratique établie qu’est aujourd’hui l’annotation d’EN (2) puis nous en
détaillons les principales difficultés (3), aussi bien sur des textes de langue générale que de
spécialité. Nous examinons ensuite les applications dans lesquelles les EN sont utilisées et en
déduisons les différents types d’annotation (4). Enfin, nous proposons des recommandations
méthodologiques permettant d’assurer un cadre d’annotation cohérent et évaluable (5).
2        Annotation d’entités nommées, une pratique établie
La reconnaissance d’entités nommées est une tâche bien connue : initiée il y a une ving-
taine d’années à l’occasion des conférences américaines MUC, elle fut rapidement reprise lors
d’autres campagnes d’évaluation, suscitant des travaux toujours plus nombreux. Sans revenir
ici sur le “ succès” de cette tâche (Nadeau & Sekine, 2007), on peut retracer son évolution selon
trois directions principales. La première correspond à des travaux dans le domaine “ général ”,
avec la poursuite de la tâche définie par MUC pour d’autres langues que l’anglais, selon un
jeu de catégories plus ou moins revisité et pour annoter des entités dans des corpus de nature
journalistique essentiellement2 . La seconde direction concerne des travaux dans des domaines
dits “ de spécialité ”, avec la reconnaissance d’entités dans les domaines de la médecine, de la
chimie ou de la microbiologie. Il fut ainsi proposé de reconnaître des noms de gènes, de pro-
téines, etc. dans de la littérature spécialisée, lors des campagnes JNLPBA (Kim et al., 2004) et
BioCreAtIvE (Hirschman et al., 2005). La dernière direction, transversale aux domaines général
et de spécialité, correspond à des travaux sur la désambiguïsation : résolution de métonymie des
EN dans SemEval2007 (Markert & Nissim, 2007) et désambiguïsation de noms de personnes
(Artiles et al., 2007)3 .
A l’occasion de chacune de ces campagnes, des corpus furent constitués et annotés manuelle-
ment. De manière générale, ces corpus annotés servent à la mise au point d’outils d’annotation
automatique. "Mise au point" est à entendre ici au sens large : il s’agit de décrire le plus précisé-
ment possible ce que les systèmes doivent faire pour guider le travail d’écriture des règles sur
lesquelles ils reposent, pour apprendre automatiquement ces règles de fonctionnement ou des
critères de décision et, enfin, pour évaluer les résultats obtenus en les confrontant à une analyse
de référence. Le processus d’annotation met en jeu deux acteurs, un annotateur et un texte, et
aboutit à une annotation du texte dont la qualité doit répondre à une méthodologie et supporter
un protocole d’évaluation, et le contenu suivre un guide d’annotation.
Pour le domaine général, les campagnes MUC, CoNLL et ACE ont travaillé avec des cor-
pus issus de la presse. Ces campagnes semblent avoir porté attention au processus d’annota-
tion manuelle des EN, avec la rédaction de guides d’annotation et des calculs d’accord inter-
annotateurs (mais non intra-annotateur), procédant par allers-retours entre le corpus à annoter et
2
Voir les campagnes d’évaluation MET, IREX, CoNNL, ACE, ESTER et HAREM (Ehrmann, 2008, pp. 19-21).
3
La campagne Web People Search fut par la suite rééditée, voir http://nlp.uned.es/weps/.

Vers une méthodologie d’annotation des entités nommées en corpus ?
le guide d’annotation à réviser, mais des points d’hésitations ont perduré quant à l’annotation,
dus principalement à “ different interpretations of vague portions of the guidelines ” (Sundheim,
1995) où à des phénomènes de superposition de sens (Doddington et al., 2004). Dans les do-
maines de la biologie et de la biomedecine, des textes des bases de données de publications
scientifiques (PubMed et MedLine4 ) ont été annotés : on observe que les guides d’annotation –
quand ils existent5 – laissent des zones d’ombre quant à la manière d’annoter les entités et que
peu d’études ont porté sur la qualité de l’annotation. Que ce soit pour les corpus GENIA (Kim
et al., 2003), PennBioIE (Kulick et al., 2004) ou GENETAG (Tanabe et al., 2005), aucun accord
inter ou intra-annotateur n’est rapporté. A la fin de leur expérience d’annotation, les auteurs de
(Tanabe et al., 2005) constatent que “ a more detailed definition of a gene/protein name, as well
as additional annotation rules, could improve interannotator agreement and help solve some of
the tagging inconsistencies ”.
On observe par ailleurs des pratiques d’annotation manuelle et une réflexion méthodologique
intéressantes dans d’autres domaine du TAL, notamment dans les travaux issus de la commu-
nauté parole (Gut & Bayerl, 2004). La campagne EVALDA MEDIA (Bonneau-Maynard et al.,
2005), par exemple, a mis en oeuvre une annotation manuelle de corpus en deux passes, la pre-
mière sur un échantillon pour faire le point sur les éventuels désaccords entre annotateurs, et
une seconde grandeur nature, après réajustement des guides d’annotation.
3        Difficultés rencontrées dans l’annotation
Pour les corpus de langue générale, un certain nombre de difficultés sont identifiées (Ehrmann,
2008). La première concerne le choix des catégories et la détermination de ce qu’elles recou-
vrent. En effet, au-delà de la triade “universelle” définie par MUC (PERSONNE, LIEU et ORGAN -
ISATION ), l’inventaire des catégories à annoter est difficile à stabiliser et à définir. Prenons l’ex-
emple de la catégorie PERSONNE : s’il est évident qu’un nom d’individu tel que Lionel Jospin
est à annoter à l’aide de cette catégorie, que faut-il faire des Kennedys, de Zorro, des Démocrates
ou de St. Nicolas ? Pour les autres catégories, il est également difficile de choisir la granular-
ité des catégories et de déterminer ce qu’elles recouvrent vraiment. Un autre type de difficuté
concerne la sélection des mentions à annoter ainsi que la délimitation des frontières des EN.
A titre d’exemple, considérons l’EN “Barack Obama” et les diverses unités lexicales suivantes
permettant d’y faire référence : Barack Obama, Monsieur Obama, le Président des Etats-Unis,
le nouveau président, il. Faut-il annoter les noms propres uniquement, ou peut-on également
considérer les descriptions définies permettant d’identifier cette personne, voire les pronoms
qui, contextuellement, y renvoient ? Et que fait-on des différents attributs accompagnant l’EN
(“monsieur” et “président”) ? De nombreux autres cas, plus ou moins compliqués (la Maire du
7e arrondissement de Paris et Garde des Sceaux Rachida Dati) se rencontrent en corpus. Dans
le même ordre d’idée, des phénomènes de coordination et d’imbrication peuvent poser prob-
lème aux annotateurs (une ou plusieurs entités pour Bill et Hillary Clinton et l’Université de
Corte ?). Enfin, une dernière difficulté résulte de phénomènes de pluralité référentielle, avec des
EN homonymes (Orange ville et Orange compagnie) et des glissements métonymiques, parfois
difficiles à distinguer (France en tant que lieu géographique, gouvernement ou équipe sportive).
Même si elles sont surmontables grâce aux guides d’annotation, ces difficultés entraînent un
coût supplémentaire et une baisse de qualité de l’annotation.
4
Respectivement : http://www.ncbi.nlm.nih.gov/pubmed/ et http://medline.cos.com/
5
Certaines campagnes comme JNLPBA et BioCreAtIvE I n’en ont pas fourni.

Karën Fort, Maud Ehrmann, Adeline Nazarenko
Notre expérience en microbiologie montre que ces difficultés sont plus aiguës encore dans les
langues de spécialité. Une expérience d’annotation a été réalisée à l’INIST pour le programme
Quaero, en collaboration avec l’équipe MIG. Un corpus anglais de 499 notices PubMed (titres
et résumés, soit environ 110 000 “mots”), pré-annotées par application d’un dictionnaire et d’un
anti-dictionnaire, a été fourni à deux experts de l’INIST, dont le travail a consisté en une révision
de ces pré-annotations. La principale difficulté rencontrée a concerné la distinction qui était de-
mandée entre noms propres et noms communs, la limite morphologique entre les deux étant peu
marquée dans ces domaines où les noms communs sont souvent reclassés comme "noms pro-
pres", comme en atteste la présence de ces noms dans des nomenclatures (“small, acid-soluble
spore protein A” est ici un cas extrême) ou les phénomènes d’acronymisation (on trouve par
exemple "across the outer membrane (OM)"). Dans ces cas, la consigne donnée aux annota-
teurs était de se référer à des listes d’autorité, telle que Swiss-Prot6 , ce qui entraîne une perte de
temps conséquente. La délimitation des frontières des éléments à annoter a elle-aussi soulevé de
nombreux questionnements, les annotateurs se demandant ce qu’il fallait inclure ou non dans le
segment annoté. On peut ainsi choisir d’annoter “nifh messenger RNA" si on considère que la
mention de l’état "messenger RNA" entre dans la détermination de la référence, ou seulement
“nifh”, si on considère que le nom propre suffit à construire la référence. Le typage sémantique
choisi a aussi posé problème aux annotateurs, notamment pour les éléments génétiques mobiles,
comme les plasmides ou les transposons. En effet, ceux-ci devaient être annotés dans les taxons
et non dans les genes alors que ce sont des fragments d’ADN, donc des parties de génome. Une
directive particulièrement perturbante pour les annotateurs a été d’annoter l’acronyme “KGFR”
comme nom propre et sa forme développée “keratinocyte growth factor receptor” comme nom
commun. Ce type de consigne, préconisée au départ pour entraîner plus efficacement les outils
de REN (Nédellec et al., 2006), est difficile à appréhender et aurait dû être mieux documentée.
Ces problèmes se traduisent par un coût d’annotation élevé, des guides d’annotation de taille
trop grande par rapport au corpus et trop d’hésitations de la part des annotateurs, ce qui induit
des incohérences et une qualité moindre de l’annotation. Cette expérience nous a ainsi permis
de reposer la question de ce que les annotateurs doivent annoter et surtout, pour quoi faire.
4        Annoter quoi ?

Pour mieux comprendre "quoi annoter dans quel texte", nous revenons sur les critères linguis-
tiques qui permettent de définir la notion d’EN : l’importance de ces critères varie d’une appli-
cation à l’autre et les annotations résultantes en dépendent.
4.1       Différents critères définitoires

(Ehrmann, 2008) propose une analyse linguistique de la notion d’EN qu’elle présente comme
une "création" du TAL. Dans ce qui suit, nous reprenons la distinction introduite dans (LDC,
2004) : les EN sont des "mentions" qui renvoient à des "entités" du domaine, ces mentions
pouvant relever de différentes catégories linguistiques : des noms propres ("Rabelais"), mais
aussi les pronoms ("il"), et plus largement des descriptions définies ("le père de Gargantua").
On peut identifier plusieurs critères définitoires des EN.
6
http://www.expasy.org/sprot/

Vers une méthodologie d’annotation des entités nommées en corpus ?
Unicité référentielle L’une des caractéristiques principales des noms propres est leur fonction-
nement référentiel : un nom propre renvoie à une entité référentielle unique, même si cette
unicité est contextelle. A la différence de (Poibeau, 2005), nous considérons que cette propriété
est essentielle dans l’utilisation que fait le TAL des EN.
Autonomie référentielle Les EN sont de surcroît autonomes du point de vue référentiel. C’est
évident dans le cas du nom propre qui permet à lui seul l’identification du référent, tout au moins
dans une situation de communication donnée (Eurotunnel). Le cas des descriptions définies
(l’opérateur du tunnel sous la Manche) est un peu différent : si elles suffisent à identifier le
référent, c’est par le truchement de connaissances extérieures.
Stabilité dénominative Les noms propres sont également des dénominations stables. Même s’il
y a des variations (Angela Merkel/Mme Merkel/A. Merkel), elles sont plus régulières et moins
nombreuses que pour les autres syntagmes nominaux7 .
Relativité référentielle L’interprétation se fait toujours relativement à un modèle du domaine,
qui peut être implicite dans les cas simples (on suppose une connaissance partagée de ce qu’est
une personne ou un pays) mais qui doit être explicité dès que la diversité des entités à prendre
en compte s’accroît (il faut au moins une typologie pour les catégoriser).
4.2    Différentes visées applicatives

La REN voit le jour dans un cadre d’extraction d’information où il s’agit d’identifier les actants
de certaines situations (des attentats aux interactions géniques), cette situation étant décrite par
un formulaire à instancier avec les informations extraites du texte. La sémantique sous-jacente
est clairement référentielle : on identifie dans le texte les "mentions" des "entités" qui jouent un
rôle dans les situations considérées (LDC, 2004). Dans les systèmes de questions/réponses, les
EN jouent le même rôle mais on a souvent recours à des typologies plus fines8 .
Les EN sont également utilisées comme "descripteurs" dans de nombreuses applications d’in-
dexation. On a des index de noms propres dans certains ouvrages et moteurs de recherche et on
sait qu’une importante proportion des requêtes adressées aux moteurs de recherche sont des EN.
On surligne les EN pour aider la lecture ou la navigation dans de gros volumes documentaires.
On utilise enfin la REN pour construire et mettre à jour des nomenclatures (Tran & Maurel,
2006) utilisées pour l’indexation, la traduction, etc. Dans ce deuxième type d’application aussi,
la sémantique sous-jacente est référentielle : si les EN sont de bons descripteurs, c’est qu’elles
fonctionnent comme des ancres référentielles qui permettent de situer ce à quoi le texte fait
référence9 . En revanche, on s’intéresse exclusivement aux mentions de type nom propre.
Troisième type d’applications, les EN sont utilisées pour l’intégration de données. Cela con-
cerne à la fois l’analyse des bases documentaires (suivi de thèmes, découverte de communautés
de pratiques (Li & Liu, 2005) et l’articulation des documents avec d’autres sources de connais-
sances (bases de données, bases d’images, etc.) pour interroger les unes et les autres de manière
homogène et naviguer facilement de l’une à l’autre (Dragos & Nazarenko, 2009). On s’appuie
alors sur les EN référencées dans les bases de données pour établir des liens entre les différentes
sources. Pour ces tâches d’intégration, on se contente des EN qui figurent dans les nomencla-
7
Ceci explique a contrario l’importance du repérage de la synonymie dans les domaines où les dénominations
sont peu stables (la génomique, par exemple).
8
Ce qui pose des problèmes de désambiguïsation et de résolution des métonymies.
9
Deux noms propres (AZF et septembre 2001) suffisent souvent à faire comprendre de quoi parle une dépêche !

Karën Fort, Maud Ehrmann, Adeline Nazarenko
tures, l’objectif étant d’articuler les sources entre elles et pas d’en décrire le contenu : il suffit
de savoir qu’une dépêche parle de Nelson Mandela, inutile de trouver toutes les mentions qui
en sont faites dans le texte.
L’anonymisation des documents est un quatrième champ d’application (Plamondon et al., 2004).
On veut éviter qu’on puisse identifier des entités (des personnes, notamment) à partir des men-
tions qui en sont faites dans le texte. Il faut repérer toutes les formes de mentions (la ville qui a
reçu la première bombe atomique) et pas seulement les noms propres.
Dernière famille d’applications, la REN est utilisée pour "peupler" des ontologies. Il s’agit alors
de modéliser un domaine, et le modèle formel des ontologies impose de distinguer les entités
du domaine, qui sont représentées comme des instances, des concepts ou classes auxquelles
ces instances se rattachent. La REN est alors utilisée pour enrichir la structure conceptuelle
avec des instances de concepts ou de rôles (relations entre instances) (Amardeilh et al., 2005).
Selon les cas, on privilégie les entités nommées ou on s’intéresse à toutes les mentions d’entités.
Dans le premier cas, l’ontologie résultante peut être vue comme une nomenclature hiérarchisée
ou comme un thesaurus formalisé. Dans le second, on cherche à modéliser un domaine en
identifiant les entités qui le composent et les relations qu’elles entretiennent. Dans les deux
cas, le typage des EN est essentiel parce qu’on doit relier l’instance nommée à un concept de
l’ontologie.
4.3       Des perspectives d’annotation différentes

Les critères définitoires cités en 4.1 ne jouent pas tous le même rôle pour toutes les applica-
tions. Dans certains cas (indexation et intégration de connaissances), on s’intéresse à des entités
référentielles qui sont désignées par des descripteurs stables et non ambigus. Ce sont donc les
EN de type noms propres ou "catalogables" qui sont à retenir et il est important de les normaliser
pour s’affranchir des variations qui peuvent apparaître malgré leur stabilité référentielle. Pour
ce type d’application, l’essentiel n’est pas de repérer toutes les mentions de telle entité dans un
document mais de repérer que ce document mentionne telle entité. Il faut donc privilégier la
précision sur le rappel de l’annotation.
A l’autre extrême, on trouve les tâches d’extraction d’information et de modélisation du do-
maine où toutes les mentions sont importantes à repérer, y compris celles qui sont des descrip-
tions définies (il faut d’ailleurs repérer des relations de coréférence entre les mentions qui ne
sont pas suffisamment autonomes référentiellement). La figure 1 montre l’impact de ces deux
perspectives d’annotation sur les résultats de l’annotation d’un tout petit exemple10 .
Comme il est impossible de repérer les mentions de toutes les entités référentielles, le modèle
du domaine détermine quelles sont les entités "d’intérêt" et la limite entre ce qui doit ou non
être annoté. Illustrons ce point sur un exemple. Quand un directeur des ressources humaines
s’intéresse aux grilles salariales de son organisation, il raisonne sur des catégories de personnels
et pas sur les personnes physiques que sont les employés. Cela se reflète dans le modèle du
domaine : les différentes catégories de personnes (techniciens, ingénieurs, etc.) sont modélisées
comme des instances rattachées au concept CAT- DE - PERSONNEL et les personnes physiques ne
sont pas représentées. A l’inverse, quand il s’occupe des fiches de paye et de la progression
des employés, il s’intéresse aux individus. Dans ce cas, le modèle doit considérer les personnes
comme instances et les catégories de personnels comme des concepts. Le même problème se
10
http ://www.ncbi.nlm.nih.gov/pubmed/1331532

Vers une méthodologie d’annotation des entités nommées en corpus ?
A NNOTATION D ’ INDEXATION ; types gene et protein
We conclude that <gene>3CDproM</gene> can process both structural and nonstructural precursors of the <EukVirus
uncertainty-type="too-generic">poliovirus polyprotein</EukVirus> and that it is active against a synthetic peptide substrate.

A NNOTATION DE MODÉLISATION ; types taxon, gene et protein
We conclude that <EukVirus>3CDproM</EukVirus> can process both structural and nonstructural precursors of the <EukVirus
uncertainty-type="too-generic"><taxon>poliovirus</taxon> polyprotein</EukVirus> and that <EukVirus>it</EukVirus> is active
against a synthetic peptide substrate.
F IG . 1 – Exemple d’annotation en biologie. La première annotation est moins riche que la seconde qui
considère plus de types sémantiques (taxon) avec une granularité plus fine (EuKVirus is a subtype
of gene), ce qui introduit des enchâssements de balises. Par ailleurs, toutes les mentions sont annotées
dans la seconde annotation alors que seuls les assimilés noms propres le sont dans la première.
rencontre en biologie où la mention du gène G peut renvoyer au gène G de l’espèce E, au gène
G d’un individu de l’espèce E ou à un gène particulier parmi les gènes G de cet individu.
Modéliser suppose de faire des choix explicites là où les textes peuvent être flous et mêler les
points de vue. Il est donc impossible d’annoter les EN d’un texte indépendamment d’un modèle
de référence. Dans le cas de l’expérience décrite plus haut, le modèle était, comme souvent,
simplement décrit par une liste de concepts : il fallait nommer les gènes et protéines, mais aussi
leurs familles, compositions, et composants. Par ailleurs, comme la REN doit servir à modéliser
le réseau d’interactions entre gènes, il était essentiel de repérer toutes les mentions des entités
considérées, chacune pouvant contribuer à enrichir sa modélisation.
5        Recommandations méthodologiques

Annoter est difficile. Il faut donc guider le travail des annotateurs, ce qui passe par des guides
et des outils d’annotation, mais aussi par l’évaluation de la qualité des annotations.
Guides d’annotations Comme le type d’annotation à produire dépend de ce qu’on cherche à
annoter et de ce à quoi cette annotation doit servir, il est essentiel de fournir aux annotateurs des
guides (ou conventions) d’annotation qui indiquent ce qu’il faut annoter plutôt que comment
annoter. Trop souvent en effet, les contraintes de faisabilité prennent le pas sur les critères
sémantiques11 ce qui brouille l’objectif pour les annotateurs. Par ailleurs, il est important de
prendre la mesure de la tâche dans toute sa complexité sans exclure a priori ce qui serait douteux
ou trop difficile à reproduire automatiquement. C’est même l’intérêt de l’annotation manuelle
que de donner une idée de l’ampleur de la tâche d’annotation.
Il faut donner aux annotateurs une vision claire de l’application visée. Cette vision doit s’ap-
puyer sur un modèle de référence explicite, du type de celui donné dans la campagne GENIA,
avec des définitions précises et des explications sur les choix méthodologiques réalisés (caté-
gories, typage sémantique, etc.). Des exemples peuvent être ajoutés à titre d’illustration mais ils
ne doivent pas se substituer à la définition des objectifs. Ces informations permettont de limiter
les incompréhensions, mais aussi de responsabiliser et de motiver les annotateurs en leur don-
nant accès à la logique sous-jacente. On entre ainsi dans une démarche didactique, permettant
de passer d’un “rapport de père à un rapport de pair" (Akrich & Boullier, 1991), ce qui est
d’autant plus nécessaire que l’annotation porte sur des corpus spécialisés, les annotateurs étant
11
"In [src homology 2 and 3], it seems excessive to require an NER program to recognize the entire fragment,
however, 3 alone is not a valid gene name." (Tanabe et al., 2005).

Karën Fort, Maud Ehrmann, Adeline Nazarenko
des experts à qui on a intérêt à donner la plus grande autonomie possible. Dans certains cas, la
tâche d’annotation étant trop complexe, on doit se restreindre à une annotation exploitable pour
l’apprentissage automatique – en se limitant par exemple aux noms propres – mais cela doit être
fait de manière explicite pour que les annotateurs aient une vue claire des choix à faire.
Outils d’annotation Le travail d’annotation doit être outillé. On utilise des dispositifs tech-
niques pour annoter les textes mais aussi pour préparer le travail d’annotation par une pré-
annotation (projection d’un dictionnaire, par exemple). Ces outils sont cependant à manier avec
précaution du fait des biais qu’ils introduisent. Dans l’expérience que nous avons menée, le
corpus d’annotation avait été pré-annoté par projection d’un dictionnaire de noms de gènes et
de protéines, pour alléger le travail des annotateurs, mais cette pré-annotation a influencé l’an-
notation. Elle a introduit un biais en faveur de la correction des pré-annotations, au détriment de
la recherche de nouvelles EN. Une solution consiste à procéder en deux temps, en étiquetant à
la main un échantillon du corpus, puis en le pré-annotant pour comparer les résultats et évaluer
le biais introduit. On peut ensuite faire une annotation à plus grande échelle, précédée par une
pré-annotation dont on connaîtra cette fois les biais.
En ce qui concerne le processus d’annotation lui-même, s’il existe aujourd’hui de nombreux
outils d’aide à l’annotation manuelle, peu sont effectivement disponibles, c’est-à-dire gratuits,
téléchargeables et utilisables . On peut citer, entre autres, Callisto, MMAX2, Knowtator ou en-
core Cadixe12 qui a été utilisé dans notre expérience. Les fonctionnalités et l’expressivité du
langage d’annotation doivent être adaptées à la tâche d’annotation visée : selon les cas, il faut
typer des segments de textes ou les mettre en relation, on a des annotations concurrentes, dis-
jointes ou superposables, etc. Dans notre expérience en microbiologie, par exemple, le fait de
ne pas pouvoir annoter les coordinations d’EN a posé problème. Les critères ergonomiques sont
également importants : on sait que les fonctionnalités difficiles d’accès ne sont pas utilisées, ce
qui biaise là aussi les résultats. Dans notre cas, les annotateurs ont souvent négligé de mention-
ner leur incertitude parce qu’ajouter l’attribut correspondant à leurs annotations était malaisé.
Évaluation de l’annotation Il est important de mesurer la qualité de l’annotation. (Gut & Bay-
erl, 2004) distingue l’accord inter-annotateur, qui permet de mesurer la stabilité de l’annotation,
et l’accord intra-annotateur, qui donne une indication de la reproductibilité de l’annotation. S’il
est important de calculer l’accord inter-annotateur, il n’est pas nécessaire de le réaliser sur tout
le corpus, ne serait-ce que pour des raisons de coût. Il est en revanche conseillé de le calculer
tôt, afin d’identifier les problèmes rapidement et de modifier l’annotation en conséquence. Il en
va de même en ce qui concerne l’accord intra-annoteur.
Un autre moyen d’évaluation consiste à faire ajouter à l’annotateur, si nécessaire, une note d’in-
certitude, de préférence typée, sur ses annotations. En l’absence de calcul de l’accord inter ou
intra-annotateur pour cette annotation nous avons ainsi fait annoter les incertitudes des anno-
tateurs a posteriori sur un sous-ensemble du corpus. Pour s’assurer de la représentativité de
l’échantillon, les annotateurs ont identifié une typologie des fichiers du corpus (six “types”).
Nous avons ensuite extrait 25 fichiers de types variés parmi les 499 notices du corpus (soit 5%).
L’objectif de cette seconde validation étant d’évaluer la confiance des annotateurs en leur val-
idation, nous avons choisi de profiter du tag uncertainty proposé dans Cadixe et de l’utiliser
de manière systématique en cas de doute. Nous avons donc redéfini les types d’uncertainty en
fonction de l’expérience des annotateurs. Cette évaluation n’a nécessité que quelques heures de
travail et nous a permis de mieux qualifier et quantifier leurs doutes. Au final, sur 555 tags, les
12
respectivement : http://callisto.mitre.org/, http://mmax2.sourceforge.net/, http:
//knowtator.sourceforge.net/, http://caderige.imag.fr/Cadixe/

Vers une méthodologie d’annotation des entités nommées en corpus ?
annotateurs ont déclaré 113 uncertainty, soit environ 20% des tags. On observe que plus de 75%
des incertitudes concernent les noms communs de type bacteria, et que ces incertitudes sont très
largement (77%) liées à une difficulté à distinguer les noms communs des noms propres.
Plus généralement, pour s’assurer à la fois de la qualité du guide d’annotation et des possibilités
d’évaluation, une bonne méthode consiste, en tout début de projet, à faire travailler les annota-
teurs chacun de leur côté sur un échantillon du corpus. Cela permet d’identifier rapidement les
désaccords, puis de les trancher, soit en faisant intervenir un autre expert, soit par concertation
des annotateurs. Ces décisions sont ensuite reportées dans le guide d’annotation.
6         Conclusion et perspectives
Au final, une démarche rigoureuse et efficace d’annotation d’EN en corpus doit prêter attention
principalement à deux points. Au regard du contenu tout d’abord, il importe de se focaliser non
pas tant sur comment annoter, mais sur quoi annoter, en fonction de l’application visée. Nous
avons vu que chaque famille d’application spécifie un certain nombre de critères linguistiques,
et que les mentions à annoter diffèrent relativement à ces derniers. Une fois spécifié ce qu’il faut
annoter, il faut être prudent en termes de méthodologie et envisager dès le départ l’évaluation de
l’annotation. Il est avantageux, du point de vue de la qualité de l’annotation, de réaliser un ou
plusieurs galops d’essai, afin d’ajuster certains éléments du guide d’annotation et d’évaluer les
biais éventuels introduits par l’outil d’annotation et/ou une mauvaise compréhension de la tâche
par les annotateurs. Relativement aux enjeux et aux difficultés de l’annotation des EN en corpus,
l’attention portée à la spécification du contenu et au respect d’une méthodologie d’annotation
permet d’assurer un cadre de travail stable et cohérent à chacune des étapes de la tâche de REN.
Nous comptons appliquer cette méthodologie dans le cadre des campagnes d’annotation de
Quaero, en pharmacologie et en économie. Couvrant la terminologie et l’extraction de relations
sémantiques, ces campagnes dépassent largement le cadre des EN, nous allons donc élargir
notre méthode à ces applications.
Remerciements
Ce travail a été réalisé en partie dans le cadre du programme Quaero 13 , financé par OSEO,
agence nationale de valorisation de la recherche. Nous en remercions les participants, en par-
ticulier l’équipe MIG de l’INRA. Nous remercions également F. Tisserand et B. Taliercio, les
annotateurs experts de l’INIST.
Références
(1995). Proc. of the 6th Message Understanding Conference. Morgan Kaufmann Publishers.
(1998). Proc. of the 7th Message Understanding Conference. Morgan Kaufmann Publishers.
A KRICH M. & B OULLIER D. (1991). Savoir faire et pouvoir transmettre, chapter Le mode
d’emploi, genèse, forme et usage, p. 113–131. éd. de la MSH (coll. Ethnologie de la France).
13
http://www.quaero.org

Karën Fort, Maud Ehrmann, Adeline Nazarenko
A MARDEILH F., L AUBLET P. & M INEL J.-L. (2005). Document annotation and ontology
population from linguistic extractions. In Proc. of K-CAP’05, p. 161–168, New York : ACM.
A RTILES J., G ONZALO J. & S EKINE S. (2007). The semeval-2007 WePS evaluation : estab-
lishing a benchmark for the web people search task. In Proc. of SemEval, ACL, Prague.
B ONNEAU -M AYNARD H., ROSSET S., AYACHE C., K UHN A. & M OSTEFA D. (2005). Se-
mantic annotation of the french media dialog corpus. In InterSpeech, Lisbonne, Portugal.
D ODDINGTON G., M ITCHELL A., P RZYBOCKI M., R AMSHAW L., S TRASSEL S. &
W EISCHEDEL R. (2004). The ACE program tasks, data, and evaluation. In Proc. of LREC’04,
Lisbonne, Portugal.
D RAGOS V. & NAZARENKO A. (2009). Towards a semantic model to enhance knowledge
sharing and discovery in organic chemistry. In Proc. of the IADIS IS’09, Barcelone, Espagne.
E HRMANN M. (2008). Les entités nommées, de la linguistique au TAL : statut théorique et
méthodes de désambiguïsation. PhD thesis, Université Paris 7.
G UT U. & BAYERL P. S. (2004). Measuring the reliability of manual annotations of speech
corpora. In Proc. of Speech Prosody, p. 565–568, Nara, Japon.
H IRSCHMAN L., Y EH A., B LASCHKE C. & VALENCIA A. (2005). Overview of biocreative :
critical assessment of information extraction for biology. BMC Bioinformatics, 6(1).
K IM J.-D., O HTA T., TATEISI Y. & T SUJII J. (2003). Genia corpus–a semantically annotated
corpus for bio-textmining. Bioinformatics, 19, 180–182.
K IM J.-D., O HTA T., T SURUOKA Y., TATEISI Y. & C OLLIER N. (2004). Introduction to the
bio-entity recognition task at JNLPBA. In Proc. of JNLPBA COLING’04 Workshop, p. 70–75.
K ULICK S., B IES A., L IBERMAN M., M ANDEL M., M C D ONALD R., PALMER M., S CHEIN
A. & U NGAR L. (2004). Integrated annotation for biomedical information extraction. In HLT-
NAACL 2004 Workshop : Biolink : ACL.
LDC (2004). ACE (Automatic Content Extraction) English Annotation Guidelines for Entities.
Livrable version 5.6.1 2005.05.23, Linguistic Data Consortium.
L I X. & L IU B. (2005). Mining community structure of named entities from free text. In
Proc. of CIKM’05, p. 275–276, New York, NY, USA : ACM Press.
M ARKERT K. & N ISSIM M. (2007). Semeval-2007 task 08 : Metonymy resolution at semeval-
2007. In Proc. of SemEval, ACL, Prague.
NADEAU D. & S EKINE S. (2007). A survey of named entity recognition and classification.
Linguisticae Investigaciones, 30(1), 3–26.
N ÉDELLEC C., B ESSIÈRES P., B OSSY R., KOTOUJANSKY A. & M ANINE A.-P. (2006).
Annotation guidelines for machine learning-based named entity recognition in microbiology.
In Proc. of the Data and text mining in integrative biology workshop, p. 40–54, Berlin.
P LAMONDON L., L APALME G. & P ELLETIER F. (2004). Anonymisation de décisions de
justice. In Proc. of TALN’04, p. 367–376, Fès, Maroc.
P OIBEAU T. (2005). Sur le statut référentiel des entités nommées. In Proc. of TALN’05, p.
173–182, Dourdan, France.
S UNDHEIM B. (1995). Overview of results of the MUC-6 evaluation. In (MUC, 1995).
TANABE L., X IE N., T HOM L., M ATTEN W. & W ILBUR 1 J. (2005). Genetag : a tagged
corpus for gene/protein named entity recognition. Bioinformatics, 6.
T RAN M. & M AUREL D. (2006). Prolexbase : un dictionnaire relationnel multilingue de
noms propres. TAL, 47(3), 115–139.

Reconnaissance d’entités nommées : enrichissement d’un système à
base de connaissances à partir de techniques de fouille de textes

Damien Nouvel Arnaud Soulet Jean-Yves Antoine
Nathalie Friburger Denis Maurel
Université François Rabelais Tours, LI
Antenne Universitaire de Blois, 3 place Jean Jaurès, F-41000 Blois, France
{prénom.nom}@univ-tours.fr

Résumé.         Dans cet article, nous présentons et analysons les résultats du système de reconnaissance
d’entités nommées CasEN lors de sa participation à la campagne d’évaluation Ester2. Nous identifions
quelles ont été les difficultés pour notre système, essentiellement : les mots hors-vocabulaire, la méto-
nymie, les frontières des entités nommées. Puis nous proposons une approche pour améliorer les perfor-
mances de systèmes à base de connaissances, en utilisant des techniques exhaustives de fouille de données
séquentielles afin d’extraire des motifs qui représentent les structures linguistiques en jeu lors de la re-
connaissance d’entités nommées. Enfin, nous décrivons l’expérimentation menée à cet effet, donnons les
résultats obtenus à ce jour et en faisons une première analyse.
Abstract.         In this paper, we present and analyze the results obtained by our named entity recognition
system, CasEN, during the Ester2 evaluation campaign. We identify on what difficulties our system was the
most challenged, which mainly are : out-of-vocabulary words, metonymy and detection of the boundaries
of named entities. Next, we propose a direction which may help us for improving performances of our
system, by using exhaustive hierarchical and sequential data mining algorithms. This approach aims at
extracting patterns corresponding to useful linguistic constructs for recognizing named entities. Finaly, we
describe our experiments, give the results we currently obtain and analyze those results.
Mots-clés :              Reconnaissance d’Entités Nommées, Séquences Hiérarchiques, Motifs, Ester2.

Keywords:                Named Entity Recognition, Hierarchical Sequences, Patterns, Ester2.
1        Introduction
La campagne d’évaluation Ester2, organisée par l’AFCP1 et la DGA2 , a porté sur la transcription, la seg-
mentation et l’extraction d’informations de flux de parole en langue française radiodiffusés (Galliano et al.,
2009). La tâche d’extraction d’information portait sur la reconnaissance d’Entités Nommées (EN) dans les
transcriptions de ces flux (manuelles ou issues de systèmes de reconnaissance de parole). Les EN détectées
devaient être catégorisées selon sept catégories : personnes (pers), lieux (loc), organisations (org), produc-
tions humaines (prod), montants (amount), mesures de temps (time) et fonctions (fonc). Cette typologie
1
Association Francophone de la Communication Parlée
2
Direction Générale de l’Armement

N OUVEL D., S OULET A., A NTOINE J.Y., F RIBURGER N., M AUREL D.

a été sous-divisée en 38 catégories fine (qui n’ont pas été évaluées). La mesure des performances était le
« Slot Error Rate » (SER) (Makhoul et al., 1999) ; la précision, le rappel et la f-mesure étaient aussi fournis.
Sept systèmes (dont le nôtre, CasEN) ont participé à cette campagne, reposant sur des méthodes variées :
apprentissage par CRF, systèmes à base de règles, avec analyses syntaxiques de surface ou profondes.
Dans cet article, nous analysons les résultats de la campagne et les performances de notre système. Nous
présentons ensuite une technique de fouille de textes expérimentée dans le but de compléter de manière
semi-automatique la base de connaissance du système.
2        Le système CasEN dans la campagne Ester2
CasEN est développé sur la plateforme CasSys (Friburger, 2002) d’analyse de textes par cascades de trans-
ducteurs. Reposant sur la plateforme Unitex3 , CasSys applique les transducteurs dans un ordre prédéfini,
pour détecter des îlots de certitude (Abney, 1991), tout en réduisant progressivement l’espace de recherche.
CasSys peut être utilisé pour toute tâche d’analyse de texte, CasEN en est une application à la reconnais-
sance d’EN, initialement dédiée aux textes écrits (Friburger & Maurel, 2004; Friburger, 2006). Elle utilise
des transducteurs pour implémenter des motifs de surface qui reconnaissent des EN dans des textes. Le
système a été adapté à la langue parlée pour l’annotation du corpus Eslo dans le cadre du projet ANR
VariLing (Maurel et al., 2009), puis pour notre participation à Ester2.
Le tableau 1 présente les résul-                                         Référence            ASR
tats officiels de la campagne Ester2        Participant (approche) SER         P     R        SER
(Galliano et al., 2009) pour la tâche                    LIA (CRF) 23,9 86,4 71,8 43,4 (-19,5)
de reconnaissance des EN. Les sys-          LIMSI (syntaxe surface) 30,9 81,1 70,9 45,3 (-14,4)
tèmes centrés connaissances effec-           LINA (syntaxe surface) 37,1 80,7 55,4 54,0 (-16,9)
tuant une analyse syntaxique pro-         LI Tours (syntaxe surface) 33,7 79,3 65,8 50,7 (-17,0)
fonde obtiennent les meilleurs ré-                      LSIS (CRF) 35 82,6 73 55,3 (-20,3)
sultats pour la transcription de ré-    Synapse (syntaxe profonde) 9,9         93 89,3 44,9 (-35,0)
férence. Pour la transcription au-        Xerox (syntaxe profonde) 9,8 93,6 91,5 44,6 (-34,8)
tomatique (ASR, fournie par le
LIMSI), une approche à base d’ap- Tableau 1 – Reconnaissance d’EN lors d’Ester2 sur les transcription
prentissage l’emporte.                de référence (SER, Précision, Rappel) et automatiques (ASR)
Les performances de CasEN (LI Tours) sont proches de celles des autres systèmes non industriels (à
l’exception de celui du LIA), ce qui est rassurant pour un système initialement développé pour l’écrit. La
dégradation des performances de CasEN sur les transcriptions automatiques est satisfaisante : en cela notre
système s’avère être relativement tolérant aux erreurs de transcription, probablement grâce à la robustesse
des analyses syntaxiques partielles et à la faible dépendance des motifs aux lexiques.
Afin d’analyser le comportement du système, nous avons caractérisé chaque erreur de CasEN (1180 erreurs
pour 2512 EN) par sa localisation, le type d’erreur (suppression, insertion, catégorie erronée, erreur de
frontière, etc.) et la règle de la convention Ester2 concernée. A cette occasion, nous avons remarqué un
nombre non-négligeable d’incohérences dans l’annotation manuelle : de fausses erreurs d’insertions (EN
omises dans la référence mais correctement détectées par CasEN), des annotations de référence qui ne
respectent pas les règles spécifiées dans le guide d’annotation, etc. Au final, nous observons une réduction
3
http ://www-igm.univ-mlv.fr/˜unitex/

R ECONNAISSANCE D ’ ENTITÉS NOMMÉES :          ENRICHISSEMENT D ’ UN SYSTÈME À BASE DE
CONNAISSANCES À PARTIR DE TECHNIQUES DE FOUILLE DE TEXTES
de presque 10% du SER, après avoir corrigé manuellement la référence.
Par ailleurs, avec 7 types d’EN, Ester2 introduit une catégorisation plus fine que celles mise en place lors
des campagnes antérieures, rendant la catégorisation plus subtile (ce qui explique en partie les difficultés
qu’ont rencontrées les annotateurs humains). Certaines difficultés sont intéressantes, car elles relèvent du
phénomène de métonymie (Markert & Hahn, 2002). Dans d’autres cas, on peut au contraire estimer que
des sous-catégorisations artificielles ont eu une influence directe sur les résultats (Nouvel et al., 2010).
La figure 1 présente les performances de
CasEN par catégories. Globalement sa-
tisfaisante, la précision est médiocre pour
la catégorie prod (hétérogène, elle a aussi
gêné les autres participants). Le rappel
varie significativement d’une catégorie à
une autre. Nos difficultés sont en par-
tie dues au relatif manque de couver-
ture de notre lexique de noms propres
(notamment pour ceux d’origine afri-
caine dans les enregistrements de la radio
Africa1). Face à ce problème (Charton &
Figure 1 – Résultats de CasEN par type d’entités nommées        Torres-Moreno, 2009) propose l’extrac-
tion d’EN d’encyclopédies. Il n’en reste pas moins que dans ce type d’application, le problème des mots
hors-vocabulaire sera toujours présent, notamment du fait de la dépendance des EN au contexte du docu-
ment à analyser (Favre et al., 2005).
Nous avons estimé la part de chaque transducteur (qui implémente une famille de motifs) dans le taux d’er-
reurs du système (Nouvel et al., 2010). Huit transducteurs génèrent 54% des erreurs, mais ils reconnaissent
60% des EN : nous ne remarquons pas de transducteurs défaillants. Les transducteurs qui reconnaissent
des lieux font des erreurs liées à des usages métonymiques (par ex. « Egypte » pour une équipe sportive,
« Paris » pour le gouvernement français). Ceux qui reconnaissent des personnes ont des difficultés avec
les EN imbriquées : certaines fonctions s’étendent sur plusieurs mots et CasEN ne parvient pas à en dé-
tecter la frontière droite. Ceci concerne également les expressions de temps ou les organisations (par ex.
« [chambre régionale des comptes d’Ile-de-France] [...] »). Pour surmonter ce problème, nous envisageons
de parenthéser l’énoncé à l’aide d’un « chunker » (Antoine et al., 2008), afin de fournir une segmentation
utile et consistante.
3    Application de la fouille de texte à la reconnaissance d’EN
Une cause majeure d’erreurs du système CasEN est donc l’insuffisance de la couverture de son dictionnaire
lors de la recherche des EN. Ce constat nous a conduit à développer des techniques semi-automatiques
d’extension des bases de connaissances à l’aide de techniques de fouille de textes. Notre objectif est d’ex-
traire des motifs qui, à partir des lemmes ou des catégories morphosyntaxiques, caractérisent des « struc-
tures linguistiques » pertinentes pour la reconnaissance d’EN. Par exemple, le motif « DET président
ADJ NP NP » est approprié pour décrire une EN telle que « le président américain Barack Obama ».
Les éléments constituant les séquences, nommés items, pourront donc être issus de niveaux hiérarchiques
différents. L’objectif étant, à terme, d’enrichir la base de transducteurs de CasEN à l’aide de ces motifs.

N OUVEL D., S OULET A., A NTOINE J.Y., F RIBURGER N., M AUREL D.

3.1   Méthode d’extraction des règles séquentielles hiérarchiques

Le processus d’extraction de règles se décompose en trois phases : transformation du corpus en séquences
hiérarchiques ; extraction et groupement de séquences fréquentes ; sélection des meilleures règles.
Le corpus est segmenté en phrases, grâce aux tours de parole et aux étiquettes morphosyntaxiques : cha-
cune sera considérée comme une séquence. Les annotations d’EN du corpus de référence sont délimitées,
pour un type donné, par des balises "<typeBEG>" et "<typeEND>". Nous utilisons l’étiqueteur TreeTagger
(Schmid, 1994) pour transformer le corpus de référence, en associant à chaque terme sa catégorie (POS1 ),
sa sous-catégorie (POS2 ) morphosyntaxiques, et son lemme. Ainsi, le corpus est représenté comme suites
d’items, chaque item appartenant à une taxonomie, nous les notons POS1 /POS2 /LEMME. Enfin, nous
omettons les lemmes pour ce qui est reconnu par TreeTagger comme un nom propre (POS1 = "NAM"),
afin d’éviter (en première approche) de construire des motifs reposant sur des éléments lexicaux.
Considérons l’exemple suivant : « Le nouveau <PERSBEG> président Barack Obama
<PERSEND> est arrivé à <LOCBEG> Moscou <LOCEND>. Il y a vu le nouveau
<PERSBEG> président Dimitri Medvedev <PERSEND>[...] »
Il est transformé en séquences d’items, dont la première est : « DET/ART/le ADJ/nouveau
<PERSBEG> NOM/président NAM NAM <PERSEND> VER/pres/être VER/pper/arriver
PRP/à <LOCBEG> NAM <LOCEND> ».
La seconde étape extrait, à travers la taxonomie, toutes les séquences d’items fréquentes (selon un seuil
donné) (Mannila et al., 1997), qui contiennent au moins une balise d’EN. La recherche étant exhaustive
et tenant compte d’une hiérarchie, l’extraction génère une grande richesse de descriptions, mais aussi de
nombreux motifs redondants. Nous groupons les motifs de même longueur et de même fréquence, lorsque
l’un est généralisation de l’autre. Par exemple, le motif « ADJ/nouveau <PERSBEG> NOM » est plus
général que « ADJ/nouveau <PERSBEG> NOM/président » : s’ils ont mêmes fréquences, ils couvrent
alors les mêmes occurrences et un seul des deux suffirait, à priori, à reconnaître la forme d’EN qu’ils
couvrent. Nous avons évalué les motifs obtenus avec ou sans ce groupement et n’avons pas observé de
différence significative des performances. Voici des groupes que l’on obtiendrait sur l’exemple précédent :
Groupe A :
DET/ART ADJ <PERSBEG> NOM/président NAM NAM <PERSEND>
DET ADJ/nouveau <PERSBEG> NOM NAM NAM <PERSEND>
DET/ART/le ADJ/nouveau <PERSBEG> NOM/président NAM NAM <PERSEND>
Groupe B :
PRP/à <LOCBEG> NAM <LOCEND>
PRP <LOCBEG> NAM <LOCEND>
La dernière étape est la sélection des motifs selon leur confiance : la proportion de séquences, parmi
celles où le motif a été observé, qui comportent effectivement les annotations d’EN à détecter. Par groupe,
nous sélectionnons le motif qui a la meilleure confiance, si celle-ci dépasse un seuil donné. Ainsi, nous
obtenons les motifs qui, à priori, conduiraient, selon une bonne probabilité (la confiance), à la découverte
d’une annotation d’EN. Ceci peut-être mis en parallèle, en fouille de données, aux règles d’association.
Pour notre exemple, nous conservons les motifs suivants :
Règle sélectionnée pour le groupe A : DET ADJ NOM/président NAM NAM
=>DET ADJ <PERSBEG> NOM/président NAM NAM <PERSEND>
Règle sélectionnée pour le groupe B : PRP/à NAM => PRP/à <LOCBEG> NAM <LOCEND>

R ECONNAISSANCE D ’ ENTITÉS NOMMÉES :          ENRICHISSEMENT D ’ UN SYSTÈME À BASE DE
CONNAISSANCES À PARTIR DE TECHNIQUES DE FOUILLE DE TEXTES
3.2   Utilisation des règles pour la reconnaissance des EN

Les règles que nous obtenons par cette méthode nous permettent de déterminer dans quels contextes ap-
paraissent une ou plusieurs annotations d’EN. Cependant, l’objectif est d’obtenir des paires d’annotations
(ouvrantes et fermantes), correctement ordonnées et sans chevauchements, qui délimitent des EN. A cet
effet, nous mettons en œuvre, pour le moment, une stratégie simple, qui consiste à ne conserver que les
paires d’annotations à l’empan le plus large possible dans une fenêtre considérée.
3.3   Résultats et améliorations envisagées

La mise en application donne de nom-
breux motifs, qui sont souvent des
mélanges de lemmes et de catégo-
ries morphosyntaxiques : ceci nous
conforte dans l’idée qu’il est utile de
s’appuyer sur une taxonomie pour re-
connaître une EN. La figure 2 présente
la dispersion précision /rappel des mo-
tifs, extraits puis évalués sur Ester2 en
validation croisée (12 groupes). Nous
obtenons des motifs d’une bonne pré-
cision, mais le rappel demeure assez
faible. Comme attendu, baisser le seuil
de fréquence ou de confiance amène à
une sélection plus large de motifs, fai-
sant baisser la précision et augmenter
le rappel. La courbe à confiance 0,6 Figure 2 – Dispersion précision / rappel pour l’évaluation des
nous paraît être la plus intéressante en motifs à diverses confiances et selon le seuil fréquence
termes de compromis précision / rappel pour l’extraction des motifs. Mais ce paramètre est spécifique au
corpus utilisé, à la méthode d’extraction et d’utilisation des règles de reconnaissance d’EN. Nous expéri-
mentons actuellement une extraction sur un corpus élargi (Ester2 + Eslo, 250 Kmots).
Nous envisageons des améliorations sur ce procédé d’extraction de motifs. D’une part, afin d’éviter la
redondance entre motifs extraits, nous cherchons à mieux les grouper, indépendamment de leur taille,
à fréquence « proche ». Par ailleurs, nous remarquons que certains motifs gagneraient à contenir des
éléments optionnels ou disjonctifs (« président », « ministre », « gouverneur », etc.), ce que nous comptons
autoriser dans les motifs. L’objectif étant de constituer des groupes cohérents de motifs, qui correspondent
à des structures linguistiques à confronter aux transducteurs implémentés dans CasEN.
4     Conclusions et perspectives
Nous avons présenté les résultats du système CasEN dans la campagne d’évaluation Ester2, que nous avons
analysés pour déterminer les faiblesses réelles de notre système. Celles-ci se répartissent essentiellement
en trois catégories : résolution de la métonymie, délimitation des frontières, couverture du système.

N OUVEL D., S OULET A., A NTOINE J.Y., F RIBURGER N., M AUREL D.

La direction que nous avons prise, plus particulièrement pour le manque de couverture, a été d’adopter
une approche semi-automatique, systématique et exhaustive, afin de trouver les structures linguistiques
correspondant à des EN. Par des techniques de fouille de données, nous pensons apporter de nouveaux
éléments, jusque là non implémentés par CasEN, qui nous donneront des pistes pour améliorer le système.
Nous cherchons actuellement à estimer les gains à espérer en intégrant les motifs trouvés par fouille de
textes dans CasEN, afin d’évaluer les perspectives que nous ouvrent cette approche. Par ailleurs, nous
menons des expériences pour déterminer la dépendance de cette extraction au corpus, autant du point de
vue du domaine et des thématiques que des dates d’enregistrement du corpus.
5    Remerciements
Ce travail a été réalisé dans le cadre des projets Variling (ANR-06-CORP-023), Epac (ANR-00-MDCA-
006-03), créés par l’Agence Nationale de la Recherche (ANR) et FEDER Région Centre.
Références
A BNEY S. P. (1991). Parsing by Chunks, In Principle-Based Parsing, p. 257–278. Kluwer.
A NTOINE J.-Y., M OKRANE A. & F RIBURGER N. (2008). Automatic rich annotation of large corpus of
conversational transcribed speech : the chunking task of the epac project. In LREC’08.
C HARTON E. & T ORRES -M ORENO J. M. (2009). Classification d’un contenu encyclopédique en vue
d’un étiquetage par entités nommées. In TALN’2009.
FAVRE B., B ÉCHET F. & N OCERA P. (2005). Robust named entity extraction from large spoken ar-
chives. In HLT/EMNLP’05.
F RIBURGER N. (2002). Reconnaissance automatique des noms propres : application à la classification
automatique de textes journalistiques. PhD thesis, Université François-Rabelais Tours, France.
F RIBURGER N. (2006). Linguistique et reconnaissance automatique des noms propres. Meta : Transla-
tors’ Journal, 51-4, 637–650.
F RIBURGER N. & M AUREL D. (2004). Finite-state transducer cascades to extract named entities in
texts. Theoretical Computer Sciences, 313, 93–104.
G ALLIANO S., G RAVIER G. & C HAUBARD L. (2009). The ester 2 evaluation campaign for the rich
transcription of french radio broadcasts. In Interspeech’09, p. 2583–2586.
M AKHOUL J., K UBALA F., S CHWARTZ R. & W EISCHEDEL R. (1999). Performance measures for
information extraction. In DARPA Broadcast News Workshop, p. 249–252.
M ANNILA H., T OIVONEN H. & V ERKAMO A. I. (1997). Discovery of frequent episodes in event
sequences. Data Mining and Knowledge Discovery, 1, 259–289.
M ARKERT K. & H AHN U. (2002). Understanding metonymies in discourse. AI, 135, 145–198.
M AUREL D., F RIBURGER N. & E SHKOL I. (2009). Who are you, you who speak ? In LTC’09.
N OUVEL D., A NTOINE J.-Y., F RIBURGER N. & M AUREL D. (2010). An analysis of the performances
of the casen named entities recognition system in the ester2 evaluation campaign. In LREC’10.
S CHMID H. (1994). Probabilistic part-of-speech tagging using decision trees. In NEMLP’94, p. 44–49.

Évaluer des annotations manuelles dispersées : les coefficients
sont-ils suffisants pour estimer l’accord inter-annotateurs ?

Karën Fort1,2 Claire François1 Maha Ghribi1
(1) INIST / CNRS, 2 allée de Brabois, 54500 Vandoeuvre-lès-Nancy
(2) LIPN, Université Paris 13 & CNRS, 99 av. J.B. Clément, 93430 Villetaneuse
{karen.fort,claire.francois,maha.ghribi}@inist.fr

Résumé.         L’objectif des travaux présentés dans cet article est l’évaluation de la qualité d’annotations
manuelles de relations de renommage de gènes dans des résumés scientifiques, annotations qui présentent
la caractéristique d’être très dispersées. Pour cela, nous avons calculé et comparé les coefficients les plus
communément utilisés, entre autres κ (Cohen, 1960) et π (Scott, 1955), et avons analysé dans quelle me-
sure ils sont adaptés à nos données. Nous avons également étudié les différentes pondérations applicables
à ces coefficients permettant de calculer le κ pondéré (Cohen, 1968) et l’α (Krippendorff, 1980, 2004).
Nous avons ainsi étudié le biais induit par la grande prévalence d’une catégorie et défini un mode de calcul
des distances entre catégories reposant sur les annotations réalisées.
Abstract.        This article details work aiming at evaluating the quality of the manual annotation of gene
renaming relations in scientific abstracts, which generates sparse annotations. To evaluate these anno-
tations, we computed and compared the results obtained using the commonly advocated inter-annotator
agreement coefficients such as κ (Cohen, 1960) or π (Scott, 1955) and analyzed to which extent they are
relevant for our data. We also studied the different weighting computations applicable to κω (Cohen, 1968)
and α (Krippendorff, 1980, 2004) and estimated the bias introduced by prevalence. We then define a way
to compute distances between categories based on the produced annotations.
Mots-clés :               Annotation manuelle, évaluation, accord inter-annotateurs.

Keywords:                 Manual annotation, evaluation, inter-annotator agreement.
1        Introduction

De nombreuses tâches de traitement automatique des langues (TAL) nécessitent une annotation manuelle
en amont, afin, non seulement d’entraîner des outils automatiques, mais également de créer une référence
pour l’évaluation. Or, s’il a été démontré qu’une annotation incohérente limite les capacités des moteurs
entraînés à partir de celle-ci (Alex et al., 2006; Reidsma & Carletta, 2008), la qualité de cette référence est
rarement justifiée. En effet, peu de campagnes détaillent la manière dont celle-ci a été constituée. Lorsque
des mesures d’accord inter-annotateurs sont données, elles le sont sous forme d’un coefficient qui est
devenu un standard de fait : le “kappa” de Cohen (1960) ou de Carletta (1996), sans plus de précision.1
1
Pour plus de détails sur les problèmes de terminologie liés aux “kappa“, voir l’introduction de (Artstein & Poesio, 2008).

K ARËN F ORT, C LAIRE F RANÇOIS , M AHA G HRIBI

Di Eugenio & Glass (2004) ont montré la sensibilité de ces coefficients au biais entre annotateurs et au
problème de prévalence. La discussion reste très ouverte concernant la représentativité de ces différents
coefficients et la nécessité d’en présenter plusieurs. Artstein & Poesio (2008) ont réalisé un inventaire
très intéressant des différents modes de calcul de l’accord inter-annotateurs et ont discuté l’utilisation de
ces mesures dans les tâches d’annotation en TAL. Cependant, il reste difficile de savoir quel coefficient
utiliser en fonction des caractéristiques des données. Nous présentons dans cet article l’évaluation que
nous avons réalisé d’une campagne d’annotations manuelles en appliquant et comparant les différentes
méthodes proposées par ces auteurs.
Nous décrivons brièvement la campagne d’annotation que nous avons menée, puis nous détaillons et ana-
lysons les résultats des accords inter-annotateurs obtenus en utilisant les coefficients simples S, π et κ.
Nous appliquons ensuite les coefficients pondérés α et κω , pour lesquels nous étudions le calcul des dis-
tances entre catégories. Enfin, nous discutons des résultats qui nous semblent nécessaires à présenter, en
particulier dans des cas comme le nôtre de répartition non homogène des phénomènes langagiers.
2       Présentation de la campagne d’annotation

L’INIST a été chargé, dans le cadre du programme Quaero2 , de faire annoter par ses experts les relations de
renommages de gènes de Bacillus Subtilis présentes dans un corpus de 1 843 textes courts (résumés), soit
plus de 400 000 tokens (ici, chaînes de caractères séparées par des blancs), sélectionnés dans Medline par
l’équipe MIG de l’INRA de Jouy en Josas3 , à l’aide de nomenclatures de noms de gènes et d’un ensemble
de mots-clefs dénotant des renommages.
Cette annotation avait pour but, d’une part, de construire une base de données de couples de renommage de
gènes de Bacillus Subtilis, et d’autre part d’entraîner et d’évaluer les outils d’extraction automatique des
partenaires du programme. Au final, cette campagne aura permis de mettre au jour manuellement environ
200 couples de renommage, tel que : “Inactivation of a previously unknown gene, yqzB (renamed ccpN
for control catabolite protein of gluconeogenic genes [..]”.
Nous avons appliqué pour cela la méthodologie proposée par Fort et al. (2009), et avons calculé l’accord
inter-annotateurs dès le début de la campagne, afin de mettre au jour les désaccords et de modifier le
guide d’annotation en conséquence. Nous avons donc fait annoter par deux annotateurs experts (que nous
noterons ici, A1 et A2) un même échantillon de 93 fichiers, correspondant à plus de 15 000 tokens, à
partir duquel nous avons ensuite calculé l’accord inter et intra-annotateurs, tel que recommandé par Gut
& Bayerl (2004). Les relations de renommage sont annotées ici très simplement, grâce à l’outil Cadixe4 ,
en sélectionnant le nom d’origine du gène (annoté Former), puis son nouveau nom (annoté New). Le reste
du texte n’est pas annoté, mais doit être pris en compte dans le calcul de l’accord inter-annotateurs. Nous
avons décidé de nommer cette “pseudo” catégorie Rien, en français, pour la différencier des catégories
signifiantes Former et New.
Certains fichiers (plus d’un tiers d’entre eux) ne comportent pas de renommage du tout. Nous obtenons
ainsi, en moyenne sur l’échantillon, 1 renommage par fichier. Les accords et désaccords ont été analysés
qualitativement, ce qui nous a permis d’ajouter les cas non traités dans le guide d’annotation. Les résultats
2
http://quaero.org
3
Merci à cette équipe : http://genome.jouy.inra.fr/bibliome/renommage/
4
http://caderige.imag.fr/Cadixe/

É VALUER DES ANNOTATIONS MANUELLES DISPERSÉES

quantitatifs de cette annotation sont présentés sous forme de matrice de confusion dans le tableau 1. Les
résultats en diagonale présentent le nombre d’éléments sur lesquels les deux annotateurs sont d’accord,
pour chaque catégorie. Les autres cellules représentent les éléments pour lesquels les annotateurs ont
choisi des catégories différentes (par exemple, 13 éléments ont été annotés New par A1 et Former par A2).
Cette matrice révèle la prédominance de la catégorie Rien (plus de 99% du corpus) et montre ainsi que les
éléments annotés sont très dispersés.

A1
Former        New Rien           Total
Former   71           13    23          107
New      8           69    15           92
A2
Rien     7            8  18 840       18 855
Total   86           90 18 878        19 054
TAB . 1 – Matrice de confusion calculée à partir de l’ensemble des tokens

Pour construire cette matrice de confusion, nous avons choisi de prendre en compte le nombre total de
tokens, soit 19 054. Partant du principe que les noms de gènes correspondent à un sous-ensemble bien
spécifique de tokens dans les textes, nous pourrions également considérer l’ensemble total des occurrences
de noms de gènes, soit 1 165, selon les résultats obtenus par MIG après application d’un dictionnaire de
noms de gènes de l’INRA. Néanmoins, ce choix nous semble discutable, d’une part parce que la fiabilité
des résultats dépend de la complétude du dictionnaire, qui, étant donné la forte évolutivité du domaine, ne
peut être totale, et d’autre part, parce que cela revient à négliger le fait que les annotateurs doivent souvent
lire tout le texte pour prendre des décisions, le renommage n’étant parfois attesté qu’à la fin du texte.
Nous verrons dans la section 3.2 que cette décision a un impact non nul sur les résultats. Il est donc
fondamental de justifier ce type de choix lorsque l’on donne des résultats d’accord inter-annotateurs.
3     Évaluation à l’aide de coefficients simples
Dans la suite de l’article, nous utiliserons les notations et les formules de Artstein & Poesio (2008) concer-
nant les mesures d’accord inter-annotateurs. Les calculs seront réalisés par défaut à partir du tableau 1.
3.1    Calcul des coefficients simples

La mesure la plus évidente d’accord inter-annotateurs est l’accord observé (Ao ), qui correspond à la pro-
portion d’éléments sur lesquels les annotateurs sont d’accord, autrement dit, le nombre total d’éléments
pour lesquels il y a accord, divisé par le nombre total d’éléments, ici :
71 + 69 + 18840
Ao =                       = 0, 996116
19054
Le résultat est extrêmement élevé, mais il ne prend pas en compte l’accord attendu (expected agreement,
Ae ), c’est-à-dire la possibilité que les annotateurs classent un élément quelconque dans une même catégo-
rie par hasard.

K ARËN F ORT, C LAIRE F RANÇOIS , M AHA G HRIBI

Pour analyser nos résultats nous utilisons donc ici les coefficients permettant de prendre en compte le
hasard, décrits par Artstein & Poesio (2008) : S (Bennett et al., 1954), κ (Cohen, 1960) et π (Scott, 1955),
qui sont tous les trois obtenus à partir de la formule suivante, dans laquelle seul l’accord attendu (Ae )
diffère selon le coefficient :
Ao − Ae
S, κ, π =
1 − Ae

La différence entre ces coefficients réside dans la manière de calculer l’accord attendu en fonction des hy-
pothèses concernant le comportement des annotateurs dans le cas d’une annotation des éléments au hasard.
S suppose que les annotations réalisées au hasard suivent une distribution uniforme dans les différentes
catégories (ici, trois), l’accord attendu est donc calculé de la façon suivante :
1
ASe =     = 0, 333333
3
S = 0, 99417
Le biais le plus important de cette mesure est qu’elle est directement corrélée au nombre de catégories. Par
conséquent, plus le nombre de catégories est élevé, plus l’accord attendu est faible, ce qu’il est en général,
sa valeur maximale étant de 0, 5 ( 12 ) pour deux catégories.
Le coefficient π (Scott, 1955), appelé également K dans (Siegel & Castellan, 1988) ou kappa dans (Car-
letta, 1996), considère lui aussi que les distributions réalisées par les annotateurs par hasard sont équiva-
lentes, mais il suppose que la répartition des éléments entre catégories n’est pas homogène et qu’elle peut
être estimée par la répartition moyenne réalisée par les annotateurs. L’accord attendu est donc calculé de
la façon suivante :
(( 86+107 )2 + ( 90+92 )2 + ( 18878+18855 )2 )
Aπe =       2             2               2
= 0, 980464
190542
π = 0, 8012

Le coefficient κ (Cohen, 1960) suppose lui dans sa modélisation du hasard que la répartition des éléments
entre catégories peut être différente pour chaque annotateur. Dans ce cas, la probabilité pour qu’un élément
soit assigné dans une catégorie est le produit de la probabilité que chaque annotateur l’assigne dans cette
catégorie. L’accord attendu est donc calculé de la façon suivante :

(86 × 107) + (90 × 92) + (18878 × 18855)
Aκe =                                            = 0, 980463
190542
κ = 0, 80121
3.2    Analyse des résultats

En comparant les 3 coefficients obtenus, nous observons que S (0,99417) est à peine plus faible que l’ac-
cord observé (0,996116), tandis que π (0,8012) et κ (0,80121) sont très proches, tout en étant plus faibles
que Ao et S. La valeur relative de ces coefficients est conforme à l’ordre S π et π κ décrit par Artstein
& Poesio (2008). La valeur élevée de S montre que les éléments sont annotés selon une certaine logique.
Pour un accord observé constant, le coefficient S ne dépend que du nombre de catégories, il n’est donc
pas sensible à la répartition des éléments dans les catégories, au contraire de π et κ (Di Eugenio & Glass,

É VALUER DES ANNOTATIONS MANUELLES DISPERSÉES

2004). Ces auteurs montrent que lorsque les catégories sont disproportionnées, en dépit d’un fort accord
sur la catégorie prédominante, les coefficients π et κ sont très sensibles aux désaccords sur les catégories
minoritaires. Les coefficients de type κ sont interprétés comme étant corrects à partir de 0,67 (Krippen-
dorff, 1980), κ et π sont donc ici très satisfaisants, ce qui nous rassure quant à l’accord obtenu dans les
deux catégories minoritaires. Par ailleurs, κ et π sont très proches, ce qui, selon Di Eugenio & Glass (2004)
est très courant, et signifie que nos données montrent peu de biais dû aux annotateurs, puisque, dans le cas
de deux annotateurs, cela reflète des distributions marginales similaires (Artstein & Poesio, 2008).
Nos résultats sont donc élevés et montrent peu de biais. Ils nous semblent pourtant peu sûrs, puisqu’ils
mettent sur le même plan des catégories très hétérogènes, deux minoritaires mais signifiantes (Former et
New), et une non signifiante (Rien) très majoritaire. Notre problème est donc de nous assurer que ces coef-
ficients calculés sur les trois catégories reflètent un accord significatif sur les deux catégories signifiantes
Former et New.
Une première preuve de l’influence de ce déséquilibre apparaît en examinant l’évolution de ces coefficients
en fonction de la référence choisie pour définir la catégorie Rien. En effet, si nous choisissons non plus le
nombre total de tokens, mais le nombre d’occurrences de noms de gènes (1 165), à partir de la matrice de
confusion du tableau 2, nous obtenons S = 0, 90472, π = 0, 77557 et κ = 0, 77571. Ces trois coefficients
ont des valeurs inférieures aux précédentes et présentent un écart constant entre eux. Même si la répartition
des éléments et le comportement des annotateurs semblent constants, la taille de la catégorie Rien influe
donc sur la valeur définitive des accord inter-annotateurs.

A1
Former    New    Rien Total Noms gènes
Former          71       13     23         107
New             8       69     15          92
A2
Rien            7       8     951         966
Total Noms Gènes      86       90    989        1 165

TAB . 2 – Matrice de confusion calculée à partir des noms de gènes
Une seconde méthode pour estimer dans quelle mesure sa très forte prévalence entraîne un biais dans le
calcul des accords inter-annotateurs est de considérer uniquement les catégories Former et New et calculer
la matrice de confusion correspondante (tableau 3).

A1
Former New       Total
Former      71    13        84
A2      New         8    69        77
Total      79    82        161

TAB . 3 – Matrice de confusion sans la catégorie Rien
A partir de ce tableau et des formules présentées en section 3.1, nous obtenons π = 0, 7390 et κ = 0, 73934.
Ces valeurs sont inférieures aux coefficients π et κ obtenus à partir de la matrice de confusion complète
(tableau 1), ou de la matrice de confusion obtenue à partir de l’ensemble des occurrences de noms de

K ARËN F ORT, C LAIRE F RANÇOIS , M AHA G HRIBI

gènes (tableau 2). La catégorie Rien fait l’objet d’un accord très important et présente une très forte pré-
valence, ce qui semble avoir induit une surestimation de ces coefficients, surestimation dont l’importance
dépend de la taille de cette catégorie.
Laignelet & Rioult (2009), confrontés à la même disproportion entre catégories dans leur campagne d’an-
notation, se sont appuyés sur une suggestion de Hripcsak & Heitjan (2002) et ont utilisé le coefficient
R (Finn, 1970) proposé dans le logiciel R5 . Le coefficient R est calculé selon la formule suivante :

V ariance observee
R=1−
V ariance attendue
la variance observée étant la moyenne des variances sur les éléments annotés et la variance attendue étant
la variance de la distribution uniforme discrète à n catégories (ci-dessous nb categories), soit6 :

(nb categories)2 − 1
V ariance attendue =
12

Dans notre cas, nous obtenons R = 0, 994. Cette valeur proche de S (0,99417) peut être expliquée par le
fait que ce coefficient modélise le hasard comme S, en considérant une distribution uniforme des catégories
et n’est donc pas plus sensible que S à la répartition des éléments dans les catégories. Notre conclusion
rejoint de ce point de vue l’opinion de Ron Artstein, lorsqu’il dit : “R is similar to Krippendorff’s alpha
except that it assumes a uniform distribution as its model of chance annotation ; R is to alpha like S is to
Scott’s pi, and the same criticisms apply.” (R. Artstein, communication personnelle, 4 décembre 2009).
Le coefficient R de Finn n’apporte pas plus que S dans des cas de dispersion des annotations et donc de
dissymétrie des catégories.
4       Évaluation utilisant des coefficients pondérés

Selon Artstein & Poesio (2008), π et κ ont pour défaut de traiter tous les désaccords de la même manière
et seuls des coefficients pondérés permettent de donner plus d’importance à certains désaccords.
4.1     Calcul des coefficients κω et α

Artstein & Poesio (2008) détaillent deux coefficients pondérés : la version pondérée de κ, κω (Cohen,
1968) et l’α (Krippendorff, 1980, 2004). Ces deux coefficients prennent pour base le désaccord entre an-
notateurs et utilisent une distance entre les catégories décrivant à quel point deux catégories sont distinctes
l’une de l’autre. On trouve dans (Artstein & Poesio, 2008) une discussion sur la définition de cette distance
en fonction du type d’annotation. Cette distance permet entre autres de traiter des annotations de struc-
tures complexes en introduisant plusieurs valeurs de distance entre annotations. Cette méthode présente
l’inconvénient de complexifier l’interprétation des résultats.
5
http ://www.r-project.org/
6
Finn (1970) ne détaille pas le calcul de cette variance attendue, mais on le trouve dans les sources de la li-
brairie irr du logiciel R. Pour une explication plus approfondie, voir : http://mathworld.wolfram.com/
DiscreteUniformDistribution.html.

É VALUER DES ANNOTATIONS MANUELLES DISPERSÉES

Dans notre cas, nous avons 2 catégories signifiantes Former et New et une non signifiante, Rien. Nous
considérons donc qu’il est plus important d’identifier les couples de noms de gènes que de déterminer
l’antériorité d’un nom par rapport à l’autre. Par conséquent, la distance entre Former et New devrait être
moindre que celle entre ceux-ci et Rien. Si nous faisons l’hypothèse qu’elle est deux fois moindre, nous
obtenons le tableau de distances entre catégories suivant (dans l’intervalle [0,1]) :

Former New         Rien
Former        0   0,5          1
New          0,5   0           1
Rien          1    1           0
TAB . 4 – Tableau de distances estimées entre catégories

Les coefficients pondérés κω et α sont calculés à partir de la formule suivante :
D0
κω , α = 1 −
De
où D0 représente le désaccord observé entre les annotateurs et De représente le désaccord attendu (expec-
ted), autrement dit, si l’affectation est réalisée au hasard. Le désaccord attendu de κω et d’α suit la même
logique que κ et π respectivement, et inclut la notion de distance entre catégories. Il est à noter que si
toutes les catégories sont parfaitement distinctes, nous obtenons α = π et κω = κ. Nous obtenons, à partir
des distances du tableau 4, α = 0, 8292 et κω = 0, 8291. Ces valeurs plus élevées que π et κ montrent que
la pondération a fait diminuer le désaccord et augmenter légèrement l’accord inter-annotateurs.
4.2    Calcul des distances entre catégories à partir de la matrice de confusion

Pour pondérer l’accord inter-annotateurs, les distances entre catégories sont définies à partir de connais-
sances préalables sur la tâche d’annotation. En parallèle, il nous semble utile de les évaluer également en
fonction de la difficulté qu’ont les annotateurs à répartir les éléments entre les catégories et de confronter
ces deux approches. Pour ce calcul, nous utilisons la matrice de confusion du tableau 1.
Nous considérons que deux catégories sont distinctes s’il y a peu de chance d’erreur de classement entre
elles. Plus précisément, soient deux catégories C1 et C2 appartenant à l’ensemble des catégories considé-
rées, P (C2 |C1 ) représente la probabilité qu’un annotateur affecte un élément à la catégorie C2 sachant que
le deuxième annotateur l’affecte à la catégorie C1 et elle se calcule de la façon suivante :

n1C1 ,2C2 + n2C1 ,1C2
P (C2 |C1 ) =
nC1
avec n1C1 ,2C2 représentant le nombre d’éléments classés par l’annotateur 1 dans la catégorie C1 alors que
l’annotateur 2 les a classés dans la catégorie C2 ; nC1 représente la somme des éléments classés dans la
catégorie C1 par les deux annotateurs.
Quand cette probabilité est faible, la catégorie C2 est peu similaire à la catégorie C1 et le risque d’obtenir
une classification différente est faible. Nous avons ainsi, selon les données du tableau 1 :
13 + 8
P (N ew|F ormer) =                = 0, 108808
107 + 86

K ARËN F ORT, C LAIRE F RANÇOIS , M AHA G HRIBI

Former    New      Rien
Former    0,735751 0,108808 0,155440
New       0,115385 0,758242 0,126374
Rien      0,000795 0,000609 0,998595
TAB . 5 – Tableau de probabilités

Dans le tableau 5, qui présente les valeurs de probabilité calculées pour notre cas d’application, la dia-
gonale permet d’estimer l’accord entre annotateurs pour chaque catégorie. Il est très important pour la
catégorie Rien (99 % d’accord) et plus faible pour les catégories Former et New (73 et 75 % d’accord res-
pectivement). Les autres cellules du tableau permettent d’estimer le désaccord entre annotateurs, catégorie
par catégorie. Ces probabilités sont très faibles, les catégories sont donc peu similaires. Nous observons
également que ces probabilités sont asymétriques. Les valeurs P (F ormer|Rien) et P (N ew|Rien) sont
très faibles (<1‰), le risque d’affecter un élément à la catégorie Former ou à la catégorie New sachant
qu’il est déjà affecté à Rien est donc quasiment nul. Inversement, le risque d’affecter un élément à la caté-
gorie Rien alors qu’il est déjà dans la catégorie Former ou dans la catégorie New est plus élevé (15 % et
12 % respectivement).
Nous utilisons ces probabilités dans le calcul des distances entre catégories selon le principe suivant :
d(C1, C1) = 0, et pour tout C1 différent de C2 , d(C1 , C2 ) = 1 − P (C1 |C2 ).
Les probabilités n’étant pas symétriques, cette formule ne peut pas être utilisée telle quelle. En utilisant
les mêmes hypothèses sur les distributions des annotations que pour la définition des coefficients, nous
proposons deux transformations. Premièrement, le coefficient α suppose que, dans le cas d’une annotation
au hasard, les annotateurs réalisent des distributions équivalentes. Nous définissons donc la distance as-
sociée comme étant la moyenne des distances orientées (calculées à partir du tableau 5) selon la formule
suivante :
(1 − P (C2 |C1 )) + (1 − P (C1 |C2 ))
dα (C1 , C2 ) =
2
Ce qui donne, dans notre cas :
13+8                13+8
(1 −   107+86
)   + (1 −   90+92
dα (F ormer, N ew) =                                         = 0, 887904
2

Le κω , quant à lui, suppose que les annotateurs procèdent de manière différente, aussi nous calculons la
distance associée comme étant le produit des distances orientées (calculées à partir du tableau 5) selon la
formule suivante :
dκω (C1 , C2 ) = (1 − P (C2 |C1 )) × (1 − P (C1 |C2 ))

Dans notre cas, nous obtenons donc :
13 + 8            13 + 8
dκω (F ormer, N ew) = (1 −             ) × (1 −         ) = 0, 788362
107 + 86          90 + 92
Dans le tableau 6, nous observons que d(F ormer, N ew) est inférieure dans les deux cas à d(F ormer, Rien)
et d(N ew, Rien), ce qui semblerait confirmer que Former et New sont plus proches entre elles que de Rien.
Cependant, ces valeurs sont nettement supérieures aux valeurs estimées dans le tableau 4, la distinction
entre ces deux catégories s’avère donc dans les faits moins problématique que ce que nous avions craint.

É VALUER DES ANNOTATIONS MANUELLES DISPERSÉES

dα       dκω
d(Former,New) 0,887904 0,788362
d(Former,Rien) 0,921882 0,843888
d(New,Rien)    0,936508 0,873094
TAB . 6 – Tableau des distances entre catégories calculées à partir des données

5        Conclusion
A partir d’une campagne d’annotation, nous avons analysé différents modes de calcul pour estimer l’ac-
cord inter-annotateurs. La particularité de cette campagne est le caractère très dispersé des annotations
dans les textes qui induit un biais lié à la grande prévalence des tokens non annotés. La matrice de confu-
sion synthétise parfaitement cette information. Le tableau des distances calculées entre catégories montre
que toutes les catégories sont bien distinctes, même les deux catégories minoritaires mais signifiantes For-
mer et New. Bien que les coefficients π, κ, α et κw soient très sensibles à ce biais de prévalence, ils restent
satisfaisants dans notre cas, indiquant ainsi un bon accord pour ces deux catégories. En première approxi-
mation, nous pouvons estimer ce biais en comparant les résultats obtenus avec les matrices de confusion
suivantes : complète, réduite aux noms de gènes et réduite aux deux catégories signifiantes.
En complément des coefficients, et quand le mode opératoire de l’annotation le permet, le premier résultat
à présenter est à notre avis la matrice de confusion accompagnée d’explications précises sur les choix
effectués. Nous rejoignons Hripcsak & Heitjan (2002) lorsqu’ils écrivent “showing the two-by-two contin-
gency table with its marginal totals is probably as informative as any measure“. En effet, cette matrice
résume les informations quantitatives obtenues dans une campagne d’annotation et permet entre autres
d’avoir rapidement une idée des problèmes de prévalence et de biais entre annotateurs.
Le tableau des distances entre catégories calculées à partir des résultats d’annotation est également très
riche en information car il permet d’analyser le risque réel d’erreur entre certaines catégories et de le
confronter aux distances définies à priori en fonction des connaissances du domaine. Ces différents ta-
bleaux permettent de comprendre au mieux les caractéristiques de la campagne d’annotation et d’interpré-
ter les différents coefficients obtenus selon leur mode de calcul.
De nouvelles campagnes d’annotation sont en cours et devraient nous permettre de tester les différents
coefficients ainsi que la reproductibilité de nos propositions dans des cas aussi variés que l’annotation de
brevets en pharmacologie (entités nommées, termes) ou des commentaires de matchs de football (entités
nommées, relations diverses). Ces campagnes devraient également nous permettre d’élargir la réflexion
à des annotations réalisées par plus de deux annotateurs. Dans ce dernier cas, le tableau des distances
entre catégories calculées à partir des résultats d’annotation permettra de réaliser une bonne synthèse des
problèmes existants pour distinguer les catégories.
Remerciements
Ce travail a été réalisé en partie dans le cadre du programme Quaero 7 , financé par OSEO, agence natio-
nale de valorisation de la recherche. Nous en remercions les participants, en particulier l’équipe MIG de
7
http://www.quaero.org

K ARËN F ORT, C LAIRE F RANÇOIS , M AHA G HRIBI

l’INRA. Nous remercions également F. Tisserand et B. Taliercio, les annotateurs experts de l’INIST, ainsi
que Ron Artstein, pour son intérêt et ses réponses détaillées.
Références
A LEX B., N ISSIM M. & G ROVER C. (2006). The impact of annotation on the performance of protein
tagging in biomedical text. In Proceedings of The Fifth International Conference on Language Resources
and Evaluation (LREC), p. 595–600, Gène, Italie.
A RTSTEIN R. & P OESIO M. (2008). Inter-coder agreement for computational linguistics. Computational
Linguistics, 34(4), 555–596.
B ENNETT E. M., A LPERT R. & C.G OLDSTEIN A. (1954). Communications through limited questio-
ning. Public Opinion Quarterly, 18(3), 303–308.
C ARLETTA J. (1996). Assessing agreement on classification tasks : The kappa statistic. Computational
Linguistics, 22, 249–254.
C OHEN J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Mea-
surement, 20(1), 37–46.
C OHEN J. (1968). Weighted kappa : Nominal scale agreement with provision for scaled disagreement or
partial credit. Psychological Bulletin, 70(4), 213–220.
D I E UGENIO B. & G LASS M. (2004). The kappa statistic : a second look. Computational Linguistics,
30(1), 95–101.
F INN R. H. (1970). A note on estimating the reliability of categorical data. Educational and Psycholo-
gical Measurement, 30, 71–76.
F ORT K., E HRMANN M. & N AZARENKO A. (2009). Vers une méthodologie d’annotation des entités
nommées en corpus ? In Actes de la 16ème Conférence sur le Traitement Automatique des Langues
Naturelles 2009 Traitement Automatique des Langues Naturelles 2009, Senlis, France.
G UT U. & BAYERL P. S. (2004). Measuring the reliability of manual annotations of speech corpora. In
Proceedings of Speech Prosody, p. 565–568, Nara, Japon.
H RIPCSAK G. & H EITJAN D. F. (2002). Measuring agreement in medical informatics reliability studies.
Journal of Biomedical Informatics, 35(2), 99–110.
K RIPPENDORFF K. (1980). Content Analysis : An Introduction to Its Methodology, chapter 12. Sage :
Beverly Hills, CA.
K RIPPENDORFF K. (2004). Content Analysis : An Introduction to Its Methodology, second edition,
chapter 11. Sage : Thousand Oaks, CA.
L AIGNELET M. & R IOULT F. (2009). Repérer automatiquement les segments obsolescents à l’aide
d’indices sémantiques et discursifs. In Actes de Traitement Automatique des Langues Naturelles (TALN
2009), Senlis, France.
R EIDSMA D. & C ARLETTA J. (2008). Reliability measurement without limits. Computational Linguis-
tics, 34(3), 319–326.
S COTT W. A. (1955). Reliability of content analysis : The case of nominal scale coding. Public Opinion
Quaterly, 19(3), 321–325.
S IEGEL S. & C ASTELLAN N. J. (1988). Nonparametric Statistics for the Behavioral Sciences. New
York : McGraw-Hill, 2nd edition.

Un turc mécanique pour les ressources linguistiques :
critique de la myriadisation du travail parcellisé

Benoît Sagot1       Karën Fort2,3        Gilles Adda4 Joseph Mariani4,5                 Bernard Lang6
(1) Alpage, INRIA Paris–Rocquencourt & Université Paris 7,
Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France
(2) INIST-CNRS, 2 allée de Brabois, 54500 Vandoeuvre-lès-Nancy, France
(3) LIPN, Université Paris Nord, 99 av J-B Clément, 93430 Villetaneuse, France
(4) LIMSI-CNRS, Bât. 508, rue John von Neumann, Université Paris-Sud BP 133, 91403 Orsay Cedex, France
(5) IMMI-CNRS, Bât. 508, rue John von Neumann, Université Paris-Sud BP 133, 91403 Orsay Cedex, France
(6) INRIA Paris–Rocquencourt, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France
{benoit.sagot, bernard.lang}@inria.fr, karen.fort@inist.fr, {gilles.adda,joseph.mariani}@limsi.fr

Résumé.          Cet article est une prise de position concernant les plate-formes de type Amazon Mechanical Turk,
dont l’utilisation est en plein essor depuis quelques années dans le traitement automatique des langues. Ces plate-
formes de travail en ligne permettent, selon le discours qui prévaut dans les articles du domaine, de faire développer
toutes sortes de ressources linguistiques de qualité, pour un prix imbattable et en un temps très réduit, par des gens
pour qui il s’agit d’un passe-temps. Nous allons ici démontrer que la situation est loin d’être aussi idéale, que
ce soit sur le plan de la qualité, du prix, du statut des travailleurs ou de l’éthique. Nous rappellerons ensuite les
solutions alternatives déjà existantes ou proposées. Notre but est ici double : informer les chercheurs, afin qu’ils
fassent leur choix en toute connaissance de cause, et proposer des solutions pratiques et organisationnelles pour
améliorer le développement de nouvelles ressources linguistiques en limitant les risques de dérives éthiques et
légales, sans que cela se fasse au prix de leur coût ou de leur qualité.
Abstract.         This article is a position paper concerning Amazon Mechanical Turk-like systems, the use of
which has been steadily growing in natural language processing in the past few years. According to the mainstream
opinion expressed in the articles of the domain, these online working platforms allow to develop very quickly all
sorts of quality language resources, for a very low price, by people doing that as a hobby. We shall demonstrate
here that the situation is far from being that ideal, be it from the point of view of quality, price, workers’ status or
ethics. We shall then bring back to mind already existing or proposed alternatives. Our goal here is twofold : to
inform researchers, so that they can make their own choices with all the elements of the reflection in mind, and
propose practical and organizational solutions in order to improve new language resources development, while
limiting the risks of ethical and legal issues without letting go price or quality.
Mots-clés :          Amazon Mechanical Turk, ressources linguistiques.

Keywords:           Amazon Mechanical Turk, language resources.
1    Introduction
Le traitement des langues a grandement évolué au cours des ces vingt dernières années, tant dans le traitement
de l’écrit que de la parole. Stimulé par le paradigme de l’évaluation, le rôle des ressources linguistiques dans
ce développement a été et reste crucial : elles sont à la fois matière première, objet d’étude et ressource pour
l’évaluation de systèmes. Nous proposons ici une critiqued’un outil nouveau de constitution de ces ressources, le
microworking par le biais du crowdsourcing. Microworking fait référence au fait que le travail est segmenté en
petites tâches, crowdsourcing au fait que le travail est délocalisé (outsourced) et est effectué par un grand nombre
de personnes (crowd), payées ou non. Nous néologiserons crowdsourcing en « myriadisation » et microworking
en « travail parcellisé », et la conjonction des deux par « myriadisation du travail parcellisé ».
Nous aborderons en détails le cas d’un système de myriadisation du travail parcellisé (m.t.p. dans la suite) qui
a fait florès ces derniers temps, Amazon Mechanical Turk (MTurk), notamment pour sa capacité à produire des
corpus annotés à un coût très faible. Les auteurs de cet article ont contribué, à des degrés divers, à la mise en
place du paradigme de l’évaluation et au développement de nombreux outils et ressources dans le domaine du

B. S AGOT        K. F ORT       G. A DDA        J. M ARIANI        B. L ANG

traitement du langage. Nous sommes à ce titre conscients de l’importance du développement et de la diffusion
de celles-ci et du frein que représente leur coût, souvent rédhibitoire. Cependant, nous voulons mettre en avant
le fait que le coût du développement est un argument non fondé en ce qui concerne la m.t.p., tout d’abord parce
qu’il masque des problèmes économiques complexes, ensuite parce qu’il met sous le boisseau le problème de la
qualité des ressources ainsi obtenues, enfin parce qu’il omet la question de l’éthique et du droit du travail. Nous
aborderons ici l’ensemble de ces questions, sans pour autant remettre en cause l’utilité de la m.t.p., à condition
que son fonctionnement et son utilisation se fassent selon certains principes.
2     Que sont les systèmes de myriadisation ?

Le concept de myriadisation est venu de l’idée qu’un certain nombre de tâches pouvaient être effectuées par des
utilisateurs d’Internet, en utilisant les atouts propres à celui-ci, c’est-à-dire pouvoir accéder à un grand nombre de
personnes, de manière quasi-instantanée, partout dans le monde. La participation de ces internautes peut être bé-
névole ou rétribuée, suivant les tâches et les systèmes. Parmi les systèmes bénévoles, nous pouvons citer l’exemple
fameux de Wikipedia et, parmi ceux avec rétribution, RentACoder (où l’on peut soumettre un projet de program-
mation à une communauté de programmeurs) ou LiveOps (qui est un centre d’appels virtuel, les opérateurs étant
des internautes). A la suite de ces systèmes est apparu le concept de Human computing. Dans ce dernier cas, on
ne fait plus appel à des compétences particulières d’internautes, mais on utilise deux propriétés très élémentaires :
être un humain et avoir du temps libre. C’est l’application des grilles de calcul aux humains : chaque utilisateur, à
la manière d’un processeur, effectue une tâche élémentaire en n’ayant accès qu’à la seule information nécessaire
pour la mener à bien. Dans ce type de systèmes, seules des tâches très simples sont effectuées par les humains,
soit parce qu’elles sont intrinsèquement simples (par exemple, mettre une étiquette sur une image), soit parce
que la tâche est découpable en micro-tâches élémentaires. Ce sont les systèmes de myriadisation du travail par-
cellisé, qui sont le cœur de cet article. Dans ce concept, il y a souvent rétribution 1 , mais celle-ci peut-être non
monétaire, comme dans certains GWAP (Games with a purpose) (von Ahn, 2006; Chamberlain et al., 2008). La
création d’Amazon Mechanical Turk en 2005 s’inscrit dans cette dernière catégorie de systèmes de m.t.p. avec
rémunération, qui a été suivie par un grand nombre d’autres systèmes (Biewald, 2010), ceux-ci n’ayant pas acquis
la même importance, en particulier en raison du nombre de personnes inscrites. Comme souvent pour les nou-
veaux usages issus du Web, on ressent à la fois une fascination pour la potentialité des m.t.p. et une méfiance en
face de ces pratiques qui ne semblent pas avoir de réelles considérations pour le droit du travail. L’apparition des
systèmes de myriadisation pose de nombreux problèmes, légaux, éthiques et philosophiques, abordés par exemple
dans (Zittrain, 2008). Elle soulève d’importantes questions : qu’est-ce que le travail ? qu’est-ce qu’une rétribution
juste ? un être humain est-il assimilable à un ordinateur ? Ces questions essentielles débordent largement à la fois
le cadre d’un article de conférence et nos compétences. C’est pourquoi nous nous limiterons, autant que possible,
aux problèmes précis que pose l’introduction de MTurk comme moyen de produire des ressources linguistiques,
car nous jugeons cela à la fois urgent et crucial.
3     Amazon Mechanical Turk : légendes et réalité

Amazon Mechanical Turk (MTurk) permet, selon de nombreux auteurs dont le premier est Snow et al. (2008),
de produire à peu de frais et rapidement des ressources linguistiques de qualité. Cette découverte est d’une telle
importance pour la communauté qui manque toujours cruellement de moyens pour développer lesdites ressources,
qu’elle a entraîné un important effet de mode. Ce phénomène est, nous allons le voir, ni totalement justifié, ni sans
conséquences pour le développement futur de telles ressources. Par ailleurs, pour de nombreux chercheurs, les
Turkers 2 utilisent MTurk comme un hobby, il n’est donc pas scandaleux de très mal les rémunérer. Nous allons
voir ici que la situation est loin d’être aussi simplement idéale.
1. mais pas toujours, par exemple dans le système reCAPTCHA http://www.google.com/recaptcha/learnmore, où les
CAPTCHAs proviennent de mots mal reconnus lors de la numérisation de Google books
2. Il est d’usage d’appeler les personnes effectuant des tâches au sein du « turc mécanique » des Turkers, et celles qui fournissent les tâches
des Requesters.

M YRIADISATION DU TRAVAIL PARCELLISÉ

Nb Articles
6

COLING
LAW
3
EMNLP
2                                             MT Summit
Interspeech
2006 2007 2008 2009 2010   Année

F IGURE 1 – Évolution de l’utilisation réelle de MTurk dans les publications TAL et parole
3.1    Etat des lieux

Créé en 2005, le système de m.t.p. MTurk est aujourd’hui de plus en plus utilisé pour la création ou la validation
de ressources linguistiques pour le TAL (Traitement Automatique des Langues), et la plupart des conférences
internationales du domaine ont vu la présentation de projets de recherche utilisant MTurk.
La figure 1, reprise de (Fort et al., 2011), montre l’évolution rapide du phénomène. Elle comptabilise le nombre
de publications dans les principales conférences internationales décrivant des expériences utilisant MTurk.
Afin de compléter cette étude, nous avons également réalisé une recherche plus globale, cette fois dans l’an-
thologie de l’ACL. 3 Cette recherche, effectuée le 5 novembre 2010, a ramené 124 résultats, dont, après filtrage
manuel, 86 papiers utilisant effectivement MTurk (Fort et al., 2011). Ces résultats incluent un atelier spécialisé,
fournissant 35 des 86 publications, le NAACL-HLT 2010 Workshop on Amazon Mechanical Turk, dont l’exis-
tence même est le signe de l’importance grandissante de MTurk dans le domaine. Mentionnons enfin que certaines
expériences relatées dans des articles ont utilisé MTurk sans le mentionner explicitement (Fort et al., 2011). Ainsi,
Kevin B. Cohen, co-auteur de l’article précité, a remarqué qu’un article (Biadsy et al., 2008) dont il avait vu la
présentation en conférence, utilisait MTurk et n’en faisait pas mention. Côté francophone, nous ne trouvons aucun
article utilisant MTurk dans les actes des précédents TALN, ni dans les numéros publiés à ce jour de la revue TAL.
3.2    MTurk est un hobby pour les Turkers ?

Afin de pouvoir efficacement juger de l’éthique et de la légalité de MTurk, il est fondamental de pouvoir qualifier
l’activité que mènent les Turkers lorsqu’ils effectuent des tâches dans MTurk. S’agit-il d’une activité bénévole,
comme celle effectuée par les participants à Wikipedia ? Clairement non, lorsque l’on regarde la page d’accueil où
MTurk met directement l’accent sur l’argent gagné. Peut-elle être assimilée à un hobby, la rétribution étant alors
assimilable à un bonus ne correspondant pas à un salaire, comme cela est suggéré dans quelques articles (Novotney
& Callison-Burch, 2010) ?
Un certain nombre d’études (Ross et al., 2009, 2010; Ipeirotis, 2010b), fournissent, grâce à des questionnaires
soumis aux Turkers via MTurk, des chiffres déclarés sur un certain nombre de facteurs socio-économiques (pays,
âge, revenu, éducation. . .), sur la façon dont ils utilisent MTurk (nombre de tâches effectuées par semaine, revenu
acquis, date d’entrée dans MTurk . . .) 4 et dont ils qualifient leur activité. La motivation financière (déclarée)
est minoritaire chez les Turkers américains (38%), mais majoritaire chez les Turkers indiens (69%). Si 60% des
Turkers pensent que MTurk est un moyen utile de gagner de l’argent sur leur temps libre, ils ne sont que 30% à
motiver leur participation par l’intérêt des tâches, et 20% (5% des travailleurs indiens) disent l’utiliser pour tuer
le temps. Enfin, ils sont 20% (30% des Indiens) à dire que MTurk leur est nécessaire pour vivre, et à peu près le
même pourcentage à dire que MTurk constitue leur principale source de revenus.
3. Association for Computational Linguistics, http://www.aclweb.org/anthology/
4. On pourra se reporter par exemple à (Adda & Mariani, 2010) pour un résumé de celles-ci. On y apprend par exemple (Ross et al.,
2010) que les Turkers en provenance d’Inde représentaient 5% à la fin de 2008, 36% fin 2009, plus de 50% en mai 2010 selon http:
//blog.crowdflower.com/2010/05/amazon-mechanical-turk-survey/ et, selon (Biewald, 2010) sont responsables de
plus de 60% de l’activité dans MTurk.

B. S AGOT     K. F ORT    G. A DDA      J. M ARIANI     B. L ANG

Un autre moyen de vérifier la nature de l’activité des Turkers est d’examiner la nature de la tâche. Certaines
tâches actuellement proposées sur MTurk correspondent à de nouveaux usages (par exemple, des expériences
artistiques comme http://www.thesheepmarket.com/), mais d’autres sont effectuées habituellement par
des employés (ce qui peut donc assimiler MTurk à une forme de délocalisation sur le web, pour faire baisser les
coûts de production), et constituent donc un travail. Tel est le cas des activités de transcription ou de traduction,
qui sont (pour ce qui concerne les ressources les plus significatives) produites par des employés d’entités comme
le LDC ou ELDA. Pour les 20% des Turkers qui passent plus de 15h par semaine sur MTurk (Adda & Mariani,
2010) et contribuent à hauteur de 80% des tâches, la durée d’activité est significative, et est assimilable à un travail.
Nous ne pouvons pas conclure de manière définitive sur la nature de l’activité de tous les Turkers, car la nature
des tâches sur MTurk et la motivation des Turkers est composite. Cependant, nous pensons que pour les 20%
des Turkers pour qui MTurk constitue la source principale de revenus, ainsi que pour les tâches assimilables à un
travail (qui ont 8 chances sur 10 d’être effectuées par des Turkers travaillant plus de 15 heures par semaine), la
nature de l’activité est assimilable à un travail.
3.3     MTurk permet de réduire les coûts ?

Dans la plupart des articles ayant utilisé MTurk, le faible coût de développement de la ressource est mis en avant.
Il est vrai que MTurk permet de proposer des rétributions si faibles aux Turkers que le coût en est forcément
réduit, par exemple 0.005$ pour transcrire un segment d’environ 5 secondes de parole téléphonique (Novotney
& Callison-Burch, 2010). Il faut cependant nuancer ces chiffres. Tout d’abord, le coût effectif n’est pas toujours
calculé avec rigueur. En effet, le temps de développement de l’interface et de mise en place des garde-fous est
non nul (Callison-Burch & Dredze, 2010). De même, le coût de validation (Kaisser & Lowe, 2008) ou de dé-
veloppement (Xu & Klakow, 2010) post-MTurk permettant de compenser la mauvaise qualité des résultats (voir
section 3.4) n’est généralement pas précisément évalué. Or, ces coûts supplémentaires ne sont jamais pris en
compte dans le calcul final. De plus, certaines tâches peuvent se révéler plus coûteuses que prévues. Ainsi, si l’on
ne trouve pas de Turkers pour faire la tâche, on peut être obligé d’augmenter la rémunération, comme Novotney
& Callison-Burch (2010), qui, partant d’un coût très bas (5 dollars de l’heure transcrite), ont été obligés de le
multiplier par 7 (37 dollars de l’heure) pour transcrire du coréen, par manque de Turkers qualifiés.
3.4     MTurk permet de produire une qualité équivalente ?

3.4.1   Limitations liées à la non expertise

Les Turkers étant des non experts, le Requester (fournisseur de tâches) doit découper les tâches complexes en
tâches plus simples (HIT, Human Intelligence Task), afin de les rendre réalisables. Ce faisant, le chercheur est
amené à faire des choix qui peuvent biaiser les résultats. Un exemple de ce type de biais est analysé dans (Cook
& Stevenson, 2010), où les auteurs reconnaissent que le fait de ne proposer qu’une phrase par type d’évolution
lexicale (amélioration ou péjoration) influence le résultat.
Plus grave encore que ces biais potentiels, certains chercheurs ont observé que, lorsque la complexité de la tâche
augmente, la qualité produite sous MTurk est insuffisante. C’est notamment le cas dans (Bhardwaj et al., 2010), qui
démontre que, pour leur tâche de désambiguïsation lexicale, un petit nombre d’annotateurs bien formés produit de
bien meilleurs résultats qu’un grand nombre de Turkers (le nombre étant supposé contrebalancer la non expertise).
De ce point de vue, leurs résultats contredisent ceux de Snow et al. (2008) dont la tâche était semblable mais
beaucoup plus simple.Cette même difficulté d’obtenir une qualité suffisante sur des tâches complexes apparaît
dans (Gillick & Liu, 2010), qui démontre que l’évaluation par des non experts de systèmes de résumé automatique
est « risquée », les Turkers n’étant pas capables d’obtenir des résultats comparables à ceux des experts. On retrouve
ce problème de qualité dans de nombreux articles, dans lesquels les auteurs ont dû faire valider les résultats
des Turkers par des spécialistes (des étudiants en thèse pour (Kaisser & Lowe, 2008)) ou leur faire subir un
post-traitement assez lourd (Xu & Klakow, 2010). Enfin, la qualité du travail des annotateurs non experts varie
considérablement (Tratz & Hovy, 2010).
Il existe également un effet « boule de neige » qui tend à surestimer la qualité signalée dans les articles : des
chercheurs louent MTurk (Xu & Klakow, 2010), citant des recherches qui ont fait usage de MTurk, mais qui
n’auraient pas donné de résultats utilisables sans une intervention postérieure plus ou moins lourde (Kaisser &
Lowe, 2008). On pourrait en conclure que MTurk ne devrait être utilisé que pour des tâches simples, or, outre le

M YRIADISATION DU TRAVAIL PARCELLISÉ

fait que son fonctionnement même induit d’importantes limitations (voir section 3.4.2), il est intéressant de noter
que dans certains cas simples, des outils de TAL font d’ores et déjà mieux que les Turkers (Wais et al., 2010).
3.4.2    Limitations liées au fonctionnement même de MTurk

Une première limitation est l’interface de MTurk. Tratz & Hovy (2010) notent ainsi que les limites de l’interface
constituent « le premier et le plus important des défauts » de MTurk. Les auteurs regrettent par ailleurs l’impos-
sibilité d’avoir la certitude que les Turkers participant à la tâche sont bien de langue maternelle anglaise. Cette
impossibilité de connaître les capacités réelles des Turkers, notamment de connaître leur langue maternelle (bien
que leurs adresses IP soient géolocalisables), est un problème bien réel. S’il est possible de mettre en place des
tests préalables, qui, là encore, représentent un coût supplémentaire à prendre en compte, il est très facile de
tricher (Callison-Burch & Dredze, 2010). Bien entendu, il est toujours possible de mettre en place des garde-
fous (Callison-Burch & Dredze, 2010; Welinder et al., 2010), mais, encore une fois, cela demande du temps
et représente donc un coût supplémentaire que peu de Requesters sont prêts à investir. Ainsi, dans (Xu & Kla-
kow, 2010), les auteurs ont identifié des spammeurs mais n’ont pas réussi à les éliminer. Pour certaines tâches, il
peut s’avérer difficile de trouver des Turkers ayant les compétences nécessaires en raison de la complexité de la
tâche (Gillick & Liu, 2010; Lambert et al., 2010), ou de la langue à maîtriser (Novotney & Callison-Burch, 2010).
Par ailleurs, il ne faut pas négliger l’impact du paiement à la tâche, qui induit comme comportement logique de
placer le nombre de tâches réalisées au-dessus de la qualité de la réalisation, et ce, quelle que soit la rétribution.
Kochhar et al. (2010) sont ainsi arrivés à la conclusion qu’il valait mieux payer à l’heure (avec, bien sûr, des
procédures de vérification et de justification du temps passé).
4       Quelques réflexions sur le statut de MTurk

4.1     Quel est le statut de l’activité dans MTurk ?

En obscurcissant la relation entre Turkers et Requesters, et entre les Turkers eux-mêmes, MTurk empêche de fait
la possibilité de s’organiser en syndicats, de protester contre d’éventuelles pratiques douteuses des Requesters ou
d’ester en justice. Au-delà des problèmes de droit du travail, il faut parler des problèmes des taxes et cotisations
sociales : Amazon considère (selon l’accord de licence de MTurk) que les Turkers sont assimilables à des tra-
vailleurs indépendants,et donc qu’il leur incombe de payer toutes les taxes et charges afférant à leur activité. Étant
donné la hauteur des rémunérations prises individuellement, il est parfaitement hypocrite de penser que cela est
possible. Il est donc fortement probable que les Turkers ne déclarent pas ces revenus et ne cotisent pas non plus à
une quelconque caisse de retraite ou de sécurité sociale. Il en va bien entendu de même pour les fournisseurs de
travail. Les états sont donc privés d’un revenu légitime.
Il faut souligner également que la nature de la relation entre les trois partenaires, Turker, Requester et MTurk,
vague pour le droit américain, est encore plus douteuse en regard du droit français. En effet, selon la législation
française du travail, en dehors du fait que le travail à la tâche est illégal, soit il s’agit de travail salarié, qui serait, en
l’occurrence, non déclaré par l’employeur, donc illégal (article 8200 et suivants du Code du Travail), soit il s’agit
d’un rapport de prestation de service, dont le donneur d’ordre serait MTurk et le prestataire le Turker et, dans ce
cas, le Turker doit être enregistré au registre du commerce (article 8222-1 du Code du Travail).
4.2     Le modèle économique de MTurk est-il fondé ?

Comme souligné dans la partie 2, lorsque l’on aborde pour la première fois MTurk, on est sidéré devant les condi-
tions financières imposées aux Turkers, qui amènent à des rémunérations horaires ridiculement basses (inférieures
à 2 dollars, soit 1,46 euros (Ross et al., 2009; Ipeirotis, 2010b)). Ce coût fabuleusement bas correspond-il à une
réalité économique saine (comme suggéré par Marge et al. (2010) et McGraw et al. (2010), qui considèrent que
cette rétribution n’est qu’une sorte de bonus pour une tâche par ailleurs effectuée avec plaisir) ?
Nous l’avons vu dans la partie 3.2, l’assertion que les Turkers considèrent MTurk comme un hobby est fausse, au
moins pour une partie significative d’entre eux. Dès lors, pourquoi, si cela constitue pour eux un travail, acceptent-
ils un salaire horaire aussi bas ? La loi de l’offre et la demande n’est pas suffisante pour l’expliquer, tout d’abord

B. S AGOT       K. F ORT       G. A DDA        J. M ARIANI        B. L ANG

parce que le nombre réel de Turkers n’est pas si important (Fort et al., 2011), ensuite, parce qu’il est souvent
difficile de faire exécuter des tâches de grande taille en un temps limité pour un coût standard (Ipeirotis, 2010a).
Un fait peut nous mettre sur la voie d’une explication crédible : beaucoup d’articles (par exemple (Marge et al.,
2010)) soulignent que la qualité n’est pas liée au coût associé à chaque tâche. Cela est dû en particulier à la
présence de spammeurs (c’est-à-dire de Turkers qui répondent au hasard ou en utilisant un système automatique),
attirés par les tâches bien rémunérées, et qui sont en grand nombre dans le système MTurk, le système de réputation
mis en place par Amazon étant notoirement facile à mettre en défaut 5 . Cela conduit à une situation semblable
au « marché des tacots », décrit par le prix Nobel Georges Akerlof (Akerlof, 1970) : l’acheteur d’une voiture
d’occasion prend en compte dans le prix qu’il offre le risque que le vendeur lui « fourgue » un tacot. Les vendeurs
propriétaires d’une bonne voiture ne peuvent donc obtenir un bon prix et quittent le système, ce qui accroît en
retour la défiance de l’acheteur, car cela augmente le risque d’acheter un tacot. La présence des spammeurs, de par
le laxisme du système mis en œuvre par Amazon, conduit à une stabilisation à un prix très bas, les bons travailleurs
quittant donc le système (70% des Turkers utilisent MTurk depuis moins de 6 mois (Ross et al., 2009)).
De plus, il est indéniable qu’un certain nombre de Turkers utilisent MTurk comme moyen de divertissement :
ceux-ci sont attirés par les tâches intéressantes, quelle que soit leur rémunération. Sur ces tâches, ils sont en
concurrence avec les Turkers-travailleurs (qui, naturellement, souhaitent également faire des tâches intéressantes),
ce qui conduit également à faire baisser le taux horaire moyen « acceptable », c’est-à-dire le taux horaire seuil, en
dessous duquel un travailleur n’acceptera pas d’effectuer la tâche.
Dernier facteur qui tend à faire accepter un taux horaire finalement inacceptable : le travail à la tâche. Un Turker,
fait bien sûr une relation entre la difficulté de la tâche et la rétribution, mais n’a pas une idée claire du salaire
horaire avant de commencer à travailler. De plus, le travail à la tâche induit un comportement que l’on peut voir
également dans des jeux en ligne ou à chaque fois que l’on effectue une tâche contre une rétribution absolue :
la personne a tendance à regarder grossir son compteur d’argent, ou de points, et à se fixer des objectifs absolus,
déconnectés d’un quelconque taux horaire : « aujourd’hui, je reste dans le système, jusqu’à ce que j’ai gagné 5
dollars ». Ce qui n’est bien sûr pas le meilleur moyen d’optimiser le taux horaire. Si le travail à la tâche est interdit
en France, c’est bien pour empêcher que des travailleurs gagnent moins que le salaire minimum.
Comme le souligne (Ipeirotis, 2010c), les défauts de la plateforme MTurk remettent en cause sa viabilité à moyen
terme, si elle n’évolue pas fondamentalement, en particulier sur les problèmes de rémunération et de systèmes de
réputation fiables pour les Turkers et les Requesters.
4.3     Quelle est la situation par rapport à la propriété intellectuelle sur MTurk ?

Au regard du droit européen, la problématique de la propriété intellectuelle pour des données telles que l’on peut
en produire via MTurk se pose le plus souvent en termes de protection associée aux bases de données, au sens
de la directive européenne du 11 mars 1996. Cette directive, qui concerne les ensembles d’informations de toutes
natures, offre une double protection : (1) par le droit d’auteur concernant la structure de la base, conditionné au
fait qu’il y ait là une création originale, et (2) par un droit spécifique couvrant le contenu, droit proche de celui
du droit d’auteur mais conditionné à la valeur économique des données (et non à leur originalité), au sens où ces
données doivent avoir été obtenues grâce à un investissement substantiel du point de vue qualitatif ou quantitatif.
Cette deuxième protection est indépendante du caractère public ou non des données, l’objet de la protection étant
la base dans son ensemble (c’est-à-dire l’assemblage des données).
Dans le cas de MTurk, les droits semblent devoir être la propriété du Requester, soit en tant qu’auteur (pour les
HIT eux-mêmes et ce qu’ils pourraient contenir, sauf lorsque sont utilisées des données elles-mêmes soumises à
des droits d’auteurs propres, comme, par exemple, si l’on fait transcrire ou traduire des contenus existants), soit
en tant qu’organisateur de ce qui est une œuvre collective 6 (pour les productions des Turkers).
Cela dit, il n’est pas clair, avec MTurk relevant des États-Unis et des Requesters et des Turkers relevant souvent
d’autres pays, qu’il y ait un droit applicable, la situation ne semblant pas envisagée dans les traités internationaux.

5. http ://behind-the-enemy-lines.blogspot.com/2010/10/be-top-mechanical-turk-worker-you-need.html
6. L’œuvre collective est définie par l’article L. 113-2 alinéa 3 du Code de la propriété intellectuelle comme étant une œuvre créée
sur l’initiative d’une personne physique ou morale qui l’édite, la publie et la divulgue sous sa direction et son nom et dans laquelle la
contribution personnelle des divers auteurs participant à son élaboration se fond dans l’ensemble en vue duquel elle est conçue, sans qu’il
soit possible d’attribuer à chacun d’eux un droit distinct sur l’ensemble réalisé. L’article 113-5 stipule alors que L’œuvre collective est, sauf
preuve contraire, la propriété de la personne physique ou morale sous le nom de laquelle elle est divulguée. [. . . ] .

M YRIADISATION DU TRAVAIL PARCELLISÉ

5     Alternatives existantes ou proposées

Comme indiqué ci-dessus, les objectifs principaux des développeurs de ressources linguistiques faisant appel
à MTurk sont l’obtention de résultats de bonne qualité, à un faible coût et dans un délai très bref. Mais ces
objectifs ne sont pas nécessairement faciles à atteindre avec MTurk, alors que des approches alternatives existent.
Tout d’abord, bien qu’une comparaison systématique entre MTurk et des algorithmes état-de-l’art impliquant un
échantillon varié de tâches liées au TAL reste à faire, il semble que certains auteurs aboutissent à la conclusion que
les annotateurs automatiques déjà disponibles pour certaines tâches font aussi bien voire mieux que les Turkers
(Wais et al., 2010) : les outils automatiques peuvent faire mieux que les non experts. La réutilisation intelligente
de ressources existantes peut également être une alternative simple et peu coûteuse à MTurk. Enfin, MTurk n’est
qu’une des nombreuses possibilités de m.t.p.
5.1    Approches non supervisées et semi-supervisées pour le développement de ressources
linguistiques à faible coût

La communauté du TAL s’intéresse depuis longtemps à des approches dites non supervisées d’apprentissage
automatique, pour un large éventail de tâches parfois complexes. De la segmentation en mots à l’analyse syn-
taxique (Hänig, 2010) en passant par l’annotation morphosyntaxique (Goldwater & Griffiths, 2007), le dévelop-
pement de ressources lexicales (y compris de niveau sémantique ou pragmatique, cf. (Pak & Paroubek, 2010)) ou la
catégorisation de documents, nombreuses sont les tâches pour lesquelles des techniques existent qui ne nécessitent
aucune ressource préalable. Bien que les résultats obtenus soient souvent inférieurs aux résultats des approches
supervisées (utilisant un corpus d’apprentissage) ou symboliques avancées (utilisant des ressources symboliques
également coûteuses à développer), on peut penser que, pour certaines tâches, ils ne sont pas inférieurs à ce que
l’on peut attendre de MTurk. C’est notamment le cas pour des tâches complexes comme l’analyse syntaxique.
Pour améliorer à faible coût la qualité des outils statistiques ainsi développés et/ou pour les faire correspondre
à des modèles préexistants (par exemple, à un inventaire préétabli de catégories dans le cadre de l’annotation
morphosyntaxique), il n’est pas forcément nécessaire de recourir à des techniques totalement supervisées. Une
utilisation optimale d’un ensemble limité d’informations (annotations, ressources externes) peut donner de bons
résultats : c’est le paradigme de l’apprentissage semi-supervisé (Abney, 2007). Dans le cas du développement de
ressources linguistiques, on peut identifier deux types (non mutuellement exclusifs) de semi-supervision.
La première idée que l’on peut avoir est d’entraîner des modèles sur les quelques données annotées, puis d’an-
noter automatiquement les autres données : on peut alors choisir parmi les données annotées automatiquement
celles pour lesquelles le modèle a un niveau de confiance optimal, et les considérer comme de nouvelles données
annotées pour l’apprentissage d’un nouveau modèle, et ainsi de suite : c’est le self-training, utilisé en TAL depuis
longtemps (Yarowsky, 1995). Cette idée peut être généralisée en utilisant deux modèles les plus différents pos-
sible, et à compléter les données d’apprentissage de l’un par les annotations automatiques les plus sûres produites
par l’autre. C’est le co-training (Blum & Mitchell, 1998), qui cherche à éliminer au maximum les biais spéci-
fiques à chaque modèle par la confrontation à un autre. À ce stade, on reste dans une situation où une annotation
manuelle peu coûteuse sert de graine pour la construction successive, mais automatisée, de modèles qui vont en
s’améliorant, jusqu’à obtenir des performances satisfaisantes. Si l’on accepte de continuer à annoter des données
manuellement au cours des étapes de construction successive de modèles, on peut faire en sorte que soient choi-
sies et présentées aux annotateurs les données telles que disposer d’une annotation de référence pour elles soit de
nature à améliorer au mieux la qualité des outils. C’est l’idée au cœur de l’active learning (Cohn et al., 1995).
La deuxième idée, combinable avec la première, consiste à utiliser au mieux des données annotées d’une fa-
çon moins complète que l’annotation visée. Par exemple, pour l’annotation morphosyntaxique, on peut disposer
d’un lexique externe mais pas d’un corpus d’apprentissage : projeter le lexique sur le corpus correspond alors
à une annotation ambiguë, qu’il faut désambiguïser (Smith & Eisner, 2005). Pour le développement de lexiques
morphologiques, disposer d’une description formalisée de la morphologie de la langue permet l’utilisation de
techniques efficaces de suggestion d’entrées lexicales (Sagot, 2005). De même, on peut chercher à exploiter un
corpus partiellement parenthésé pour guider des modèles d’analyse syntaxique complets (Watson et al., 2007).

B. S AGOT        K. F ORT      G. A DDA       J. M ARIANI        B. L ANG

5.2     Réutilisation de ressources existantes

Moins coûteux encore, la construction de ressources linguistiques peut se faire en réutilisant des ressources exis-
tantes. Considérons par exemple la tâche de détection d’entités nommées. Nothman et al. (2008) montrent qu’il
est possible de transformer Wikipedia en une ressource annotée en entités nommées de large couverture et de
très bonne qualité. De tels corpus ont pourtant été construits au moyen de MTurk, notamment sur des corpus non
standard, en particulier médicaux (Yetisgen-Yildiz et al., 2010), twitter (Finin et al., 2010), e-mails (Lawson et al.,
2010). Naturellement, ces corpus sont très différents de ce que l’on peut obtenir au moyen de Wikipedia. Mais la
taille des données extraites, ainsi que les caractéristiques de Wikipedia en tant que corpus, font que les détecteurs
d’entités nommées entraînés sur un corpus construit à partir de Wikipedia tendent à avoir de très bons résultats
lorsqu’ils sont utilisés sur d’autres types de corpus (Balasuriya et al., 2009).
Il ne s’agit là que d’un exemple, mais nombreuses sont les ressources susceptibles de fournir des données de
toutes natures : Wikipedia 7 et autres projets wiki, notamment wiktionary, corpus (annotés ou non, oraux ou tex-
tuels) et ressources lexicales (phonétiques, morphologiques, syntaxiques, sémantiques), pour peu qu’elles soient
disponibles pour la communauté. Il s’agit ici d’un autre débat, sur lequel nous n’insisterons donc pas plus avant.
5.3     Développement collaboratif ou myriadisé de ressources linguistiques au-delà de MTurk

Toutes les méthodes alternatives décrites jusqu’à présent ont prouvé leur utilité et leur efficacité, mais elles re-
quièrent des compétences expertes, ne serait-ce que pour concevoir et développer les outils automatiques, mais
également pour effectuer, si besoin est, les tâches d’annotation optimisées. Il existe des méthodes de développe-
ment de ressources linguistiques qui ne font pas nécessairement appel à des experts, sans être pour autant touchées
par tous les problèmes décrits pour MTurk. Il s’agit en particulier des approches collaboratives, des approches lu-
diques mais également de certaines plateformes de m.t.p., qui se sont données les moyens d’en éviter les écueils.
Les approches collaboratives de développement de ressources lexicales reposent sur la stratégie mise en place
par le projet Wikipedia, les autres projets de la constellation Wikimedia, et d’autres types de wiki comme les
Semantic Wiki (Freebase, OntoWiki. . .). Différents participants volontaires, experts ou non, enrichissent progres-
sivement une même ressource, soit sous forme d’annotations soit sous forme de bases de données (lexicales,
ontologiques. . .). C’est une première étape vers la m.t.p. : ici, le travail n’est pas parcellisé, et il n’est que fai-
blement myriadisé. Les annotations des uns sont « contrôlées » par les autres, et des divergences de vues entre
différents participants se manifestent le plus souvent par des discussions, conduisant éventuellement à ce que
l’administrateur tranche et décide. C’est ainsi qu’un très haut niveau de qualité peut être finalement atteint. Une
des premières plateformes wiki dédiée au développement d’une ressource TAL est l’outil Serengeti, développé
à l’Université de Bielefeld (Stürenberg et al., 2007), à des fins d’annotation sémantique des textes. Cet outil est
utilisé actuellement dans le cadre du projet AnaWiki (http://www.anawiki.org).
Toutefois, ces approches restent plus adaptées pour le développement de ressources de taille raisonnable avec une
bonne qualité (gold standard). Elles sont moins indiquées pour le développement rapide de ressources à grande
échelle. Une autre stratégie, qui repose également sur le Web, est d’attirer de grand nombres de non experts au
moyen de jeux en ligne dits ayant un but (en anglais games with a purpose, ou GWAP). Cette idée, initiée par (von
Ahn, 2006; von Ahn & Dabbish, 2008) avec le jeu en ligne ESP (http://www.espgame.org/) consiste à
faire étiqueter des images par des joueurs qui rentrent en compétition : ceux-ci reçoivent des crédits lorsque leurs
réponses coïncident avec celles d’autres joueurs 8 . ESP a connu un succès important en mobilisant 13 500 utilisa-
teurs, créant 1, 3 million d’étiquettes dans les premiers mois suivant son apparition sur la Toile. Cette idée a été par
la suite déclinée pour divers types de tâches, y compris en TAL. Des exemples en sont le jeu JeuxDeMots (Lafour-
cade & Joubert, 2008, http://www.lirmm.fr/jeuxdemots), qui vise à collecter des relations entre mots,
et son alter ego PtiClic (ibid., http://www.lirmm.fr/pticlic), qui vise à typer explicitement ces rela-
tions. Le jeu PhraseDetective (Chamberlain et al., 2008, http://www.phrasedetectives.org), quant
à lui, a pour objectif l’annotation de liens anaphoriques, tâche pourtant réputée complexe. L’idée est alors que
l’on peut aussi utiliser le jeu pour former les utilisateurs à la tâche. Phrase Detective comprend ainsi une phase
d’entraînement où l’on apprend la tâche au nouveau joueur, par le biais de tests de plus en plus durs basés sur un
petit ensemble de données venant de corpus existants (annotés par des experts).

7. Il faut notamment citer le projet DBpedia (http://dbpedia.org), qui cherche à extraire des informations ontologiques structurées
à partir de Wikipedia, constituant ainsi une ressource aux potentialités multiples et déjà largement utilisée.
8. Différentes procédures sont prévues pour exclure les utilisateurs malveillants, notamment le contrôle des adresses IP, la vérification
aléatoires des étiquetages pour des réponses connues, etc. (von Ahn & Dabbish, 2008)

M YRIADISATION DU TRAVAIL PARCELLISÉ

Toutefois, la frontière entre jeu de type GWAP et m.t.p. à la MTurk n’est pas nette. On ne peut pas distinguer
facilement les GWAP, qui seraient plus ludiques, et MTurk, qui serait stricto sensu du travail : même contribuer à
Wikipedia est un travail, certes bénévole. On ne peut pas non plus distinguer les GWAP de MTurk en tant qu’ils
ne donneraient lieu à aucune récompense tangible : certains jeux en ligne sont des GWAP mais proposent des
rémunérations non-monétaires (ainsi, PhraseDetective permet de gagner des bons à dépenser sur le site d’achat
en ligne Amazon). Enfin, on ne peut pas distinguer la m.t.p. à la MTurk par le caractère « éthique » des premiers.
Il existe en effet des alternatives à MTurk pour développer des ressources linguistiques dans le paradigme de la
m.t.p., tout en évitant les écueils évoqués tout au long de cet article.
Pour le recueil de données langagières, en particulier pour les langues peu dotées, des alternatives à MTurk
semblent être plus appropriées. Ainsi, l’utilisation d’applications sur des téléphones portables de nouvelle gé-
nération est un moyen plus efficace de pouvoir accéder à toute une population. Hughes et al. (2010) ont ainsi
embauché des locuteurs locaux et leur ont prêté des téléphones sur lesquels tournait une application dédiée. Les
auteurs ont ainsi recueilli 3 000 heures en 17 langues. Un exemple de m.t.p. éthique est Samasource, une ONG qui
utilise ce type de méthode pour faire effectuer des tâches 9 à des personnes réellement nécessiteuses formées et ré-
munérées équitablement selon des barèmes dépendant du pays. Il s’agit là d’une alternative éthique à l’utilisation
de MTurk qui permet également de tirer parti des avantages de la m.t.p.
5.4     Optimiser le coût de l’annotation manuelle : pré-annotation et interfaces dédiées

Indépendamment de la façon dont on s’en sert, l’annotation manuelle par des experts peut être considérable-
ment accélérée voire améliorée au moyen d’outils d’annotation automatique utilisés comme pré-annotateurs. Par
exemple, Fort & Sagot (2010) ont démontré que, dans le cas de l’étiquetage morphosyntaxique, une préannotation,
même de piètre qualité et donc développable à faible coût, permet d’améliorer très largement le temps et la qualité
des annotations manuelles. Ainsi, les auteurs ont montré que 50 phrases annotées à la main sans pré-annotation,
ce qui prend environ 40 minutes 10 , permettent de construire un préannotateur tel que la vitesse de l’annotation
manuelle par un expert est quasiment identique à ce que l’on obtient avec un préannotateur de niveau état-de-
l’art, c’est-à-dire que l’on peut construire un corpus complet de taille standard (10 000 phrases) en environ 6 000
minutes (100 heures). Des annotateurs experts et coûteux, pour peu que leur travail soit préparé puis utilisé de
façon optimale, permettent donc le développement de ressources de très bonne qualité à un coût qui reste limité. À
l’inverse, sur cette tâche d’apparence simple, des Turkers seraient bien en peine de suivre correctement un guide
d’annotation détaillé, nécessairement complexe s’il est linguistiquement sérieux.
Par ailleurs, les remarques de Tratz & Hovy (2010) mentionnées ci-dessus concernant les limitations des interfaces
déployables dans MTurk s’appliquent de manière générale. L’expérience acquise, par exemple, dans le dévelop-
pement de corpus annotés syntaxiquement ou sémantiquement montre que la rapidité et la qualité l’annotation, de
quelque nature qu’elle soit, est fortement influencée par l’interface d’annotation elle-même (cf. par exemple (Erk
et al., 2003)). Il y a donc là aussi matière à accélérer et améliorer toute étape d’annotation manuelle, au point
qu’une interface adaptée à une tâche donnée pourrait permettre de réduire les coûts dans des proportions compa-
rables à celles obtenues par l’utilisation de MTurk, sans en présenter les inconvénients.
6     Conclusion et perspectives
MTurk illustre la complexité et la difficulté d’appréhender les relations (commerciales, de travail et autres) dans
les nouveaux modes d’activités sur Internet. Les chercheurs qui ont utilisé MTurk l’ont fait souvent de bonne foi,
par manque de moyens financiers, pour produire plus de données et les redistribuer à la communauté. Pour ceux
qui ont eu des doutes sur de possibles problèmes d’éthique et de droit du travail, une recherche superficielle les a
convaincus que MTurk est une sorte d’avatar de Wikipedia, et que les Turkers sont motivés surtout par le plaisir
d’effectuer des tâches amusantes.
Nous pensons avoir montré que MTurk n’est pas une panacée et que d’autres solutions existent aujourd’hui pour
réduire les coûts de construction de ressources linguistiques de qualité, tout en respectant ceux qui travaillent sur
ces ressources et en tirant un meilleur parti de leurs compétences. Car derrière le débat autour de MTurk se trouve
9. Comme traduire des SMS en créole, lors du tremblement de terre à Haïti afin de permettre aux secours internationaux d’aller à leur
secours, en liaison avec le site CrowdFlower. http://www.samasource.org/haiti/.
10. Les estimations proposées dans ce paragraphe, très grossières, reposent sur celles de (Fort & Sagot, 2010).

B. S AGOT     K. F ORT    G. A DDA     J. M ARIANI     B. L ANG

finalement la question de la considération due aux annotateurs, aux traducteurs, aux spécialistes de la transcription.
Nous aimerions, en conclusion, aller au-delà des faits actuels et mettre l’accent sur les conséquences à plus ou
moins long terme de cette « mode ». En effet, sous la pression de ce type de systèmes à bas coût, les agences de
moyens pourraient bientôt être plus réticentes à financer des projets de développement de ressources linguistiques
à des coûts « normaux » (ou plutôt réalistes). Le coût à la MTurk deviendrait alors une norme de fait et nous
n’aurions plus le choix de nos méthodes de développement.
Nous avons vu, dans la partie 5.3, qu’un système de m.t.p. peut permettre de faire produire des tâches rémunérées
en préservant l’éthique, cela peut même être une chance pour des personnes qui ne peuvent se trouver sur le
marché du travail, de par leur isolement, leur handicap, etc ; mais cela nécessite un encadrement légal strict afin de
s’assurer que ce système n’est pas une remise en cause des droits des travailleurs. On peut penser à moyen terme
au développement d’une plateforme m.t.p. opérée par les acteurs de recherche au niveau européen, et un guide des
bonnes pratiques concernant l’utilisation des m.t.p., comme cela se fait dans d’autres secteurs de recherches, par
exemple en sciences sociales. Mais ces solutions risquent de ne pas freiner le développement actuel de l’utilisation
de MTurk, au nom du “pragmatisme” et de la concurrence avec les équipes (par exemple) outre-Atlantique ; c’est
pourquoi nous proposons la création d’un label de qualité et d’éthique, qui pourrait être décerné aux ressources
par les associations savantes concernées, l’ATALA 11 pour le TAL et l’AFCP 12 pour la parole. Les questions
d’éthique sont dès à présent un critère de sélection pour les projets européens, ce label permettrait de préciser le
statut des ressources comme critère de sélection pour l’ensemble des agences de moyens, tout en valorisant les
bonnes pratiques de développement.
Remerciements
Ce travail a été réalisé en partie dans le cadre du programme Quaero, financé par OSEO, agence nationale de
valorisation de la recherche, et dans celui du projet ANR EDyLex (ANR-09-CORD-008).
Références
A BNEY S. (2007). Semisupervised Learning for Computational Linguistics. Chapman & Hall/CRC, 1ère edition.
A DDA G. & M ARIANI J. (2010). Language resources and amazon mechanical turk : legal, ethical and other
issues. In LISLR2010, “Legal Issues for Sharing Language Resources workshop”, LREC2010.
A KERLOF G. A. (1970). The market for ’lemons’ : Quality uncertainty and the market mechanism. Quarterly
Journal of Economics, 84(3), 488–500.
BALASURIYA D., R INGLAND N., N OTHMAN J., M URPHY T. & C URRAN J. R. (2009). Named entity recog-
nition in wikipedia. In People’s Web ’09 : Proceedings of the 2009 Workshop on The People’s Web Meets NLP,
p. 10–18, Morristown, NJ, USA : Association for Computational Linguistics.
B HARDWAJ V., PASSONNEAU R., S ALLEB -AOUISSI A. & I DE N. (2010). Anveshan : A tool for analysis of
multiple annotators’ labeling behavior. In Proceedings of The fourth linguistic annotation workshop (LAW IV),
Uppsala, Suède.
B IADSY F., H IRSCHBERG J. & F ILATOVA E. (2008). An unsupervised approach to biography production using
Wikipedia. In Proceedings of ACL 2008, p. 807–815 : Association for Computational Linguistics.
B IEWALD L. (2010). Better crowdsourcing through automated methods for quality control. SIGIR 2010 Work-
shop on Crowdsourcing for Search Evaluation.
B LUM A. & M ITCHELL T. (1998). Combining labeled and unlabeled data with co-training. In COLT : Procee-
dings of the Workshop on Computational Learning Theory : Morgan Kaufmann Publishers.
C ALLISON -B URCH C. & D REDZE M. (2010). Creating speech and language data with amazon’s mechanical
turk. In CSLDAMT ’10 : Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, Morristown, NJ, USA : Association for Computational Linguistics.
C HAMBERLAIN J., P OESIO M. & K RUSCHWITZ U. (2008). Phrase Detectives : a Web-based Collaborative
Annotation Game. In Proceedings of the International Conference on Semantic Systems (I-Semantics’08), Graz.
11. http://www.atala.org/
12. http://www.afcp-parole.org/

M YRIADISATION DU TRAVAIL PARCELLISÉ

C OHN D. A., G HAHRAMANI Z. & J ORDAN M. I. (1995). Active learning with statistical models. In G.
T ESAURO , D. T OURETZKY & T. L EEN, Eds., Advances in Neural Information Processing Systems, volume 7,
p. 705–712 : The MIT Press.
C OOK P. & S TEVENSON S. (2010). Automatically identifying changes in the semantic orientation of words.
In N. C. C. C HAIR ), K. C HOUKRI , B. M AEGAARD , J. M ARIANI , J. O DIJK , S. P IPERIDIS , M. ROSNER &
D. TAPIAS, Eds., Proceedings of the Seventh conference on International Language Resources and Evaluation
(LREC’10) : European Language Resources Association (ELRA).
E RK K., KOWALSKI A. & PADO S. (2003). The salsa annotation tool. In D. D UCHIER & G.-J. M. K RUIJFF,
Eds., Proceedings of the Workshop on Prospects and Advances in the Syntax/Semantics Interface, Nancy, France.
F ININ T., M URNANE W., K ARANDIKAR A., K ELLER N., M ARTINEAU J. & D REDZE M. (2010). Annotating
named entities in twitter data with crowdsourcing. In Proceedings of the NAACL HLT 2010 Workshop on Crea-
ting Speech and Language Data with Amazon’s Mechanical Turk, CSLDAMT ’10, p. 80–88, Stroudsburg, PA,
USA : Association for Computational Linguistics.
F ORT K., A DDA G. & C OHEN K. B. (2011). Amazon mechanical turk : Gold mine or coal mine ? Computatio-
nal Linguistics (editorial), 37(2).
F ORT K. & S AGOT B. (2010). Influence of Pre-annotation on POS-tagged Corpus Development. In Proc. of the
Fourth ACL Linguistic Annotation Workshop, Uppsala, Suède.
G ILLICK D. & L IU Y. (2010). Non-expert evaluation of summarization systems is risky. In Proceedings of
the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,
CSLDAMT ’10, p. 148–151, Stroudsburg, PA, USA : Association for Computational Linguistics.
G OLDWATER S. & G RIFFITHS T. (2007). A fully bayesian approach to unsupervised part-of-speech tagging. In
Proceedings of ACL, Prague, République tchèque.
H ÄNIG C. (2010). Improvements in unsupervised co-occurrence based parsing. In Proceedings of the Four-
teenth Conference on Computational Natural Language Learning, CoNLL ’10, p. 1–8, Stroudsburg, PA, USA :
Association for Computational Linguistics.
H UGHES T., NAKAJIMA K., H A L., VASU A., M ORENO P. & L E B EAU M. (2010). Building transcribed speech
corpora quickly and cheaply for many languages. In Proceedings of Interspeech, p. 1914–1917.
I PEIROTIS P. (2010a). Analyzing the amazon mechanical turk marketplace.            CeDER Working Papers,
http ://hdl.handle.net/2451/29801. CeDER-10-04.
I PEIROTIS P. (2010b).            Demographics of      mechanical    turk.        CeDER     Working    Papers,
http ://hdl.handle.net/2451/29585. CeDER-10-01.
I PEIROTIS P. (2010c).      A plea to amazon : Fix mechanical turk !                http ://behind-the-enemy-
lines.blogspot.com/2010/10/plea-to-amazon-fix-mechanical-turk.html.
K AISSER M. & L OWE J. B. (2008). Creating a research collection of question answer sentence pairs with
amazon’s mechanical turk. In Proceedings of the International Language Resources and Evaluation (LREC-
2008).
KOCHHAR S., M AZZOCCHI S. & PARITOSH P. (2010). The anatomy of a large-scale human computation
engine. In Proceedings of Human Computation Workshop at the 16th ACM SIKDD Conference on Knowledge
Discovery and Data Mining, KDD 2010, Washington D.C.
L AFOURCADE M. & J OUBERT A. (2008). JeuxDeMots : un prototype ludique pour l’émergence de relations
entre termes. In JADT’08 : Journées internationales d’Analyse statistiques des Données Textuelles, p. 657–666.
L AMBERT B., S INGH R. & R AJ B. (2010). Creating a linguistic plausibility dataset with non-expert annotators.
In Proceedings of Interspeech, p. 1906–1909.
L AWSON N., E USTICE K., P ERKOWITZ M. & Y ETISGEN -Y ILDIZ M. (2010). Annotating large email datasets
for named entity recognition with mechanical turk. In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon’s Mechanical Turk, CSLDAMT ’10, p. 71–79, Stroudsburg,
PA, USA : Association for Computational Linguistics.
M ARGE M., BANERJEE S. & RUDNICKY A. I. (2010). Using the amazon mechanical turk for transcription of
spoken language. In IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), p.
5270–5273, Dallas, TX.
M C G RAW I., YING L EE C., H ETHERINGTON L., S ENEFF S. & G LASS J. (2010). Collecting voices from the
cloud. In Proceedings of the International Language Resources and Evaluation (LREC-2010), p. 1576–1583.

B. S AGOT    K. F ORT   G. A DDA    J. M ARIANI    B. L ANG

N OTHMAN J., C URRAN J. R. & M URPHY T. (2008). Transforming Wikipedia into Named Entity Training
Data. In Proceedings of the Australian Language Technology Workshop, p. 124–132.
N OVOTNEY S. & C ALLISON -B URCH C. (2010). Cheap, fast and good enough : automatic speech recognition
with non-expert transcription. In Human Language Technologies : The 2010 Annual Conference of the North
American Chapter of the Association for Computational Linguistics, HLT ’10, p. 207–215, Stroudsburg, PA,
USA : Association for Computational Linguistics.
PAK A. & PAROUBEK P. (2010). Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings
of the Seventh conference on International Language Resources and Evaluation (LREC’10), La Valette, Malte :
European Language Resources Association (ELRA).
ROSS J., I RANI L., S ILBERMAN M. S., Z ALDIVAR A. & T OMLINSON B. (2010). Who are the crowdworkers ? :
shifting demographics in mechanical turk. In Proceedings of the 28th of the international conference extended
abstracts on Human factors in computing systems, CHI EA ’10, p. 2863–2872, New York, NY, USA : ACM.
ROSS J., Z ALDIVAR A., I RANI L. & T OMLINSON B. (2009). Who are the turkers ? worker demographics in
amazon mechanical turk. Social Code Report 2009-01, http ://www.ics.uci.edu/ jwross/pubs/SocialCode-2009-
01.pdf.
S AGOT B. (2005). Automatic acquisition of a Slovak lexicon from a raw corpus. In Lecture Notes in Artificial
Intelligence 3658 ((c) Springer-Verlag), Proceedings of TSD’05, p. 156–163, Karlovy Vary, République tchèque.
S MITH N. & E ISNER J. (2005). Contrastive estimation : Training log-linear models on unlabeled data. In
Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL’05), p. 354–362,
nn Arbor, Michigan, USA.
S NOW R., O’C ONNOR B., J URAFSKY D. & N G . A. Y. (2008). Cheap and fast - but is it good ? evaluating
non-expert annotations for natural language tasks. In Proceedings of EMNLP 2008, p. 254–263.
S TÜRENBERG M., G OECKE D., D IE - WALD N., C RAMER I. & M EHLER A. (2007). Web-based annotation of
anaphoric relations and lexical chains. In ACL Workshop on Linguistic Annotation Workshop (LAW), Prague,
République tchèque.
T RATZ S. & H OVY E. (2010). A taxonomy, dataset, and classifier for automatic noun compound interpreta-
tion. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, p. 678–687,
Uppsala, Suède : Association for Computational Linguistics.
VON   A HN L. (2006). Games with a purpose. IEEE Computer Magazine, p. 96–98.
VON A HN L. & DABBISH L. (2008). General techniques for designing games with a purpose. Communications
of the ACM, p. 58–67.
WAIS P., L INGAMNENI S., C OOK D., F ENNELL J., G OLDENBERG B., L UBAROV D., M ARIN D. & S IMONS
H. (2010). Towards building a high-qualityworkforce with mechanical turk. In Proceedings of Computational
Social Science and the Wisdom of Crowds (NIPS).
WATSON R., B RISCOE T. & C ARROLL J. (2007). Semi-supervised training of a statistical parser from unlabeled
partially-bracketed data. In Proceedings of the 10th International Conference on Parsing Technologies, IWPT
’07, p. 23–32, Stroudsburg, PA, USA : Association for Computational Linguistics.
W ELINDER P., B RANSON S., B ELONGIE S. & P ERONA P. (2010). The multidimensional wisdom of crowds.
In Neural Information Processing Systems Conference (NIPS).
X U F. & K LAKOW D. (2010). Paragraph acquisition and selection for list question using amazon’s mechanical
turk. In Proceedings of the International Language Resources and Evaluation (LREC-2010), p. 2340–2345, La
Valette, Malte.
YAROWSKY D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings
of the 33rd Annual Meeting of the Association for Computational Linguistics, p. 189–196, Cambridge, MA.
Y ETISGEN -Y ILDIZ M., S OLTI I., X IA F. & H ALGRIM S. R. (2010). Preliminary experience with amazon’s
mechanical turk for annotating medical named entities. In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon’s Mechanical Turk, CSLDAMT ’10, p. 180–183, Stroudsburg,
PA, USA : Association for Computational Linguistics.
Z ITTRAIN J. (2008). Ubiquitous human computing. Phil. Trans. R. Soc. A 28, 366(1881), 3813–3821.
Annotation manuelle de matchs de foot :
Oh la la la ! l’accord inter-annotateurs ! et c’est le but !

Karën Fort1, 2         Vincent Claveau3
(1) INIST-CNRS, 2 allée de Brabois, 54500 Vandoeuvre-lès-Nancy
(2) LIPN, Université Paris 13 & CNRS, 99 av. J.B. Clément, 93430 Villetaneuse
(3) IRISA - CNRS, Campus de Beaulieu, 35200 Rennes
karen.fort@inist.fr, vincent.claveau@irisa.fr

RÉSUMÉ
Cet article présente une campagne d’annotation de commentaires de matchs de football en
français. L’annotation a été réalisée à partir d’un corpus très hétérogène, contenant à la fois des
comptes-rendus minute par minute et des transcriptions des commentaires vidéo. Nous montrons
ici comment les accords intra- et inter-annotateurs peuvent être utilisés efficacement, en en
proposant une définition adaptée à notre type de tâche et en mettant en exergue l’importance
de certaines bonnes pratiques concernant leur utilisation. Nous montrons également comment
certains indices collectés à l’aide d’outils statistiques simples peuvent être utilisés pour indiquer
des pistes de corrections des annotations. Ces différentes propositions nous permettent par
ailleurs d’évaluer l’impact des modalités sources de nos textes (oral ou écrit) sur le coût et la
qualité des annotations.
ABSTRACT
Manual Annotation of Football Matches : Inter-annotator Agreement ! Gooooal !
We present here an annotation campaign of commentaries of football matches in French. The
annotation was done from a very heterogeneous text corpus of both match minutes and video
commentary transcripts. We show how the intra- and inter-annotator agreement can be used
efficiently during the whole campaign by proposing a definition of the markables suited to our
type of task, as well as emphasizing the importance of using it appropriately. We also show
how some clues, collected through statistical analyses, could be used to help correcting the
annotations. These statistical analyses are then used to assess the impact of the source modality
(written or spoken) on the cost and quality of the annotation process.
MOTS-CLÉS : annotation manuelle, accords inter-annotateurs.
KEYWORDS: manual annotation, inter-annotator agreement.
383

1     Introduction
Nous étudions dans cet article la création d’un corpus textuel annoté construit à partir de
transcriptions de commentaires vidéos et de sites Web spécialisés. Ce corpus annoté est développé
dans le but de mettre au point des techniques automatiques d’analyse, tels que le résumé vidéo,
le repurposing (transformation du contenu et du format pour un autre support de diffusion) ou
l’extraction d’information pour les vidéos d’événements sportifs. Cette application, développée
dans le cadre d’un partenariat industriel, n’est pas détaillée plus avant dans cet article, mais il est
important de noter qu’elle guide la définition des éléments à annoter (cf. section 2).
Outre la présentation d’une nouvelle ressource annotée, cet article a pour objectif de montrer
l’intérêt d’analyses fines pour évaluer la qualité d’une telle ressource hétérogène. En particulier,
nous proposons une définition des mesures d’accord inter- et intra-annotateur adaptée à ce type
d’annotation où seuls certains éléments des corpus sont annotés. Nous montrons également
comment certains indices collectés à l’aide d’outils statistiques simples peuvent être utilisés
pour souligner les difficultés de la tâche d’annotation et indiquer des pistes de corrections des
annotations. Ces différentes propositions nous permettent par ailleurs d’évaluer l’impact des
modalités sources de nos textes (oral ou écrit) sur le coût et la qualité des annotations.
D’un point de vue applicatif, quelques travaux (Nemrava et al., 2007, par exemple) font référence
à un corpus annoté du domaine du football, mais à notre connaissance, aucun ne détaille
l’annotation du corpus utilisé. D’autres études ont fait usage de corpus de football pour créer
des lexiques monolingues (Gasiglia, 2003) or multilingues (Schmidt, 2008) plus ou moins
détaillés. Dans ces cas, si les publications associées détaillent l’annotation du corpus utilisé, les
annotations elles-même sont de nature linguistique plutôt que du domaine et soulèvent des
questions différentes. D’un point de vue méthodologique, l’analyse statistique des annotations
repose principalement sur les calculs d’accord inter-annotateurs (Artstein et Poesio, 2008, pour
une revue détaillée). Ces derniers sont généralement fournis sur les corpus annotés comme
mesure d’évaluation de la qualité de la ressource produite (Dandapat et al., 2009, inter alia). Les
méthodes d’annotation agiles (Voormann et Gut, 2008) proposent d’utiliser ces mesures pendant
toute l’annotation du corpus, pour assurer la cohérence des annotations et limiter les divergences
dans les cas, majoritaires, où l’on ne peut pas tout annoter en double avec adjudication. Notre
travail se situe dans ce cadre mais aborde plusieurs problèmes posés par les particularités de nos
annotations. Après une présentation des données et des annotations en section 2, nous détaillons
les différentes analyses menées en section 3 et nous concluons en donnant quelques perspectives
à ce travail.
2     Campagne d’annotation

2.1    Données, annotations et méthodologie

Le corpus annoté couvre 16 matchs de football. Il est composé de 24 transcriptions de commen-
taires tirés de vidéos (1 par mi-temps, 12 matchs) et de 16 fichiers contenant une description
minute-par-minute du match (dont les 12 de la transcription et 4 matchs additionnels) tirés
de sites Web spécialisés. La parole contenue dans les vidéos a été transcrite manuellement en
utilisant TRANSCRIBER (Barras et al., 1998) et son guide de transcription par défaut. L’ensemble du
corpus a une taille d’environ 250 000 mots. Sa principale caractéristique est d’être très hétérogène
384

(Fort et al., 2011), que ce soit d’un point de vue des types de match (ligues, championnats...),
de la taille des fichiers (de 1 116 tokens par match pour les minutes à 21 000 tokens pour les
transcriptions), ou de la source (chaînes de diffusion des vidéos, commentateurs, sites Web...).
Le jeu d’étiquettes a été construit en définissant les éléments intéressants pour l’application finale
et ensuite affiné durant les phases d’entraînement et de pré-campagne. L’ensemble des étiquettes
retenues a été divisé en trois couches, Unités, Actions et Relations (cf. tableau 1 1 ), chacune corres-
pondant à un niveau d’analyse de complexité croissante à aborder successivement par les annota-
teurs. Par cohérence avec les besoins applicatifs et pour prendre en compte le style elliptique de
l’oral (« Makoun. Et c’est récupéré. Clerc, avec Cris. Boumsong, Makoun. »),
nous avons décidé de ne pas faire porter les annotations sur les prédicats dénotant les actions ou
les relations, souvent absents, mais sur les acteurs impliqués.

acteurs         Joueur, Equipe, Arbitre, Entraineur, ArbitreAssistant, Président
Unités
circonstants    EspaceSurTerrain, LieuDuMatch, TempsDansMatch
arbitrales      TirerCoupFrancDirect, TirerCoupFrancIndirect, TirerCorner, TirerPenalty, Faire-
Actions
FauteDeJeu, HorsJeu, MarquerBut, PrendreCartonJaune, PrendreCartonRouge,
PrendreRappelALOrdre
autres          Centrer, FaireTentative2Centre, Dribbler, RaterBut, ArreterBut, IntercepterBallon,
PossederBallon, ActionDuPublic
arbitrales      FaireFauteSurJoueur, TaclerFaute, RemplacerJoueur
Relations
autres          FaireCombinaison, FairePasse, FaireTentative2Passe
TABLE 1 – Couches d’annotations retenues et étiquettes correspondantes
La méthodologie employée pour l’annotation de ce corpus suit les recommandations de Bonneau-
Maynard et al. (2005) et Gut et Bayerl (2004) ; elle est décrite en détail dans (Fort et Claveau,
2012). L’annotation a été réalisée par deux annotateurs experts du domaine avec l’outil d’anno-
tation GLOZZ (Widlöcher et Mathet, 2009), choisi en raison de sa facilité d’utilisation et de la
possibilité qu’il offre d’annoter des relations. Les temps d’annotation par couche ont été mesurées
à l’aide de l’outil TIMETRACKER 2 . Nous avons également invité les annotateurs à ajouter des
commentaires sur leurs annotations, et un attribut Incertitude a été mis à leur disposition dans
GLOZZ .
2.2 Données générales sur le processus d’annotation

Le nombre total d’annotations produites s’élève à 37 784 dont 27 736 (soit plus de 73 %) pour
les transcriptions. Toutes les catégories ont été utilisées, mais avec une grande disparité : par
exemple, TirerCoupFrancIndirect et TirerPenalty n’ont servi que 2 fois (et uniquement dans les
minutes), PrendreCartonRouge 6 fois et Président 9 fois.
Le tableau 2 présente le temps d’annotation moyen (pour 1 000 tokens) par annotateur et par
source. Un t-test de Welsh à deux échantillons (avec p = 0, 05) montre que les différences
entre annotateurs ne sont pas significatives, que ce soit pour les transcriptions ou pour les
1. Le regroupement des étiquettes à l’intérieur de ces couches (circonstants, acteurs, etc) est proposé ici pour faciliter
la lecture et l’analyse, mais n’existait pas dans le modèle de données utilisé pour l’annotation.
2. http://www.formassembly.com/time-tracker/#
385

Minutes   Transcriptions
Annotateur 1    36,92        20,03
Annotateur 2    41,30        16,06
TABLE 2 – Temps moyen d’annotation par source et par annotateur, en minute/1 000 tokens
minutes. En revanche, les différences entre modalités sont jugées statistiquement significatives,
pour les deux annotateurs. Cela s’explique par la différence (statistiquement significative) de
densité d’annotations (nombre d’annotations par token) : 0,16 pour les minutes et 0,08 pour les
transcriptions. En effet, les commentateurs sportifs ne parlent pas uniquement des événements
du matchs et ont tendance à digresser. En revanche, si l’on rapporte le temps d’annotation au
nombre d’annotations produites, aucune différence n’est constatée entre minutes et transcriptions.
Les différences de temps entre les deux modalités s’expliquent donc uniquement par le nombre
plus important d’annotations à produire à volume de texte constant.
3     Analyse statistique des annotations

3.1    Mesures d’accord et estimation des “annotables”

Les calculs d’accords inter- et intra-annotateur servent à quantifier la fiabilité, et donc la qualité,
des annotations produites, mais aussi à fixer une limite supérieure aux performances que l’on
peut attendre d’un système automatique, et enfin, dans notre cas, à mesurer la difficulté de
la tâche selon la modalité d’origine. Pour ce faire, les Kappa (κ) de Cohen (Cohen, 1960) et
de Carletta (Carletta, 1996) sont préférés aux mesures plus simples telles que la F-mesure car
ils normalisent l’accord observé en fonction de l’accord attendu (ou dû au hasard). Carletta
considère que l’annotation par hasard se traduit par une unique distribution valable pour les deux
annotateurs, alors que Cohen considère que ces distributions dépendent de chaque annotateur
(Artstein et Poesio, 2008, pour une description complète et des comparaisons).
Cependant, ces définitions posent problème dès lors que ce ne sont pas seulement les étiquettes
qui peuvent varier, mais aussi les éléments à annoter (les marquables ou annotables), puisqu’elles
ne précisent en rien comment le désaccord sur les annotables doit être traité. Nous proposons
donc d’étendre les κ en décomposant l’accord en un accord sur l’annotable et un accord sur
l’étiquette. De telles mesures nécessitent donc de connaître le nombre d’annotables           . Ce
nombre d’annotables est évident ou connu a priori pour certaines tâches (comme l’étiquetage
morphosyntaxique : tous les tokens sont annotables), mais ne peut être qu’estimé a posteriori pour
des tâches comme la nôtre (Grouin et al., 2011). Nous proposons pour ce faire une estimation
originale basée sur une procédure EM (Expectation-Maximization) décrite dans l’algorithme 1.
Celui-ci énumère itérativement le nombre d’annotables δ (étape de Maximization) en utilisant la
probabilité γ (estimée itérativement) que tous les annotateurs aient manqué le même annotable,
elle-même calculée grâce à l’estimation du nombre d’annotables δ de l’itération précédente
(expectation).
Avoir une estimation la plus exacte possible du nombre d’annotables est un enjeu d’importance
pour obtenir des accords inter-annotateurs réalistes. Par exemple, si l’on considère que tous
386

Algorithme 1 Estimation EM des annotables

Entrées : {   j}   (ensembles des éléments annotés par les annotateurs A j ) ; δ0 =       j
j
for (i=1 ; δi = δi−1 ; i++) do
δi−1 −|   j|
expectation : γi =    j   P(A j manque un marquable) =    j      δi−1
δ0
maximization : δi =
1 − γi
end for
return δ
les mots (tokens) des textes sont des annotables (et donc ceux non annotés sont considérés
annotés par défaut par une étiquette sans-annotation), le Kappa de Cohen pour les accords
intra- et inter-annotateurs atteindrait respectivement 0,9456 et 0,9404, principalement par
l’abondance des accords sur les très nombreux mots sans-annotation. De telles valeurs masquent
des différences qui sont révélées avec l’estimation plus réaliste des annotables que nous proposons
(voir sous-section 3.2).
Les deux κ, tels que nous les avons implémentés, sont aussi très stricts, puisque la moindre
différence dans les annotations (étiquette bien sûr, mais aussi délimitation des entités) est
considérée comme un désaccord. Quand cela est possible, nous fournissons donc également
la mesure d’accord entropique implémentée dans GLOZZ (Mathet et Widlöcher, 2011) ; celle-ci
autorise en effet les correspondances partielles d’annotation et fournit donc des valeurs d’accord
prenant en compte ces accords partiels. Elle ne s’applique cependant pas encore aux relations.
3.2      Accords inter-annotateurs

Le tableau 3 présente l’accord inter- et intra-annotateur, selon la modalité, calculés avec le κ de
Cohen, et, à des fins de comparaison, la mesure d’entropie de GLOZZ. Le κ de Carletta a également
été calculé et est très proche dans la quasi-totalité des cas au κ de Cohen ; nous ne reportons donc
pas ses valeurs par manque de place. Cette proximité signifie qu’il n’y a pas de biais d’annotateur :
les distributions des annotations produites par chacun des annotateurs sont très similaires
(Artstein et Poesio, 2008). On constate sans surprise que l’accord (aussi bien inter- qu’intra-
annotateur) a tendance à être plus faible dans les transcriptions que dans les minutes, à l’exception
d’une transcription pour laquelle les unités/actions ont produit un accord bien supérieur (près de
0,65). Cette tendance générale se manifeste spécialement dans les cas d’annotations complexes
comme les relations. Les spécificités de l’oral mentionnées précédemment, et en particulier le
style elliptique propre aux commentaires, expliquent facilement cette différence.
Si le calcul d’accord inter-annotateurs est devenu une bonne pratique standard du développement
de ressources annotées, nous souhaitons promouvoir dans cet article l’intérêt d’une analyse plus
détaillée. Cela est d’autant plus important quand les éléments annotés relèvent de catégories
différentes et que ces catégories elles-mêmes ont des populations très différentes, comme c’est
le cas ici. En effet, les valeurs présentées précédemment masquent des disparités importantes
entre catégories d’annotation. Dans le tableau 4, colonnes 2 et 5, nous développons les résul-
tats d’accord inter-annotateurs par regroupements de catégories. Les difficultés accrues sur les
transcriptions se vérifient à cette échelle, mais l’on constate en outre de très faibles accords pour
387

inter-annotateurs       intra-annotateur A1       intra-annotateur A2
κ de Cohen     Glozz     κ de Cohen     Glozz      κ de Cohen     Glozz
Minutes unités/actions            0,5992      0,7627       0,7531       0,8753         0,7109    0,8519
Minutes relations                 0,5707         -         0,6377          -           0,5983       -
Transcriptions unités/actions     0,6234      0,7498       0,7558       0,8327         0,6812    0,8179
Transcriptions relations          0,4345         -         0,4010          -           0,4701       -

TABLE 3 – Accords inter-annotateurs et intra-annotateur par modalité

Minutes                                 Transcriptions
κ      Incertitude   Gain d’entropie       κ      Incertitude    Gain d’entropie
Acteurs                0,9228       0,5              -          0,8974           1,0            -1
Circonstants           0,4827       1,9             49          0,4441          10,0            15
Actions arbitrales     0,5999       4,3              -          0,5082          19,7             7
Actions autres         0,3240       1,3             92          0,1407           9,8            26
Relations arbitrales   0,6355      10,7              -          0,4520          18,4             8
Relations autres       0,5540      10,2              8          0,3793          69,9            23

TABLE 4 – Accords inter-annotateurs par modalité et par famille d’annotations
certaines catégories. Les accords sur les entités offrent un grand contraste entre les annotations
des acteurs et des circonstants, davantage sujets à interprétations. De la même manière, les évé-
nements (actions ou relations) sanctionnés par une action de l’arbitre obtiennent des accords bien
supérieurs aux autres événements. Un examen détaillé des résultats montre que les annotateurs
sont rarement en désaccord sur les types des éléments annotés, mais qu’ils annotent des éléments
différents. Ce dernier point justifie d’autant plus l’emploi de notre technique d’estimation des
annotables et explique pourquoi la définition standard des κ sur-estime tant l’accord.
3.3      Incertitudes

Les annotateurs avaient la possibilité d’indiquer les annotations leur posant problème, pour
quelque raison que ce soit, à l’aide d’un champ Incertitude. Ces incertitudes permettent, lors de
la campagne, de préciser les instructions d’annotations, de comprendre certaines annotations
lors de l’utilisation du corpus, mais aussi d’aider à l’analyse automatique des résultats, comme
indicateur de la difficulté d’annotation. Il est à noter qu’un seul des annotateurs de la campagne
a véritablement utilisé les incertitudes, mais de manière systématique.
Dans les colonnes 3 et 6 du tableau 4, nous présentons les taux d’incertitude par catégorie
d’annotations et par modalité. On y constate encore une fois que proportionnellement plus
d’incertitude concerne l’oral retranscrit (différence statistiquement significative, test de Student
pour deux ensembles, avec p = 0, 05).
Nous nous sommes intéressés au lien éventuel entre incertitude et désaccord. Nous avons cherché
à savoir si la présence d’une incertitude est liée au désaccord. Par contre, nous considérons
non interprétable l’absence d’incertitude. Pour ce faire, nous avons calculé la différence entre
l’entropie de l’accord H(Acc) (eqn 2) de la variable aléatoire Acc indiquant s’il y a accord ou
non ( Acc = {vrai ; faux}) et l’entropie conditionnelle de l’accord sachant qu’une incertitude est
388

présente (H(Acc|I nc = présent), eqn 2). Un gain positif signifie que l’incertitude aide à discerner
les accords des désaccords. Autrement dit, pour une catégorie donnée, un gain positif indique
que l’incertitude peut aider à prédire les catégories susceptibles de désaccord.

H(Acc) = −             v∈   Acc
P(Acc = v) log P(Acc = v)                           (1)
H(Acc|I nc = vrai) = −          v∈   Acc
P(Acc = v|I nc = vrai) log P(Acc = v|I nc = vrai)              (2)

Ces gains sont indiqués en colonnes 4 et 7 du tableau 4 pour les familles d’annotation (dans
trois cas, il n’y a pas assez d’incertitudes pour les calculer). À une exception près, ils sont tous
positifs, ce qui signifie que ces incertitudes sont des bons indicateurs d’erreurs, même si elles
n’ont été posées que par un seul annotateur. Que ce soit pour les minutes ou les transcriptions, il
faut remarquer que le gain est d’autant plus fort que le taux de désaccord est important. L’étude
des causes de ces incertitudes est donc une piste privilégiée pour la correction systématisée des
désaccords et donc des éventuelles erreurs d’annotation.
4     Conclusion et perspectives

L’analyse de la campagne d’annotation présentée dans cet article 3 a mis en exergue différents
éléments. D’un point de vue méthodologique, notre technique d’estimation des annotables doit
permettre un calcul d’accord inter-annotateurs plus réaliste dans les cas où leur nombre peut
varier selon l’annotateur. Nous avons aussi montré que les bonnes pratiques ne sauraient se
limiter à un calcul d’accord inter-annotateurs unique pour l’ensemble des annotations quand
celles-ci relèvent de catégories différentes et d’effectifs non équilibrés. Enfin, nous avons montré
que l’étude statistique des incertitudes met au jour une possibilité de détecter systématiquement
les désaccords ou erreurs potentiels. Ces différentes analyses nous ont aussi permis de montrer
que le coût d’annotation des textes issus de l’oral est moindre que pour ceux issus de l’écrit,
du fait de la différence de densité des annotations. En revanche, les indicateurs de qualité
(désaccord, incertitudes) indiquent sans ambiguïté la difficulté accrue de traiter de l’oral. Les
annotations seront librement disponibles sous licence LGPL-LR à http://www.irisa.fr/
texmex/people/claveau/corpora/FootQuaero/ dès que les corrections identifiées auront
été effectuées. Le guide d’annotation mis à jour sera lui-aussi fourni.
En suite de ce travail, et aussi bien d’un point de vue théorique que pratique, nous souhaitons
développer des approches permettant de propager automatiquement des corrections d’annota-
tions à partir de quelques corrections apportées à une petite quantité de données. Ces approches
s’appuieraient d’une part sur les analyses précédentes pour détecter les catégories les plus pro-
blématiques, et éventuellement sur des approches d’apprentissage artificiel pour proposer des
corrections.
3. Nous remercions chaleureusement les annotateurs de la campagne, C. Ris et A. Zérouki, de l’INIST-CNRS, pour leur
travail minutieux et leurs précieux retours. Nous remercions également V. Lux et A.-R. Ebadat pour leur participation à la
préparation de la campagne, et Technicolor pour la mise à disposition d’une partie des données. Ce travail a été réalisé
dans le cadre du programme Quæro (http://www.quaero.org), financé par OSEO, agence nationale de valorisation
de la recherche.
389

Références
ARTSTEIN, R. et POESIO, M. (2008). Inter-Coder Agreement for Computational Linguistics.
Computational Linguistics, 34(4):555–596.
BARRAS, C., GEOFFROIS, E., WU, Z. et LIBERMAN, M. (1998). Transcriber: a free tool for segmenting,
labeling and transcribing speech. In Actes de First International Conference on Language Resources
and Evaluation (LREC 1998), Grenade, Espagne.
B ONNEAU-MAYNARD, H., ROSSET, S., AYACHE, C., KUHN, A. et MOSTEFA, D. (2005). Semantic
annotation of the french media dialog corpus. In Actes de InterSpeech, Lisbonne, Portugal.
CARLETTA, J. (1996). Assessing Agreement on Classification Tasks: the Kappa Statistic. Compu-
tational Linguistics, 22:249–254.
COHEN, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological
Measurement, 20(1):37–46.
DANDAPAT, S., BISWAS, P., CHOUDHURY, M. et BALI, K. (2009). Complex Linguistic Annotation -
No Easy Way Out ! A Case from Bangla and Hindi POS Labeling Tasks. In Proceedings of the
third ACL Linguistic Annotation Workshop, Singapour.
FORT, K. et CLAVEAU, V. (2012). Annotating football matches: : Influence of the source medium
on manual annotation. In Actes de Eighth International Conference on Language Resources and
Evaluation (LREC 2012), Istanbul, Turquie.
FORT, K., NAZARENKO, A. et RIS, C. (2011). Corpus linguistics for the annotation manager. In
Actes de Corpus Linguistics, Birmingham, Angleterre.
GASIGLIA, N. (2003). Pistes méthodologiques pour l’exploration d’un corpus à haut rendement
relatif au parler du football, une langue de spécialité de grande diffusion. In 3es journées de
linguistique de corpus. Centre de Recherche en Littérature, Linguistique et Civilisation (CRELLIC),
Université de Bretagne-Sud, Lorient.
GROUIN, C., ROSSET, S., ZWEIGENBAUM, P., FORT, K., GALIBERT, O. et QUINTARD, L. (2011). Proposal
for an extension of traditional named entities: from guidelines to evaluation, an overview. In
Actes de 5th Linguistic Annotation Workshop, pages 92–100, Portland, Oregon, USA. Association
for Computational Linguistics.
GUT, U. et BAYERL, P. S. (2004). Measuring the reliability of manual annotations of speech
corpora. In Actes de Speech Prosody, pages 565–568, Nara, Japon.
MATHET, Y. et WIDLÖCHER, A. (2011). Une approche holiste et unifiée de l’alignement et de la
mesure d’accord inter-annotateurs. In Actes de Traitement Automatique des Langues Naturelles
2011 (TALN 2011), Montpellier, France.
NEMRAVA, J., SVATEK, V., SIMUNEK, M. et BUITELAAR, P. (2007). Mining over: football match data:
seeking associations among explicit and implicit events. In Proc. of Znalosti 2007.
SCHMIDT, T. (2008). The Linguistics of Football (Language in Performance 38), volume 38, chapitre
The Kicktionary: Combining corpus linguistics and lexical semantics for a multilingual football
dictionary, pages 11–23. Gunter Narr, Tübingen, Allemagne.
VOORMANN, H. et GUT, U. (2008). Agile corpus creation. Corpus Linguistics and Linguistic Theory,
4(2):235–251.
WIDLÖCHER, A. et MATHET, Y. (2009). La plate-forme Glozz : environnement d’annotation et
d’exploration de corpus. In Actes de Traitement Automatique des Langues 2009 (TALN 2009),
Senlis, France.
390
TCOF-POS : un corpus libre de français parlé
annoté en morphosyntaxe

Christophe Benzitoun1             Karën Fort2,3 Benoît Sagot4
(1) ATILF, Nancy Université & CNRS, 44, avenue de la Libération, BP 30687, 54063 Nancy cedex
(2) INIST-CNRS, 2 allée de Brabois, 54500 Vandoeuvre-lès-Nancy
(3) LIPN, Université Paris 13 & CNRS, 99 av. J.B. Clément, 93430 Villetaneuse
(4) Alpage, INRIA Paris–Rocquencourt & Université Paris 7, Rocquencourt, France
❝❤r✐st♦♣❤❡✳❜❡♥③✐t♦✉♥❅❛t✐❧❢✳❢r✱ ❦❛r❡♥✳❢♦rt❅✐♥✐st✳❢r✱ ❜❡♥♦✐t✳s❛❣♦t❅✐♥r✐❛✳❢r

RÉSUMÉ
Nous présentons dans cet article un travail portant sur la création d’un corpus de français parlé
spontané annoté en morphosyntaxe. Nous détaillons la méthodologie suivie afin d’assurer le
contrôle de la qualité de la ressource finale. Ce corpus est d’ores et déjà librement diffusé pour
la recherche et peut servir aussi bien de corpus d’apprentissage pour des logiciels que de base
pour des descriptions linguistiques. Nous présentons également les résultats obtenus par deux
étiqueteurs morphosyntaxiques entrainés sur ce corpus.
ABSTRACT
TCOF-POS : A Freely Available POS-Tagged Corpus of Spoken French
This article details the creation of TCOF-POS, the first freely available corpus of spontaneous
spoken French. We present here the methodology that was followed in order to obtain the best
possible quality in the final resource. This corpus already is freely available and can be used as a
training/validation corpus for NLP tools, as well as a study corpus for linguistic research. We also
present the results obtained by two POS-taggers trained on the corpus.
MOTS-CLÉS : Etiquetage morpho-syntaxique, français parlé, ressources langagières.
KEYWORDS: POS tagging, French, speech, language resources.
1    Introduction

L’annotation automatique du français parlé est généralement réalisée par le biais de pré-
traitements de corpus ou d’adaptation d’outils existant pour le texte (Dister, 2007; Blanc et al.,
2008). Une autre solution peut consister à masquer certains phénomènes tels que les "disfluences"
(répétitions, amorces de mots, etc.) (Valli et Véronis, 1999). Pourtant, l’utilisation d’étiqueteurs
automatiques élaborés pour et à partir de données écrites n’est pas une solution optimale étant
données les particularités des corpus oraux par rapport à l’écrit. Même si l’étiquetage de corpus
oraux ne représente pas un problème spécifique (Benzitoun, 2004), l’utilisation de modèles
entraînés sur des données écrites donne des résultats médiocres. Ainsi, nous avons testé Tree-
Tagger (Schmid, 1997), avec son modèle standard pour le français, sur un échantillon de 3 007
tokens extraits du corpus de référence décrit dans cet article et nous avons obtenu une précision
de seulement 83,1 %.
99

Un corpus du français parlé annoté en morphosyntaxe librement disponible serait donc utile, non
seulement pour les logiciels d’annotation en morphosyntaxe, mais également pour améliorer les
systèmes de transcription automatique (Huet et al., 2006) ou d’autres outils. Cependant, il n’existe
pas encore, à notre connaissance, de corpus de français parlé spontané annoté en morphosyntaxe
(parties du discours et/ou lemmes) qui soit diffusé librement. Parmi les corpus annotés mais non
diffusés librement, on peut citer les projets elicop (Mertens, 2002), C-ORAL-ROM (Campione
et al., 2005), Valibel (Dister, 2007), Corpus de Français Parlé Parisien (Branca-Rosoff et al., 2010)
ou bien encore ESLO (Eshkol et al., 2010).
Notre objectif est donc de développer et diffuser librement à l’ensemble de la communauté
scientifique un corpus pré-annoté automatiquement puis corrigé manuellement, dont la qualité
aura été précisément évaluée. Il pourra servir notamment de corpus d’apprentissage spécifique
au français parlé et plus largement de corpus exploitable pour des recherches en linguistique ou
en Traitement Automatique des Langues (TAL).
Nous présentons tout d’abord le corpus de français parlé TCOF (Traitement des Corpus Oraux du
Français), puis la méthodologie utilisée pour l’annotation manuelle, les différentes évaluations
réalisées pendant la campagne et enfin les résultats obtenus par les étiqueteurs morphosyn-
taxiques entrainés sur une partie du corpus annoté TCOF-POS.
2     Présentation du corpus

Le corpus d’origine que nous avons annoté est celui du projet TCOF (André et Canut, 2010),
librement disponible sur le site du CNRTL 1 . Ce corpus est constitué de transcriptions de données
orales recueillies dans des contextes aussi naturels que possible. Il comporte une partie d’interac-
tions entre adultes et une autre entre adultes et enfants. En ce qui concerne la partie adulte (la
seule que nous ayons exploitée jusqu’à présent), elle est composée :
– d’interactions sollicitées, dans lesquelles au moins deux locuteurs sont engagés dans des
récits de vie, d’événements ou d’expériences, ou dans des explications sur un savoir-faire
professionnel ou technique ;
– de conversations à bâtons rompus ou portant sur des thématiques spécifiques ;
– de données non sollicitées dans des situations publiques ou professionnelles : réunions pu-
bliques, activités professionnelles diverses.
De ce corpus, nous avons extrait un échantillon de 22 240 tokens 2 , soit 11 transcriptions
différentes. Cet échantillon contient des conversations, des réunions professionnelles, ainsi
que des extraits d’une Assemblée Générale à l’Université. L’intégralité des paroles prononcées
a été scrupuleusement retranscrite en orthographe standard, sans artifice ou aménagement
orthographique (donc sans ponctuation), suivant en cela les recommandations de (Blanche-
Benveniste et Jeanjean, 1987) 3 largement diffusées et utilisées. Elles sont au format généré par
le logiciel Transcriber (trs - XML).

1. http ://cnrtl.fr/corpus/tcof/
2. Notre conception de la notion de token est assez élémentaire (aucune insertion possible).
3. Les       conventions     de    transcription      sont    disponibles      sur     le    site   suivant   :
http ://cnrtl.fr/corpus/tcof/TCOFConventions.pdf
100

Ces transcriptions sont automatiquement converties en texte brut à l’aide d’une feuille de style
XSLT (qui élimine l’intégralité des balises XML), puis d’une série d’expressions régulières qui
supprime les informations non désirées, telles que les pauses. Le texte final contient les mentions
des locuteurs (L1, L2, etc.), l’intégralité des paroles prononcées, ainsi que les multi-transcriptions.
Il s’agit donc de transcriptions brutes non retouchées, dont voici un exemple :
L1 et puis je crois que c’est en je crois je crois même que c’est en zone industrielle
L2 ouais ouais je pense aussi ça doit pas être en ville
L1 oui mais
L2 en Belgique aussi il y a des trucs euh un genre de grand tr- enfin un genre de grande
galerie en Belgique et puis c’est que des magasins de fringues aussi
Les transcriptions ont été faites dans le cadre d’un cours de deuxième année de Sciences
du Langage à l’Université Nancy 2, puis revues par des enseignants de Sciences du Langage.
L’anonymisation, quant à elle, a été réalisée manuellement par des étudiants-vacataires. A la
lecture, ils devaient repérer les toponymes, anthroponymes, etc. puis les remplacer par un
symbole et insérer un son dans la portion de signal sonore correspondante.
3    Méthodologie

L’annotation totalement manuelle de corpus étant très coûteuse, nous avons procédé, comme
décrit dans (Marcus et al., 1993), à une correction manuelle de corpus pré-annotés automatique-
ment. La nature du pré-annotateur, ainsi que les modalités de la correction manuelle diffèrent
selon les étapes du processus, comme nous allons le voir dans cette section. Toutefois, toutes
les pré-annotations ont été produites par différentes instances du système TreeTagger (Schmid,
1997), qui fournit pour chaque token d’entrée une étiquette morphosyntaxique et un lemme.
Comme indiqué en introduction, l’utilisation comme pré-annotateur pour un corpus de parole
spontanée transcrite, d’un étiqueteur morphosyntaxique entraîné sur un corpus écrit n’est pas
adaptée. Parmi les phénomènes qui posent problème, lesquels ne sont pas totalement absents
des corpus écrits mais y sont bien plus rares (Benzitoun, 2004), on peut citer :
– les répétitions de mots ou de groupes de mots (ça ça redevient ça redevient le bordel comme ça),
– les reformulations (peut-être séparer complètement euh junior euh homme enfin euh adulte),
– les ruptures de construction (ouais ouais que de la gueule que de la),
– les amorces de mots (moi j’aurais p- j’aurais pas mis de pantalon),
– les incises (euh on considérait que former les hommes et c’est toujours euh en en en vigueur ça
hein former les les en- les les enfants d’aujourd’hui c’est aussi former les hommes de demain),
– les formes non conventionnelles (tu sais genre trop vénère ; avoir du matos en entrée de mag),
– les particules discursives (hein, eh ben, etc.). . .
Pour ne prendre que deux exemples, la version standard de TreeTagger pour le français considère
que bon est systématiquement un adjectif et quoi un pronom, alors qu’ils sont majoritairement des
particules discursives. De plus, nos transcriptions ne sont pas segmentées en « phrases » (Blanche-
Benveniste et Jeanjean, 1987), ce qui peut également poser des problèmes aux outils. Par
exemple, l’étiquette SENT (pour sentence) indiquant une frontière de phrase doit obligatoirement
être présente dans le lexique servant pour l’apprentissage de TreeTagger (même s’il ne s’en sert
pas par la suite). En conséquence, nous avons procédé, dès que possible, à l’entraînement de
versions de TreeTagger à partir des annotations déjà obtenues sur notre corpus.
101

La méthodologie retenue, décrite en détail dans cette partie, peut être résumée comme suit :
1. Définition de critères de tokenisation et d’identification des composés, puis tokenisation
automatique ;
2. Définition d’un jeu d’étiquettes adapté à la parole spontanée transcrite ;
3. Création d’un corpus de référence Cref de 22 240 tokens par correction d’une pré-annotation
automatique, effectuée par deux experts linguistes :
– Les 10 000 premiers tokens de Cref ont été pré-annotés avec la version standard de
TreeTagger ;
– Les 12 240 tokens suivants de Cref ont été pré-annotés avec une version de TreeTagger
entraînée sur les 10 000 premiers ;
4. Ré-annotation par deux étudiantes d’environ 7 500 tokens du corpus de référence Cref
(suivie d’une phase d’adjudication), afin d’évaluer la qualité des annotations dans deux
configurations distinctes :
– environ 6 000 tokens ont été pré-annotés par la version standard de TreeTagger ;
– environ 1 500 tokens ont été pré-annotés avec une version de TreeTagger entraînée sur
les 16 312 premiers tokens de Cref ;
L’objectif était ici de mesurer l’impact de la différence de qualité entre pré-annotateurs en
termes de vitesse d’annotation et de précision du résultat de l’étape manuelle ;
5. Application de cette méthodologie à un plus grand nombre d’étudiants pour en valider la
robustesse ;
6. Annotation par deux étudiantes d’un corpus additionnel Cadd de 80 000 nouveaux tokens,
pré-annotés avec la version de TreeTagger entraînée sur la totalité de Cref .
Nous avons appliqué pour cette campagne les bonnes pratiques actuelles en annotation manuelle
de corpus, qui consistent à évaluer le plus tôt possible l’accord inter-annotateurs et de mettre
à jour le guide d’annotation (Bonneau-Maynard et al., 2005). La répétition régulière de ce
processus conduit à ce qu’on appelle maintenant l’annotation agile (Voormann et Gut, 2008).
3.1    Tokenisation et gestion des composés

Le corpus ayant fait l’objet d’une pré-annotation (voir section 3.3), nous avons pris comme base
la tokenisation par défaut de TreeTagger, qui repose notamment sur un fichier de composés. Mais
ce dernier s’est avéré insuffisant (par exemple, parce que reste découpé en deux tokens distincts
mais puisqu’ils en un seul token). Nous l’avons donc complété au fur et à mesure, en respectant
le critère suivant : toute séquence dans laquelle il est possible d’insérer un élément est découpée
en plusieurs tokens, afin d’exclure les unités discontinues. Ainsi, un peu est découpé en deux
tokens (car on peut trouver un tout petit peu).
3.2    Un jeu d’étiquettes adapté à la parole spontanée transcrite

Afin de bénéficier au mieux des ressources développées pour l’écrit et de limiter le travail de
correction, tout en prenant en considération les phénomènes spécifiques à la parole spontanée
cités ci-dessus, nous avons décidé d’utiliser un jeu d’étiquettes basé au départ sur les étiquettes
par défaut fournies par TreeTagger. Nous l’avons complété à l’aide de (Abeillé et Clément, 2006).
102

Les répétitions, reformulations, etc. n’ont pas fait l’objet de traitements spécifiques, chacun des
tokens a la catégorie qu’il a habituellement (ex : le[DET] le[DET] le[DET] chat). Au final, même
si les identifiants des étiquettes sont différents, les catégories retenues sont quasiment identiques
à (Abeillé et Clément, 2006), avec toutefois un peu moins de sous-catégories (notamment aucune
pour les adverbes et les adjectifs) et l’ajout de la catégorie « auxiliaire » ainsi que de trois
étiquettes spécifiques à l’oral : MLT (multi-transcription), TRC (amorce de mot) et LOC (locuteur)
(cf. tableau 1). Afin de nous aider dans la rédaction du manuel d’annotation, nous nous sommes
d’ailleurs inspirés de (Abeillé et Clément, 2006). Notre jeu d’étiquettes comprend 62 étiquettes.
En outre, il a été affiné durant la phase de constitution du corpus servant de référence. En effet,
nous voulions que les étiquettes soient apposées de manière aussi systématique que possible pour
que nos choix soient réversibles et que les modifications soient automatisables, autant que faire
se peut. De ce fait, même si cela peut paraître discutable d’un point de vue théorique, nous avons
privilégié les choix qui potentiellement génèrent le moins de fluctuations entre annotateurs. Par
exemple, la distinction entre participe passé et adjectif n’est pas aisée et plutôt que d’obtenir une
annotation de qualité moindre, nous avons préféré neutraliser celle-ci. Ainsi, chaque fois que
la forme verbale existe (sauf cas de changement notoire de sens), nous avons annoté « verbe ».
Dans le cas contraire, nous avons annoté « adjectif ».
Nous avons également décidé d’essayer de limiter les cas de transferts d’une catégorie vers
une autre (trans-catégorisation). En effet, ceux-ci auraient artificiellement été limités aux cas
rencontrés dans le corpus à annoter, sans possibilité d’avoir une vision globale du phénomène.
De plus, cela aurait complexifié la tâche de correction. Ainsi, dans rouler tranquille, tranquille est
considéré comme un adjectif et non comme un adverbe (ce qui, de toute façon, est discutable
d’un point de vue théorique). Enfin, il n’a pas été possible d’exclure totalement les cas d’étiquettes
limitées à un mot unique. Ainsi, l’étiquette « particule interrogative » ne s’utilise que pour est-ce
qu-e/i et « prédéterminant » uniquement pour tous.
3.3 Création du sous-corpus de référence

Comme indiqué ci-dessus, la création de la première tranche de 10 000 tokens du corpus de
référence Cref de 22 240 tokens a été réalisée en utilisant comme pré-annotateur la version
standard de TreeTagger, entraînée sur un corpus écrit. Nous (L. Bérard et C. Benzitoun) avons
ensuite corrigé ces pré-annotations en plusieurs passes. Nous avons tout d’abord effectué des
remplacements automatiques, lorsque les modifications étaient systématiques ou que l’étiquette
majoritaire n’était pas celle apposée par défaut par le logiciel (ce qui est le cas pour bon (ADJ/INT)
et quoi (PRO :int/INT), par exemple). Ensuite, nous nous sommes répartis les données à corriger
et, après les avoir intégralement traitées, nous nous les sommes échangées pour révision. Nous
avons ensuite discuté des cas où nous n’étions pas en accord jusqu’à trouver des solutions.
Nous avons effectué ces étapes plusieurs fois, jusqu’à obtenir des annotations fiables. Le guide
d’annotation était mis à jour à chaque étape.
Nous avons par ailleurs généré automatiquement des fichiers de fréquences, afin de faciliter
le repérage des erreurs. A ainsi été calculée la fréquence de chaque étiquette pour un même
lemme ou un même token, ce qui nous a permis d’identifier et de corriger quelques erreurs
supplémentaires. Par exemple, C.E. ayant été annoté 1 fois NAM et 3 fois NOM :sg (pour un
même lemme C.E.), cette dernière étiquette a été attribuée aux 4 occurrences. De même, cela
nous a permis de corriger deux occurrences de du, indûment annotées DET :ind.
103

ADJ         adjectif                          NUM         numéral
ADV         adverbe                           PRO :clo    clitique objet
AUX :cond   auxiliaire au conditionnel        PRO :cls    clitique sujet
AUX :futu   auxiliaire au futur               PRO :clsi   clitique sujet impersonnel
AUX :impe   auxiliaire à l’impératif          PRO :dem    pronom démonstratif
AUX :impf   auxiliaire à l’imparfait          PRO :ind    pronom indéfini
AUX :infi   auxiliaire à l’infinitif          PRO :int    pronom interrogatif
AUX :pper   auxiliaire au participe passé     PRO :pos    pronom possessif
AUX :ppre   auxiliaire au participe pré-      PRO :rel    pronom relatif
sent
AUX :pres   auxiliaire au présent             PRO :ton    pronom tonique
AUX :simp   auxiliaire au passé simple        PRP         préposition
AUX :subi   auxiliaire au subjonctif im-      PRP :det    préposition/déterminant
parfait
AUX :subp   auxiliaire au subjonctif pré-     PRT :int    particule interrogative (est-
sent                                          ce que)
DET :def    déterminant défini                SYM         symbole
DET :dem    déterminant démonstratif          TRC         amorces de mots
DET :ind    déterminant indéfini              VER         verbe sans flexion (voilà)
DET :int    déterminant interrogatif          VER :cond   verbe au conditionnel
DET :par    déterminant partitif (du)         VER :futu   verbe au futur
DET :pos    déterminant possessif             VER :impe   verbe à l’impératif
DET :pre    pré-déterminant (tout (le))       VER :impf   verbe à l’imparfait
EPE         épenthétique                      VER :infi   verbe à l’infinitif
ETR         mots étrangers                    VER :pper   verbe au participe passé
FNO         forme noyau (oui, non, d’ac-      VER :ppre   verbe au participe présent
cord, etc.)
INT         interjection et particules dis-   VER :pres   verbe au présent
cursives
KON         conjonction                       VER :simp   verbe au passé simple
LOC         locuteur                          VER :subi   verbe au subjonctif impar-
fait
MLT         multi-transcription               VER :subp   verbe au subjonctif présent
NAM         nom propre                        NOM :trc    nom commun tronqué
NOM         nom commun                        NAM :trc    nom propre tronqué
NOM :sig    sigle                             VER :trc    verbe tronqué
NAM :sig    sigle                             ADJ :trc    adjectif tronqué

TABLE 1 – Jeu d’étiquettes du corpus TCOF-POS
104

Nous avons ensuite appliqué les résultats obtenus par Fort et Sagot (2010) sur l’intérêt d’une
pré-annotation avec un outil de qualité moyenne. Ainsi, une fois la première tranche de 10 000
tokens annotés, nous avons ré-entraîné TreeTagger sur ce sous-corpus (mais sans utiliser de
lexique externe) et avons pré-annoté les transcriptions suivantes de Cref (12 240 tokens) avec ce
nouvel outil. La même méthodologie que celle utilisée pour corriger les 10 000 premiers tokens
nous a permis de finaliser le corpus de référence Cref de 22 240 tokens.
3.4 Création du sous-corpus additionnel

Le corpus diffusé est composé pour une part du sous-corpus de référence Cref et pour une autre
part d’un autre sous-corpus additionnel Cadd corrigé par deux étudiantes (de L3 et M2 de Sciences
du Langage) recrutées spécifiquement pour cette tâche. Dans un premier temps, afin d’évaluer a
priori la méthodologie prévue pour l’annotation de Cadd , nous avons mené une campagne de tests
en nous servant de Cref comme référence. Pour ce faire, les deux étudiantes ont eu 15 fichiers
extraits de Cref d’environ 500 tokens chacun 4 à corriger dans un ordre contraint. Les 12 premiers
fichiers avaient été pré-annotés par la version standard de TreeTagger. Afin de mesurer l’impact
de la qualité du pré-annotateur, les 3 derniers fichiers avaient été pré-annotés par une version
de TreeTagger ré-entraînée à partir d’un extrait de 16 312 tokens de la référence Cref (et sans
lexique externe). Naturellement, ces tokens forment un sous-ensemble de Cref disjoint des 15
fichiers à ré-annoter. Les étudiantes avaient l’interdiction d’échanger des informations durant la
phase d’annotation.
La correction a été effectuée dans un tableur, les cellules contenant les étiquettes étant mu-
nies d’une liste déroulante se limitant au jeu d’étiquettes défini ci-dessus. La saisie était donc
contrainte. Une fois la correction terminée, les fichiers annotés en parallèle ont été compa-
rés automatiquement. Les cas de divergence entre les deux annotateurs ont ainsi été repérés
automatiquement et corrigés par un expert 5 .
Dans le cadre de ce travail, les mesures suivantes ont été effectuées :
– le temps mis par les étudiantes pour annoter chaque fichier ;
– la précision de chaque fichier par rapport à la référence ;
– l’accord inter-annotateurs des étudiantes (Kappa de Cohen (Cohen, 1960)) ;
– la précision après fusion et adjudication.
L’évaluation de leurs annotations sur ces 15 fichiers est reproduite ci-dessous (figures 1 et 2 et
tableau 2). Elle tient compte de la lemmatisation et des parties du discours.

1e      2e     3e     4e     5e    6e     7e     8e     9e     10e     11e     12e     13e     14e     15e
107     71     80     67     60    60     57     65     50     50      52      47      32      32      31

TABLE 2 – Temps d’annotation (en minutes)

Entre le 12e et le 13e fichier, la différence de temps est vraisemblablement imputable au change-
ment de pré-annotation par TreeTagger.
4. Cette taille a été retenue car nous avons observé qu’elle permet une correction rapide et une attention soutenue
sans être obligé de s’interrompre en cours d’annotation.
5. Pour des raisons pratiques, il n’a pas été possible de confier cette phase à un expert externe. La personne qui
l’a réalisée a également collaboré à la réalisation du corpus servant de référence, ce qui peut représenter un biais
méthodologique.
105

FIGURE 1 – Évolution de la précision des deux étudiantes
FIGURE 2 – Évolution du Kappa (à gauche) des 2 étudiantes et de la précision après adjudication
(à droite)
La qualité des corrections après ré-entraînement et pré-étiquetage ainsi que le faible temps
de correction (pour les 3 derniers fichiers, donc) nous ont paru suffisants pour valider notre
méthodologie et ainsi poursuivre l’élaboration du corpus. Sur les 3 derniers fichiers, la précision
moyenne est de 98,03 % en ne tenant compte que des étiquettes. Nous sommes donc passés
à l’annotation par les deux étudiantes du corpus Cadd . Elles ont ainsi reçu le même jeu de 160
nouveaux fichiers de 500 tokens chacun, pré-annotés par la version ré-entraînée de TreeTagger.
En 60 heures, elles ont corrigé 80 000 tokens chacune, ce qui fait une moyenne d’un peu plus
de 21 minutes par fichier de 500 tokens. Sur l’ensemble, l’accord inter-annotateurs (Kappa de
Cohen (Cohen, 1960)) est en moyenne de 96,5 % et le temps moyen consacré à l’adjudication de
2 min. 45 sec par fichier.
4    Élargissement de l’évaluation

Afin d’évaluer le caractère robuste de notre méthodologie, nous avons élargi l’évaluation à
plus d’étudiants. En effet, nous comptons augmenter de manière importante la quantité de
fichiers corrigés dans les années à venir et nous voulons vérifier si notre méthodologie donne
des résultats comparables quels que soient les correcteurs. Pour ce faire, nous avons adopté
106

la même méthodologie que celle décrite ci-dessus, à savoir une double-annotation de chaque
fichier puis une adjudication pour les cas de divergence uniquement. Cette évaluation a porté sur
les corrections fournies par 10 étudiants en Sciences du Langage à l’Université Nancy 2 (L3 et
M2) dans le cadre de deux enseignements. A chaque binôme, nous avons donné 6 fichiers (4
fichiers pré-annotés avec le TreeTagger de base et 2 fichiers avec le TreeTagger ré-entraîné) à
corriger dans un ordre contraint. Dans cette expérience, comme dans la précédente, les étudiants
devaient corriger les lemmes en plus des étiquettes. Dans la suite de ce travail, les mesures que
nous présentons tiennent compte à la fois des lemmes et étiquettes (sauf précision contraire).
4.1    Temps d’annotation et accord inter-annotateurs

En ce qui concerne le temps d’annotation, nous avons observé une diminution systématique avec
une nette différence entre les 4 premiers fichiers et les deux derniers (voir tableau 3).

1e annot.    2e annot.    3e annot.    4e annot.    5e annot.    6e annot.
110,1        101,8        79,2         72,5         41,3         39,7

TABLE 3 – Temps d’annotation (en minutes)

Au-delà de la diminution du temps de correction inhérente à une meilleure maîtrise des étudiants,
il paraît difficile d’expliquer la diminution du temps entre le quatrième et le cinquième fichier par
un autre facteur que le basculement entre le TreeTagger standard et la version ré-entraînée. Le
même phénomène peut être observé concernant l’accord inter-annotateurs (cf. figure 3). Le coef-
ficient d’accord inter-annotateurs présenté ici est, comme précédemment, le κ de Cohen (Cohen,
1960).
FIGURE 3 – Évolution de l’accord inter-annotateurs (kappa) des étudiants
Dans la figure 3, la courbe noire représente l’évolution de la moyenne des accords inter-
annotateurs, de même que dans les graphiques suivants.
107

4.2    Précision

Outre une diminution significative du temps d’annotation et une augmentation de l’accord inter-
annotateurs, nous avons également constaté une importante augmentation de la précision en
moyenne pour chaque étudiant (cf. figure 4). On observe encore une fois une nette augmentation
entre le quatrième et le cinquième fichier, et ce chez tous les étudiants. La figure 5 indique la
précision de chaque fichier après fusion et adjudication.
FIGURE 4 – Evolution de la précision de chaque étudiant
FIGURE 5 – Évolution de la précision de chaque fichier corrigé après fusion et adjudication
Finalement, sur les deux derniers fichiers annotés, le taux de précision moyen est respectivement
de 97,28 % et 97,12 % en évaluant les erreurs portant à la fois sur les lemmes et les étiquettes. Si
l’on prend en compte seulement les étiquettes, le taux de précision moyen est alors respectivement
de 97,42 % et de 97,8 % pour ces mêmes fichiers, ce qui est légèrement inférieur à ce qui a
108

été relevé précédemment pour les deux étudiantes. Cependant, nous estimons que cela permet
d’affirmer que les principes adoptés permettent d’obtenir des corpus annotés de qualité proche,
et ce quelle que soit la personne qui corrige.
5      Premiers résultats de l’annotation automatique

Pour effectuer les premiers tests concernant l’apprentissage automatique, notre choix s’est porté
sur deux étiqueteurs morphosyntaxiques : MElt (Denis et Sagot, 2009, 2012), étiqueteur état
de l’art pour le français, et TreeTagger, utilisé comme pré-annotateur pour constituer le corpus,
et largement utilisé bien qu’il ne soit pas celui qui donne les meilleurs résultats à l’heure
actuelle (Denis et Sagot, 2009; Eshkol et al., 2010). Tous deux sont librement disponibles et
multi-plateformes.
Notre objectif était d’étudier la courbe d’apprentissage, et ce sous plusieurs angles : la précision
de l’étiqueteur entraîné augmente-t-elle avec la taille du corpus d’entraînement ? L’utilisation
d’un lexique externe augmente-t-elle de façon significative la précision de l’étiqueteur ? Avec
quelle taille de corpus d’entraînement obtient-on le meilleur étiqueteur ? Quelle est sa précision ?
À partir de quelle taille de corpus d’entraînement l’étiqueteur obtenu peut-il être utilisé comme
pré-annotateur dans une campagne d’annotation manuelle qui consiste en la correction manuelle
de l’annotation automatique ? Les deux systèmes d’étiquetage, MElt et TreeTagger, conduisent-ils
à des résultats similaires concernant les questions précédentes ?
Nous avons donc procédé à l’entraînement de MElt et de TreeTagger sur 10 sous-corpus suc-
cessifs du corpus de référence Cref , dont la taille croît de 2 000 à 20 000 tokens. Nous avions
préalablement mis de côté trois tranches de 500 tokens afin de servir d’échantillons de test. Pour
rendre nos résultats comparables avec ceux présentés dans d’autres campagnes, l’évaluation de
la précision s’est limitée aux seules étiquettes.
Pour l’entraînement de TreeTagger, nous avons utilisé comme lexique externe le lexique Mor-
phalou 2.0 (Romary et al., 2004). Nous avons dû convertir Morphalou au format attendu par
TreeTagger puis le fusionner, pour chacun des 10 sous-corpus d’apprentissage, avec le lexique
qui en est extrait. Nous avons également effectué des tests sans Morphalou, uniquement avec
un lexique endogène. Pour l’entraînement de MElt, nous avons utilisé la version du lexique Lefff
(Sagot, 2010) utilisée pour l’entraînement de la version standard de l’étiqueteur MElt pour le
français. Le lexique externe étant utilisé par MElt comme une source de traits pour le modèle
d’étiquetage, nous avons pu conserver le jeu de catégories du lexique externe bien qu’il soit
différent des catégories du corpus d’entraînement. Pour MElt, le lexique externe reste distinct du
lexique extrait du corpus d’entraînement.
Premier constat : à l’exception du premier étiqueteur entraîné sur 2 000 tokens, les étiqueteurs
obtenus avec MElt sont systématiquement meilleurs que ceux obtenus avec TreeTagger. Les
meilleurs scores, obtenus à partir du corpus de 20 000 tokens, sont respectivement 96,9 % avec
MElt et 94,9 % avec TreeTagger. Comparés aux 85–90 % annoncés par (Eshkol et al., 2010) et
aux 80 % obtenus par A. Dister (c.p. du 24 janvier 2008 6 ), nos résultats constituent donc une
amélioration significative. Mais cela masque des variations d’un échantillon à un autre, ainsi
qu’au niveau de la moyenne (voir figure 6), mais surtout des différences entre jeux d’étiquettes.

6. Diaporama disponible à l’adresse : http ://rhapsodie.risc.cnrs.fr/docs/Dister_Syntaxe_240108.pdf
109

Deuxième constat, sans surprise : l’utilisation du lexique externe améliore la précision de l’étique-
teur. Par exemple, sur le corpus de 2 000 tokens, TreeTagger n’atteint que 78,4 % de précision
sans lexique externe contre 90,9 % avec Morphalou. Pour ce même corpus, la différence est
moins importante avec MElt, mais elle est significative : la précision passe de 84,9 % à 88,1 %.
Avec 20 000 tokens, l’utilisation du lexique externe permet à la précision de l’étiqueteur entraîné
par MElt de passer de 95,5 % à 96,9 %. On note que MElt, sans lexique externe, donne des
résultats supérieurs à TreeTagger avec lexique externe dès que le corpus d’entraînement fait plus
de 12 000 tokens.
FIGURE 6 – Évolution de la précision de l’annotation automatique par tranche de 2 000 tokens

La figure 6 montre qu’à partir de 6 000 tokens, les résultats commencent à progresser de manière
moins marquée, que ce soit avec MElt ou TreeTagger. Les précisions obtenues avec cette taille
de corpus (94,3% avec MElt) sont suffisantes pour lancer une campagne de correction telle
que nous la décrivons ci-dessus, après ré-entraînement. Il n’est pas indispensable que le corpus
d’apprentissage soit plus volumineux. En tout cas, au vu de nos résultats, il n’est pas utile
d’aller au-delà de 10 000 tokens si l’on utilise TreeTagger. L’utilisation de MElt semble toutefois
préférable, avec des résultats qui continuent à croître jusqu’à 20 000 tokens.
6      Conclusion et perspectives

Le corpus TCOF-POS (Cref + Cadd ) est disponible sur le site du CNRTL 7 sous licence Creative
Commons BY-NC-SA 2.0 8 , héritée du corpus TCOF. Il contient un peu plus de 100 000 tokens,
dont un peu plus de 20 000 tokens de référence et 80 000 tokens obtenus grâce à la double-
annotation (par les deux étudiantes recrutées) puis adjudication par un expert linguiste (C.
7. http ://cnrtl.fr/corpus/perceo/
8. http ://creativecommons.org/licenses/by-nc-sa/2.0/fr/
110

Benzitoun). Les meilleurs modèles d’étiquetage pour TreeTagger et pour MElt, qui ont une
précision respectivement de 94,9 % et 96,9 % à ce stade de développement du corpus, seront
également mis à disposition sous peu sur ce site (pour l’instant, seul le fichier paramètre pour
TreeTagger est téléchargeable). Le lexique fusionné avec Morphalou est également disponible à
cette même adresse. La version ré-entraînée de TreeTagger a déjà été utilisée par les concepteurs
du Corpus de Français Parlé Parisien 9 pour annoter leurs données.
Dans le cadre d’une campagne de correction d’une pré-annotation automatique, nous avons mis
en évidence le seuil de 6 000 tokens comme base de départ minimale pour ré-entraîner le logiciel.
A ce stade, on obtient de bons résultats (94,3 % pour MElt et 93,6 % pour TreeTagger) et la
précision progresse de manière moins marquée. Mais cette recommandation est valable lorsque
le logiciel d’étiquetage est couplé à un lexique externe. Or, dans le cadre de notre campagne
d’évaluation des corrections manuelles, nous n’avons pas utilisé de lexique externe pour ré-
entraîner TreeTagger. Il faudra donc tester si les résultats sont de meilleure qualité lorsque l’on
ajoute ce paramètre, travail que nous effectuons à l’heure actuelle.
Remerciements

Nous tenons à remercier les 12 étudiants ayant collaboré à ce projet et plus particulièrement
M. Salcedo et M. Paquot, recrutées spécifiquement pour faire l’annotation. De même, L. Bérard,
E. Jacquey, V. Meslard, S. Ollinger et E. Petitjean ont apporté une contribution significative à ce
projet. Nous souhaitons également remercier l’ATILF pour son soutien financier dans le cadre d’un
projet interne. La participation de K. Fort a été financée dans le cadre du programme Quæro 10 ,
financé par OSEO, agence nationale de valorisation de la recherche. Celle de B. Sagot entre dans
le cadre du projet ANR EDyLex (ANR-09-CORD-008).
Références
ABEILLÉ, A. et CLÉMENT, L. (2006). Annotation morpho-syntaxique. Les mots simples - Les mots
composés Corpus Le Monde.
ANDRÉ, V. et CANUT, E. (2010). Mise à disposition de corpus oraux interactifs : le projet TCOF
(traitement de corpus oraux en francais). Pratiques, 147/148:35–51.
BENZITOUN, C. (2004). L’annotation syntaxique de corpus oraux constitue-t-elle un problème
spécifique ? In Actes de la conférence RECITAL, pages 13–22, Fès, Maroc.
BLANC, O., CONSTANT, M., DISTER, A. et WATRIN, P. (2008). Corpus oraux et chunking. In Journées
d’étude sur la parole (JEP), Avignon, France.
BLANCHE-BENVENISTE, C. et JEANJEAN, C. (1987). Le Francais parlé. Transcription et édition. Didier
Érudition, Paris, France.
B ONNEAU-MAYNARD, H., ROSSET, S., AYACHE, C., KUHN, A. et MOSTEFA, D. (2005). Semantic
Annotation of the French Media Dialog Corpus. In InterSpeech, Lisbonne, Portugal.

9. http ://cfpp2000.univ-paris3.fr/search-transcription-tt/
10. ❤tt♣✿✴✴✇✇✇✳q✉❛❡r♦✳♦r❣
111

BRANCA-ROSOFF, S., FLEURY, S., LEFEUVRE, F. et PIRES, M. (2010). Discours sur la ville. corpus de
francais parlé parisien des années 2000 (CFPP2000). Rapport technique.
CAMPIONE, E., VÉRONIS, J. et DEULOFEU, J. (2005). C-ORAL-ROM, Integrated Reference Corpora for
Spoken Romance Languages, édité par E. Cresti et M. Moneglia, chapitre 3. The French corpus,
pages 111–133. John Benjamins, Amsterdam, Hollande.
COHEN, J. (1960). A Coefficient of Agreement for Nominal Scales. Educational and Psychological
Measurement, 20(1):37–46.
DENIS, P. et SAGOT, B. (2009). Coupling an annotated corpus and a morphosyntactic lexicon for
state-of-the-art pos tagging with less human effort. In Proceedings of PACLIC 2009, Hong Kong,
Chine.
DENIS, P. et SAGOT, B. (2012). Coupling an annotated corpus and a lexicon for state-of-the-art
POS tagging. Language Resources and Evaluation. À paraître.
DISTER, A. (2007). De la transcription à l’étiquetage morphosyntaxique. Le cas de la banque de
données textuelle orale VALIBEL. Thèse de doctorat, Université de Louvain, Belgique.
ESHKOL, I., TELLIER, I., TAALAB, S. et BILLOT, S. (2010). étiqueter un corpus oral par apprentissage
automatique à l’aide de connaissances linguistiques. In 10th International Conference on
statistical analysis of textual data (JADT 2010), Rome, Italie.
FORT, K. et SAGOT, B. (2010). Influence of Pre-annotation on POS-tagged Corpus Development.
In Proc. of the Fourth ACL Linguistic Annotation Workshop, Uppsala, Suède.
HUET, S., GRAVIER, G. et SÉBILLOT, P. (2006). Peut-on utiliser les étiqueteurs morphosyntaxiques
pour améliorer la transcription automatique. In Actes des 26èmes Journées d’Études sur la Parole
(JEP), Dinard, France.
MARCUS, M., SANTORINI, B. et MARCINKIEWICZ, M. A. (1993). Building a large annotated corpus
of english : The penn treebank. Computational Linguistics, 19(2):313–330.
MERTENS, P. (2002). Les corpus de francais parlé ELICOP : consultation et exploitation. In
BINON, J., DESMET, P., ELEN, J., MERTENS, P. et SERCU, L., éditeurs : Tableaux Vivants. Opstellen
over taal-en-onderwijs aangeboden aan Mark Debrock, pages 101–116. Universitaire Pers, Leuven,
Belgique.
ROMARY, L., SALMON-ALT, S. et FRANCOPOULO, G. (2004). Standards going concrete : from LMF
to Morphalou. In Workshop on Electronic DictionariesWorkshop on Electronic Dictionaries, Coling
2004, Genève, Suisse.
SAGOT, B. (2010). The Lefff, a freely available and large-coverage morphological and syntactic
lexicon for french. In 7th international conference on Language Resources and Evaluation (LREC
2010), La Vallette, Malte.
SCHMID, H. (1997). New Methods in Language Processing, Studies in Computational Linguistics,
édité par D. Jones et H. Somers, chapitre Probabilistic part-of-speech tagging using decision
trees, pages 154–164. UCL Press, Londres.
VALLI, A. et VÉRONIS, J. (1999). étiquetage grammatical de corpus oraux : problèmes et
perspectives. Revue Francaise de Linguistique Appliquée, IV(2):113–133.
VOORMANN, H. et GUT, U. (2008). Agile corpus creation. Corpus Linguistics and Linguistic Theory,
4(2):235–251.
112

La Charte Éthique et Big Data :
pour des ressources pour le TAL (enfin !)
traçables et pérennes

Karën Fort1,2,3       Alain Couillault4,5
(1) Université de Lorraine
(2) LORIA 54500 Vandœuvre-lès-Nancy
(3) ATALA
(4) Université de La Rochelle
(5) APROGED
karen.fort@loria.fr,alain.couillault@univ-lr.fr

RÉSUMÉ
La charte Ethique & Big Data a été conçue à l’initiative de l’ATALA 1 , de l’AFCP 2 , de l’APROGED 3
et de CAP DIGITAL 4 , au sein d’un groupe de travail mixte réunissant d’autres partenaires
académiques et industriels (tels que le CERSA-CNRS, Digital Ethics, Eptica-Lingway, le cabinet
Itéanu ou ELRA/ELDA). Elle se donne comme objectif de fournir des garanties concernant la
traçabilité des données (notamment des ressources langagières), leur qualité et leur impact
sur l’emploi. Cette charte a été adoptée par Cap Digital (co-rédacteur). Nous avons également
proposé à la DGLFLF et à l’ANR de l’utiliser. Elle est aujourd’hui disponible sous forme de wiki 5 ,
de fichier pdf et il en existe une version en anglais. La charte est décrite en détails dans (Couillault
et Fort, 2013).
ABSTRACT
The Ethics & Big Data Charter : for tractable and lasting NLP resources
The Ethics & Big Data Charter was designed by ATALA, AFCP, APROGED and CAP DIGITAL, in a
working group including other academic and industrial partners (such as CERSA-CNRS, Digital
Ethics, Eptica-Lingway, Itéanu office or ELRA/ELDA). Its aims at ensuring the traceability and
quality of the data (including language resources), how they are produced and their impact on
working conditions. This charter has been adopted by Cap Digital (co-writer). We also proposed
it to DGLFLF and ANR. As of today, it is available as a wiki, a pdf file and an English version 6 .
The charter is detailled in (Couillault et Fort, 2013).

MOTS-CLÉS : éthique, big data, ressources langagières.
KEYWORDS: ethics, big data, language resources.
1.   http://www.atala.org/
2.   http://www.afcp-parole.org/
3.   http://www.aproged.org/
4.   http://www.capdigital.com/
5.   http://wiki.ethique-big-data.org
6.   http://wiki.ethique-big-data.org/index.php?title=Accueil#English_Version
Références
COUILLAULT, A. et FORT, K. (2013). Charte éthique et big data : parce que mon corpus le vaut
bien ! In Actes de colloque international Corpus et Outils en Linguistique, Langues et Parole :
Statuts, Usages et Mésusages, Strasbourg, France.
TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

ProL_MF version 1.2.
Une ressource llbre de noms ro res avec
des expansmns conte e1 es

Denis Maurel, Beatrice Bouchou Markhoff
Université Francois Rabelais Tours
denis . maurel@univ—tours . fr, beatrice . bouchou@univ—tours . fr

RESUME
ProLMF est la version LMF de la base lexicale multilingue de noms propres Prolexbase.
Disponible librement sur le site du CNRTL, la version 1.2 a été largement améliorée et
augmentée par de nouvelles entrées en francais, complétées par des expansions
contextuelles, et par de petits lexiques en une huitaine de langues.

ABSTRACT
ProLMF 1.2, Proper Names with their Expansions

ProLMF is the LMF version of Prolexbase, a multilingual lexical database of Proper Names. It
can be freely downloaded on the CNRTL Website. Version 1.2 had been widely improved and
increased, with new French entries whose description includes contextual expansions, and
eight small lexica for other languages.
MOTS-CLES: ressource libre, base lexicale multilingue, noms propres, expansions
contextuelles, schémas de contextualisation, relations sémantiques, alias, point de vue,
Prolexbase.

KEYWORDS: free resource, multilingual lexical database, Proper Names, context, semantic
relations, alias, point of view, Prolexbase.
1 Les bases de données lexicales

Parmi les ressources utilisées en TAL, les bases de données lexicales occupent une place
importante, souvent a l'origine d'applications diverses. Citons entre autres Wordnet (Miller
et al., 1990), les dictionnaires Dela (Courtois, Silberztein, 1990), le lexique Morphalou
(Romary et al., 2004), le projet Papillon (Mangeot—Lerebours et al., 2003), etc. D'autres
ressources libres comme Wikipédia, DBpedia (Auer, Lehmann, 2007), Geonames, Yago 2
(Hoffart et al., 2012), etc., sont structurées autour des entrées lexicales, qu'elles décrivent
avec éventuellement des relations paradigmatiques, mais sans informations linguistiques.

Prolexbase (Tran et Maurel, 2006) a la particularité de rassembler des noms propres, en
s'intéressant aussi a la morphologie ﬂexionnelle et dérivationnelle de ces noms. Une
premiere version de ProLMF (Bouchou et Maurel, 2008) a été déposée en 2008 sur le site
Prolex1 du CNRTL (Centre national de ressources textuelles et linguistiques), sous une
licence libre. Les concepts les plus importants de Prolexbase sont ceux de point de VH8 sur 1111
référent et de proIeXéme. Le premier concept, interlingue, matérialisé par un pivot, signiﬁe
que des entrées de Prolexbase peuvent correspondre dans la réalité a un méme référent, s'il
s'agit de points de vue différents sur celui-ci. Prenons l'exemple récent du pape Franpois et
1htt : www.cnrtl.fr lexi ues rolex .
TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

du cardinal large Bergoglio: ces deux noms propres correspondraient a deux pivots
différents. Le second concept est un ensemble de formes morphosémantiquement (Fellbaum
et Miller, 2003) liés a la projection du pivot dans une langue. En francais, cet ensemble
comprend en général le nom propre lui-méme, parfois une forme courte ou un acronyme,
souvent un nom et un adjectif relationnels, ces derniers étant les seuls a se ﬂéchirz. Dans
d'autres langues, ou la morphologie ﬂexionnelle et/ou dérivationnelle est plus développée,
ce prolexeme peut comprendre un grand nombre de lemmes et de formes. Les pivots sont
reliés entre eux par trois relations: la synonymie, la méronymie et l'accessibilité (voir
section 2.4).

2 Présentation générale de ProLMF
2.1 La norme LMF

La norme LMF (ISO 24613:2008) pour Lenka] Markup Framework est un meta modele de
représentation des données lexicales (Francopoulo et al., 2006). LMF permet la
représentation de bases tres différentes dans leurs conceptions, de la simple liste de mots
aux bases morphologiques, sémantiques, multilingues, etc. Elle est composée d'un module
central (le care package) et d'extensions. Le module central, obligatoire, contient les
informations générales (par exemple le codage des caracteres), le lemme et, facultativement,
une ou des formes, un ou des sens. Les extensions permettent de traiter la syntaxe, la
sémantique, la morphologie, le multilinguisme, etc. Les attributs de chaque classe respectent,
dans la mesure du possible, le registre des Data categories (Francopoulo et al., 2008). La
Figure 1 présente les classes utilisées par ProLMF; les classes grisées correspondent a la
partie multilingue.

ProLMF 1.2 comporte :

— un lexique francais avec lemme, forme et sens pour chaque entrée lexicale, ainsi que
des schémas de contextualisation ;

— quelques petits lexiques (allemand, anglais, italien, néerlandais, polonais, portugais
et serbe) avec uniquement lemme et sens ;

— et une description au niveau multilingue avec des informations typologiques et,
surtout, des relations entre pivots (synonymie, méronymie et accessibilité).

2.2 Les informations globales

Les informations globales indiquent que ProLMF respecte la norme ISO 639 pour le codage
des noms de langues sur trois lettres3 et la norme ISO 15924 pour le codage des noms
d'écriture sur quatre lettres4 ; elles indiquent aussi que le codage des caracteres est implanté
en UTF-8.
2 Par exemple 1e pivot 38558 correspond en francais a cinq lemmes et a un ensemble de dix-sept formes (Paris,
Parisien, Parisienne, Parisiens, Parisiennes, parisien, parisienne, parisiens, parisiennes, Parigot, Parigobe, Parigots,
parigobes, parigot, parigobe, parigots, parigobes}, qui ne contient pas parisianisme, pourtant bien dérivé
morphologiquement de Paris, mais dont 1e sens est lexicalisé.

3 C'est-a-dire respectivement: dell, eng, fra, ita, nld, p01, p01; spa et srp.

4 C'est-a-dire 1am pour Iatz'n et cyrl pour Lyrillique.
TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

<LexicalRessource>
<Globallnformation languageCoding="ISO 639" scriptCoding="ISO 15924" characterCoding="UTF-8"
entrySource:"Prolexbase" resourceName="ProLMF" version="1.2"/>
Global Information : Lexical Resource
1
—<>
1..*
Lexicon
w Subcategorization
0 Frame
HT 0..
Lemma _<> Lexical Entry 0--*
1 o  Syntactic Behaviour
°--* 0.3 ofF o..*
(_
Word Form Sense 0 * 0 * Sense Axis
1 1
0..* 0..* 0..*
°--* CIT o..* WT o..*
Form Representation Sense Relation Sense Axis Relation
0..* 0..*
Monolingual External Rel Interlingual External Ref
FIGURE 1 — Les classes utilisées par ProLMF.

Chaque lexique comporte comme attribut son code de langue et son code d'écriture. Lorsque
deuX écritures sont utilisées, comme par exemple en serbe (latin et cyrillique), il faudrait en
principe distinguer ces écritures a l'intérieur de chaque forme. La version 1.2 de ProLMF
n'implante pas cette solution. A titre transitoire, nous avons créé deuX lexiques serbes, un en
latin et l'autre en cyrillique.

<Lexicon languageldentiﬁer="deu" script: "latn">
</Lexicon>
<Lexicon languageldentiﬁer="srp" script="cyrl">

/Lexicon>
2.3 Les entrées lexicales

Une entrée lexicale correspond a une seule partie du discours et comporte un lemme, une ou
TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

des formes et un ou des sens. Comme cela a été dit plus haut, pour les langues autres que le
francais, la partie forme n'est pas renseignée. Sinon, chaque forme comprend son genre et
son nombre.

Par exemple, la ville de Paris en serbe a pour lemme Pariz:

<LexicalEntry partOfSpeech="noun">

<Lemma> Pariz</Lemma>

<Sense idSense="P400" relSenseAxis="38558" termProvenance: "fullForm" label="properName"/>
</LexicalEniry>

Dans la norme LMF, les entrées sont regroupées par catégories du discours, lemmes et
formes ; les sens peuvent différer. Pour ProLMF, lorsqu'il y a plusieurs sens, il s'agit
d'homographes. Par exemple, l'adjectif relationnel neuille’en correspond dans la base a deux
villes : NeuillJI-lfvéque (rarelyUsed !) —pivot: 13346- et, bien sﬁr, Neuilly-sur—Seine —pivot:
18220-.

<LexicalEntry partOfSpeech="adjective">
<Lemma> neuilléen</Lemma>
<WordForm grammaticalGender:"masculine" grammaticalNumber: "singular">neuilléen</WordForm>
<WordForm grammaticalGender:"masculine" grammaticalNumber: "plural">neuilléens</WordForm>
<WordForm grammaticalGender:"feminine" grammaticalNumber:"singular">neuilléenne</WordForm>
<WordForm grammaticalGender: "feminine" grammaticalNumber: "plural" >neuilléennes </WordForm>
<Sense idSense="D82 33" refSenseAxis="13346" termProvenance: "relationalAdjective"
frequency: "rarelyUsed" label="derivative" relSense="P13346"/>
<Sense idSense="D11433" refSenseAxis="1822 0" termProvenance:"relationalAdjeciive"
frequency: "infrequentlyUsed" label="derivative" refSense="P1822 0"/>
</LexicalEniry>

Le sens comprend jusqu'a six attributs: un identiﬁant (idSense), la référence au pivot
multilingue (refSenseAXis), éventuellement la catégorie d'alias 5 ou de dérivé 6
(termProvenance), la notoriété7 (ﬁ‘equencﬂ, l'indication s'il s'agit d'un nom propre ou d'un
dérivé (label) et, dans ce dernier cas, la référence au sens dont il est dérivé (refSense)8.

Dans le cas d'un nom propre, chaque sens peut étre associé a un ou des contextes; ces
contextes sont décrits dans le lexique correspondant, au meme niveau que les entrées
lexicales (voir section 2.5).

2.4 La partie multilingue

Comme cela a été rappelé en section 1, la partie multilingue comprend un pivot par « point
de vue » sur un nom propre. Par exemple, on distinguera un pivot pour Paris —pivot: 38558-
et un autre pour Ville lumiére —pivot: 55120-, meme si le référent, la ville de Paris est le
méme. Ces deux pivots seront en relation de synonymie diaphasique9. DeuX autres
5 Par exemple : fullForm, sliortForm, acronym...

6 Par exemple : relational/l djective, relationalName, quasiRelationalName...

7 Suivant les data categories, l'attribut de notoriété prend trois valeurs : rarelyl/sed, infrequentlyl/sed et
cammonlyl/sed.

8 En francais, c'est en général le nom propre an un alias, comme 0nusien, dérivé de 01111 et non d'0rganisation des
Nations Unis. Dans des langues a forte productivité dérivationnelle, comme 1e serbe, cet attribut est beaucoup plus
diversiﬁé.

9 Nous utilisons les traits déﬁnis par (Coseriu, 1998) : diapliasique (variation d'emploi), diachronique (variation
dans le temps), diatopique (variation dans l'espace) et diastratique (variation socio-culturelle).
TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

relations10 existent: la méronymie11 (les Champs-Elysées —pivot: 492 15- est une avenue
parisienne) et l'accessibilité12 (Paris est la capitale de la France —pivot: 27-). Ces pivots sont
associés a la typologie des noms propres du projet Prolex et a un paradigme d'existence13 :
<SenseAxis id="38558">
<InterlingualExternalRef externalSystem="typology" externalReference:"city"/>
<InterlingualExternalRef externalSystem="existence" externalReference:"historical"/>
<SenseAxisRelation label: "partitiveRelation" refSenseAxis="49215"/>
<SenseAxisRelation label: "quasiSynonym" refSenseAxis="55120" usageNote="diaphasic"/>
<SenseAxisRelation id="1" label:"associativeRelation" refSenseAxis="27" subjectField="capital"/>
</SenseAxis>

2.5 Les régles d'aliasisation

Dans la version 1.2, un grand nombre d'alias ont été ajoutés par des regles d'aliasisation,
pour permettre la création automatique de formes courtes14. Par exemple, le prolexeme
Wolfgang Amadeus Mozart est complété par l'alias Wolfgang Mozart et les noms de ville
construits avec une préposition et un toponyme, comme NeHI'IIy sur Seine, sont pour une
grande part associés a une forme courte sans complément prépositionnel, comme NeuiIIy.

3 Les expansions contextuelles

La nouveauté la plus importante de ProLMF 1.2 est l'introduction de regles d'expansion
contextuelle. Celles-ci peuvent se diviser en trois categories :

— La présence éventuelle d'un determinant (la France) et le choix de la préposition
locative pour les noms de pays (en France).

— l'expansion classiﬁante (la Ville de Paris)

— l'expansion d'accessibilité (Paris, 1a capitaIe de la France)

Certains sens sont aussi complétés par un lien vers Wikipédia (classe
MonolingualEXternalRef).

3.1 Determinants et préposiﬁons locatives

Les noms propres en francais sont parfois précédés d'un déterminant. Nous avons noté cette
information en indiquant de quel determinant il s'agit Dans le cas particulier des noms de
pays, nous avons indiqué aussi la préposition locative a utiliser. Par exemple France est en
général précédé de l'article 1a et s'utilise avec la préposition en (contrairement a Portugal,
par exemple). Cette indication se trouve dans le sens associé a l'entrée lexicale France :
1° Toutes les relations ne sont bien sﬁr notée qu'une fois, sur un seul des deux pivots.

11 Celle-ci comprend les relations classiques (lieux et événements), mais nous l'avons aussi étendue aux ﬁliales
d'entreprises, 51a nationalité des personnes, etc.

12 Dans un « dictionnaire de noms propres », un nom propre est accessible via un autre nom propre et non via une
définition. L'accessibilité comporte volontairement dans Prolexbase douze repérages (subjectFI'Ie) trés larges :
relative, create]; capital" Ces repérages seront démulu'pliés dans chaque langue par les contexbes d'accessibilité
(section 3).

13 La typologie Prolex est volontairement réduite a trente types et supertypes. Celui-ci comprend trois valeurs.

14 L'application de ces régles est bien sﬁr supervisée, comme route la base. Dans ProLMF, ces alias ne sont pas
distingués des alias entrés manuellement, mais cette information est dans Prolexbase, ainsi que la régle appliquée.
TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

<LexicalEntry partOfSpeech="noun">
<Lemma> France</Lemma>
<WordForm grammaticalGender:"feminine" grammaticalNumber:"singular">France</WordForm>
<Sense idSense="P2 7" relSenseAxis="27" termProvenance:"fullForm" frequency:"commonlyUsed"
label="properName">
<SyntacticBehaviour relSubcategorizationFrame: "CO 3 "/>
<SyntacticBehaviour relSubcategorizationFrame: "CO 7" / >
<MonolingualEx‘rernalRef externalSystem="Wikipedia"
externalReference:"http://fr.wikipediaorg/wiki/France"/>
</Sense>
</LexicalEniry>

Les balises SyntacticBeIzaViour font référence a des regles de sous-categorisation, elles aussi
décrites dans le lexique, apres les entrées lexicales :

<SubcategorizationFrame id="C03" introducer:"Determiner">la</SubcategorizationFrame>
<SubcategorizationFrame id="CO7" introducer:"locativePreposition">en</SubcategorizaﬁonFrame>

Cette relation s'applique a tous les noms propres de la base, prolexemes et alias.

3.2 Les expansions

La relation d'expansion classiﬁante associe a un prolexeme une expansion qui peut
apparaitre, soit a sa gauche, soit a sa droite. Toutes les expansions qui existent dans une
langue ne se retrouvent pas forcément dans une autre langue. Si l'expansion d'un nom
propre est omise dans un texte, il est parfois nécessaire de la rétablir lors de la traduction de
celui-ci, aﬁn d'apporter un complément d'information au lecteur. Ainsi, le nom propre la
Loire devient en anglais the Loire River. Dans la version 1.2, un grand nombre de prolexemes
sont liés a des expansions, comme des précisions toponymiques (ville, riviere, aéroport...),
des professions (acteur, industriel, compositeur...) ou autres (archange, cité légendaire,
féte...).

La relation d'expansion d'accessibilité est la projection dans une langue de la relation
d'accessibilité sur les pivots interlingues. Comme cela a été rappelé ci-dessus, la relation
d'accessibilité comprend une indication tres large de repérage (capitale, parent, créateur...)
qui correspond a diverses informations (pere/frere, auteur/compositeur...). Ces formes sont
parfois différentes d'une langue a l'autre et d'un mot a l'autre (par exemple, le repérage
capitale donnera en francais capitale ou chef]1'eu,suivantle nom propre considéré .

Par exemple, Paris —pivot: 38558- a pour expansion classiﬁante la Ville de et pour expansion
d'accessibilité Ia capitale de, toutes deux féminin et singulier :

<LexicalEntry partOfSpeech="noun">
<Lemma> Paris</Lemma>
<WordForm grammaticalGender:"masculineFeminine"
grammaticalNumber:"singular"> Paris </WordForm>
<Sense idSense="P38558" relSenseAxis="38558" termProvenance:"fullForm" frequency:"commonlyUsed"
label="properName">
<SyntacticBehaviour relSubcategorizationFrame:"CC222 "/>
<SyntacticBehaviour relSenseAxis Relation="1" refSubcategorizau'onFrame:"AC4"/>
<SyntacticBehaviour relSubcategorizationFrame: "CO 1 "/>
<MonolingualEx‘rernalRef externalSystem="Wikipedia"
externalReference:"http://fr.wikipediaorg/wiki/Paris"/>
</Sense>
</LexicalEniry>
TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Avec les descriptions suivantes :

<SubcategorizationFrame id="C01" introducer:"Determiner">zero</SubcategorizaﬁonFrame>

<SubcategorizationFrame id="CC222" introducer:"classifyingContext" grammaticalGender:"feminine"
grammaticalNumber:"singular">la ville de </SubcategorizationFrame>
<SubcategorizationFrame id="AC4" introducer:"accessibilityContext" grammaticalGender:"feminine"
grammaticalNumber:"singular">la capitale de la </SubcategorizationFrame>

4- Conclusion

Nous avons présenté dans cet article ProLMF 1.2 en détaillant les différences signiﬁcatives
avec la version 1.1. Cette ressource est disponible sur le site Prolex15 du CNRTL (Centre

national de ressources textuelles et linguistiques) sous une licence
d'un schéma XML. Le tableau 1 indique le nombre d'entrées pour chaque langue.
ProLMF 1.2
73 029 Entrées lexicales16
10 Déterminants et prépositions locatives

fra

228 Expansions classiﬁantes

101 Expansions d'accessibilité
deu 1 124
eng 790
ita 751
nld 683
pol 8 236 Entrées lexicales (Lemmes)
por 523
spa 741
srp-latn 1355
srp-cyrl 980
TABLE 1 — ProLMF 1.2 en chiffres
15 http: ( (www.cnrtlfr (lexigues (prolexl.

1" Dont 3 267 entrées lexicales obtenues par des régles d'aliasisation.

529

LGPL-LR accompagnée

© ATALA

TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
Nous avons comme perspective pour la poursuite de ce travail :

— le complément des liens vers Wikipédia et l'introduction de liens vers Geonames ;

— la mise en ligne sur le site du CNRTL d'une version 2.1 de ProLMF avec un nombre
important d'entrées lexicales en anglais et en polonais ;

— l'ajout de la langue arabe (a plus long terme).

Remerciements

Nous remercions vivement ceux qui, apres téléchargement de ProLMF 1.1, ont pris la peine
de nous signaler des erreurs et des suggestions. Tout particulierement Pascal Malaise, Karen
Fort et Benoit Sagot Nous remercions aussi Malgorzata Spedzia qui a créé le module
polonais de la version 1.2.

Références
AUER S., LEHMANN]. (2007). What have Innsbruck and Leipzig in common? Extracting

Semantics from Wiki Content ESWC 2007. LNCS 4519:503-517.

BOUCHOU B., MAUREL D. (2008), Prolexbase et LMF: vers un standard pour les ressources
lexicales sur les noms propres, Traitement automatique des langues, 49(1):61-88.

COSERIU E. (1998), Le double probleme des unités dia-s, Les Cabiers 6m: Etudes sur la
diaclimm'e etIa variation 11'11gu1'st1'que 1:9-16.

COURTOIS B., SILBERZTEIN M. (1990), Dictionnaires électroniques du francais, Langues
ﬁ‘anpaise, 87:11-22.

FELLBAUM C., MILLER G. A. (2003), Morphosemantic Links in WordNet, TAL, 44(2):69 —80.

FRANCOPOULO G., MONTE G., CALZOLARI N., MONACHINI M., BEL N., PET M., SORIA C. (2006), Lexical
Markup Framework (LMF), LREC 2006.

FRANCOPOULO G., DECLERCK T., SORNLERTLAMVANICH V., DE LA CLERGERIE E., MONACHINI M. (2008),
Data Category Registry: morpho-syntactic and syntactic proﬁles, Workshop Uses and usage
oflanguage resource-related standards (LRE C ’2008), 3 1 - 3 9.

HOFFART ]., SUCHANEK F. M., BERBERICH K., WEIKUM G. (2012). YAGOZ: A Spatially and
Temporally Enhanced Knowledge Base from Wikipedia. Artiﬁcial Intelligence ]ournal, Special
Issue on Artiﬁcial Intelligence, Wikipedia and Semi-Structured Resources.

MANGEOT-LEREBOURS M., SERASSET G., LAFOURCADE M. (2003), Construction collaborative d'une
base lexicale multilingue, le projet Papillon, TAL, 44-2:1 51-1 76.

MILLER G., BECKWITH R., FELLBAUM C., GROSS D., MILLER K. (1990), Introduction to WordNet: an
on-line lexical database, International journal ofLeXicogTapIJ y, 3:2 35-2 44.

ROMARY L., SALMON-ALT S. FRANCOPOULO G. (2004), Standards going concrete: from LMF to
Morphalou, in Workshop on Electronic Dictionaries, COLING-04.

TRAN M., MAUREL D. (2006), Prolexbase: Un dictionnaire relationnel multilingue de noms
propres, Traitementautomatique des langues, Vol. 47( 3 ):1 15-139.

TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

ANCOR, premier corpus de francais arlé
d’envergure annoté en coréférence et istribué
librement

Iudith Muzerellel, Anai's Lefeuvrez, Iean-Yves Antoinez, Emmanuel Schangl, Denis
Maurelz, Ieanne Villaneau3, Iris Eshkol1

(1) LLL Orléans, Université d’Orléans
(2) Université Francois Rabelais Tours, LI, 3 place Jean Iaurés, 41000 Blois
(3) IRISA, Université Européenne de Bretagne, 56100 Lorient

Jean—Yves .Antoine@univ—tour . fr, Emmanuel . Schang@univ—orleans . fr,
Jeanne . Villaneau@univ—ubs . fr

RESUME
Cet article présente la réalisation d'ANCOR, qui constitue par son envergure (453 000
mots) le premier corpus francophone annoté en anaphores et coréférences permettant le
développement d’approches centrées sur les données pour la résolution des anaphores et
autres traitements de la coréférence. L’annotation a été réalisée sur trois corpus de parole
conversationnelle (Accueil_UBS, OTG et ESLO) qui le destinent plus particuliérement au
traitement du langage parlé. En l’absence d'équivalent pour le langage écrit, il est toutefois
susceptible d'intéresser l'ensemble de la communauté TAL. Par ailleurs, le schéma
d’annotation retenu est sufﬁsamment riche pour permettre des études en linguistique de
corpus. Le corpus sera diffusé librement a la mi-2 013 sous licence Creative Commons BY-
NC-SA. Cet article se concentre sur sa mise en oeuvre et décrit briévement quelques
résultats obtenus sur la partie déja annotée de la ressource.

ABSTRACT
ANCOR. the first large French speaking corpus of conversational speech
annotated in coreference to be freely available.

This paper presents the ﬁrst French spoken corpus annotated in coreference whose size
(453,000 words) is sufﬁcient to investigate the achievement of data oriented systems of
coreference resolution. The annotation was conducted on three different corpora of
conversational speech (Accueil_UBS, OTG, ESLO) but this resource can also be interesting
for NLP researchers working on written language, considering the lack of a large written
French corpus annotated in coreference. We followed a rich annotation scheme which
enables also research motivated by linguistic considerations. This corpus will be freely
available (Creative Commons BY-NC-SA) around mid-2 013. The paper details the
achievement of the resource as well as preliminary experiments conducted on the part of
the corpus already annotated.
MOTS-CLES : Corpus, annotation, coréférence, anaphore, parole conversationnelle

KEYWORDS : Corpus, annotation, coreference, anaphora, conversational speech
TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
1 Introduction

Au cours des deux dernieres décennies, le TAL a connu des avancées qui ont conduit a de
nombreuses applications dédiées au grand public comme aux professionnels. Parmi celles-
ci, la recherche d'information et l'indexation de documents constituent un champ applicatif
promis a un bel avenir. La qualité des outils d’indexation ou d’interrogation développés
pour ces taches dépend de leur capacité a résoudre les relations de coréférences présentes
dans un (ou plusieurs) texte(s). Un des sauts technologiques auquel est confronté ce
domaine est en effet celui du suivi des entités faisant l’ objet d’ une recherche ou
indexation.
L’importance de cette résolution a conduit a l’organisation de multiples campagnes
d’évaluation internationales (MUC, ACE, SemEval) ou nationales (DEFT) qui ont
accompagné l'évolution des techniques de résolution. Aux travaux initiaux basés sur des
méthodes symboliques (Lappin et Leass, 1994) ont succédé des approches plus
heuristiques (Mitkov, 1994). Enﬁn, la mise a disposition de larges corpus annotés en
coréférence a ouvert la porte aux approches par apprentissage sur données (Soon et al.,
2001, Ng et Cardie, 2002). La plupart des recherches actuelles font ainsi appel a des
classiﬁeurs reposant généralement sur un modele a base de paires (Poesio et al., 2011).
Celui-ci consiste a identifier dans un premier temps des paires de mentions coréférentes, et
a regrouper ensuite ces paires en clusters de méme référence. Il n’existe toutefois pas a
l’heure actuelle de corpus francophone libre et d'envergure de taille sufﬁsante pour
apprendre un systeme de résolution efﬁcace. C’est a ce manque que répond le corpus
ANCOR. Portant sur le francais parlé, il soutient avec ses 418 000 mots la comparaison avec
les autres langues de grande diffusion.

2 Etat de l’art

Le corpus que nous présentons a été réalisé par le Laboratoire d’Informatique de
l’Université de Tours (LI) et le Laboratoire Ligérien de Linguistique (LLL). Ces deux
laboratoires s’intéressent particulierement a la langue parlée. Il est donc naturel que le
corpus porte sur la parole conversationnelle. Le Groupe Aixois de Recherche en Syntaxe a
été un des pionniers de l’étude du langage parlé en corpus. Ses travaux fondateurs n’ont
malheureusement pas eu pour consequence le développement ultérieur de ressources
informatisées en francais. La table 1 présente la liste par idiome des corpus annotés en
coréférence de plus 200 kMots (Recasens, 2010). Le francais est completement absent de
ce panorama. A notre connaissance, le seul corpus en coréférence disponible en francais est
le corpus DEDE, centré sur l'étude des descriptions définies. Il ne comporte
malheureusement que 48 kMots (Gardent et Manuelian, 2005) et ne peut servir a un
apprentissage automatique. De méme, le corpus du CRISTAL, de grande envergure, ne peut
étre considéré par le TAL car il ne code que certaines formes particulieres d’anaphore
(Tuttin et al. 2000).

Le corpus ANCOR que nous avons constitué vise a répondre a cette situation dans le cas
d’un genre linguistique particulier: le francais parlé conversationnel. Par sa taille
(418 kMots), il ne peut étre comparé qu’ a deux autres corpus oraux de coréférences
d’envergure: Switchboard pour l’anglais américain (200 kMots) et COREA pour le
néerlandais. Notons toutefois que seule une partie de cette seconde ressource de 350 kMots
concerne la parole (Hendrickx et al., 2008).
Langue Corpus Genre

allemand TuBa-D/Z News = journaux d’ information
radio-diffuses (transcription de
l’ oral)

anglais ARRAU, Switchboard, News, weblog, forum, chat, récit oral,

ACE, SemEval OntoNotes conversation téléphonique

arabe ACE News

catalan AnCora-Ca News

chinois (mandarin) ACE, OntoNotes News

espagnol ACE, Ancora-Es News

italien I-CAB News

japonais NAIST Text News

néerlandais CO REA News, oral, texte encyclopédique

tcheque PDT News
TABLE 1 — Corpus annotés manuellement en coréférence de plus de 200 000 mots.
3 Elaboration du corpus ANCOR

3.1 Financement et contexte scientifique

La création du corpus ANCOR a été ﬁnancée en deuX étapes. Un premier projet interne au
PRES Centre Val-de-Loire a permis de réaliser un premier corpus (C02) de 35 kMots et de
valider nos choix d'annotation. Notre souci a été de suivre un schéma d’annotation assez
riche pour répondre aux besoins du TAL et des linguistiques. Les premiers résultats
obtenus avec C02 ont ainsi validé l’intérét de la ressource en linguistique (Schang et al.
2011). Sa taille restait toutefois insufﬁsante pour constituer une ressource
d’ apprentissage. Le projet ANCOR, soutenu par la région Centre, nous a précisément
permis d’ atteindre cet objectif.

3.2 Corpus retenus pour l’annotation

Le corpus ANCOR résulte de l’ annotation de trois corpus orauX transcrits sous Transcriber
(Barras et al., 2001) qui étaient disponibles dans nos laboratoires et diffusés librement:

- ESLO, qui correspond a des entretiens sociolinguistiques (Baude et Dugua 2011,
Eshkol-Taravella et al. 2012) ;

- OTG, qui correspond a des dialogues interactifs en présentiel entre des individus et
le personnel d’accueil de l’Ofﬁce du Tourisme de Grenoble ;

- Accueil_UBS, qui correspond a des dialogues interactifs par téléphone recueillis
aupres du standard téléphonique d’une université (Nicolas et al., 2002).

Notre objectif a été de représenter une certaine diversité de genres en termes de degré
d’interactivité du dialogue. Le corpus ESLO, qui correspond a des entretiens, a une
interactivité limitée a la difference des deuX autres : le plus souvent, l’enquéteur pose en
TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

effet une question a laquelle s’ensuit un assez long monologue de réponse. La table 2
présente la distribution des corpus de parole dans la ressource annotée.
Corpus Licence de diffusion Nb de Durée
Parole mots d'enregistrement
ESLO_ANCOR Extrait ESLO (CC-BY-NC-SA) 417 kMots 25 heures
OTG CC-BY-NC-SA 26 kMots 2 heures
Accueil_UBS CC-BY-NC-SA 10 kMots 1 heure
TABLE 2 — Répartition des corpus oraux annotés dans ANCOR

3.3 Procédure d’annotation

L’annotation a été réalisée sur le logiciel GIozz (Mathet et Widlocher, 2009). Nous n’avons
pas retenu MMAXZ (Muller et Strube, 2006) car son interface a été considérée comme
moins conviviale par nos annotateurs. ANCOR sera toutefois diffusé sous les formats GLOZZ
et MMAXZ, du fait de la grande diffusion de ce dernier. GIozz produit une annotation au
format XML reposant sur une DTD que nous avons adaptée a notre schéma d’annotation (cf
§ 4). Autre intérét, les annotations sont séparées du corpus source avec lequel elles sont
synchronisées. Cette annotation déportée (stand-off annotation) permet un enrichissement
multi-niveaux du corpus, ce qui est intéressant en termes d’évolutivité. Le principal souci
rencontré avec GIozz est sa difﬁculté a gérer de gros fichiers (afﬁchage et gestion mémoire).
Si cette limitation n’a pas posé de souci sur OTG et Accueil_UBS (courts dialogues), nous
avons dﬁ procéder a un découpage des entretiens ESLO. La forte structuration des
entretiens fait toutefois que ce découpage n’a pas détruit de chaines coréférentielles.

Le corpus ANCOR a fait l’objet d’un codage par deux annotateurs suivi d’une révision, selon
une procédure en quatre phases successives :

1) Repérage et caractérisation des entités nommées et autres mentions par un annotateur,
2) Révision croisée du repérage par l’autre annotateur et recherche de consensus,

3) Repérage et caractérisation des relations anaphoriques par un annotateur,

4) Révision ﬁnale des relations caractérisées par un superviseur.

Cette démarche séquentielle évite une surcharge cognitive aux codeurs et favorise la
cohérence des annotations sur la durée. Annotateurs et superviseurs avaient un bon niveau
d’ expertise (Master ou doctorat en Sciences du Langage). L’ annotation s’ est déroulée
au rythme de 40 000 mots par homme.mois pour un coﬁt global de construction de 90
000€.
La ﬁabilité du corpus a été estimée sur une expérience pilote qui a consisté a mesurer
l’ accord entre 4 experts ayant participé a l’ annotation, sur un sondage de 10 fichiers.
L’ estimation de l’ accord inter-annotateur reste une question ouverte dans le cas de la
coréférence, du fait des problemes d'alignement entre annotations (Passoneau, 2004 ;
Artstein et Poesio, 2008 ; Matthet et Widlocher, 2011). Nous proposons de contourner ce
probleme par le calcul de mesures d’ accords successifs sur la délimitation des mentions,
l’ identiﬁcation de paires coréférentes et le typage des relations suivant le schéma
suivant:
1) Délimitation des mentions : calcul d’ accord sur le nombre de mentions retenues,

2) Création d’ une annotation en mentions par vote majoritaire pour l’ étape suivante,

3) Identiﬁcation des paires de mentions coréférentes: mesure d’accord sur toutes les
paires d'entités suivant une matrice de confusion présence / absence de relation,

4) Création d’ une annotation en relations non typées toujours par vote majoritaire
5) Typage des relations de coréférences, et mesure d’accord sur le typage seul.

Cette experimentation est en cours. Les premiers résultats suggerent que la ﬁabilité est
acceptable. Nous obtenons pour l’ identiﬁcation des paires une valeur de 0,62 avec les
trois métriques K (Cohen, 1960), n (Scott, 1955) et (x (Krippendorff, 2004), valeur tres
proche du seuil de fiabilité proposé par Cohen. Cette estimation est par ailleurs pénalisée
par des problemes de prévalence et la prise en compte des entités explétives dans
l’ annotation. Nous réﬂéchissons a une adaptation de nos mesures pour compenser ces
biais et présenter une estimation plus ﬁable de l’ accord inter-annotateur.

4- Schéma d'annotation du corpus ANCOR

Le schéma d'annotation que nous avons proposé cherche de maniere classique a identiﬁer
pour chaque entité référentielle (ou mention) si elle introduit une nouvelle entité du
discours, puis si elle réfere a une entité précédemment mentionnée (coréférence) ou si la
référence a une entité précédemment mentionnée dans le teXte est nécessaire pour son
interprétation (anaphore associative). Il n’eXiste pas de consensus sur le codage de ces
relations. Souvent, l’ annotation est adaptée a la tache étudiée ou a une théorie
linguistique sous-jacente. Notre annotation cherche a rester générique et est adaptée aux
besoins du TALN en identifiant toutes les entités, isolées ou non. Par contre, nous ne
procédons pas a une annotation des propriétés utilisées par les algorithmes de
classiﬁcation. Nous partons du principe que les utilisateurs du corpus pourront procéder a

ces prétraitements.

4.1 Repérage des entités référentielles

Nous avons annoté l’ensemble du groupe nominal et pas uniquement sa téte. L’annotation a
également concerné les pronoms et les groupes prépositionnels (GP). Dans ce dernier cas,
la préposition introductive n’est pas intégrée a l’annotation, mais est prise en compte sous
forme d’un attribut associé (GP=YES). Nous avons en outre exclu le pronom pa et ses
dérivés car il reprend souvent l’ensemble d’un groupe verbal, comme dans l'exemple :

(1) L1 : Pierre a encore casse’ sa V01 ture.
L2 : Venant de 1L11', pa ne m ’e’tonne pas.

Ces reprises correspondent a des anaphores abstraites. Comme le notent (Dipper et
Zinmeister, 2010), un schéma particulier d'annotation est nécessaire pour décrire ce type
de coréférence. Ce type d'annotation dépasse largement le cadre du projet ANCOR.

Nous avons par contre annoté les formes explétives de 1'] (cf. 1'] pIeut). Il est en effet
important de repérer ces usages non référentiels qui peuvent tromper les systemes de
résolution. Enﬁn, dans le cas de structures coordonnées (Mazur et Dale, 2007) ou
TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

enchassées, nous avons choisi d’identiﬁer le groupe ainsi que chaque membre le
composant. Tous ces éléments peuvent en effet ancrer une reprise coréférentielle.

4.2 Delimitation des relations

La délimitation des relations consiste a relier les éléments anaphoriques. Certains travauX
privilégient une annotation en chaines (Gardent et Manuélian, 2005 ; Amsili et al, 2007)
c'est-a-dire en « se’quences d’eXpressions singuII'éres apparaissant dans un con teXte teIIes
que 51' Mme de ces expressions re’fere a ququue chose, toutes 1es autres y referent
e’gaIement » (Corblin, 2005). Dans le projet ANCOR, il a été décidé de relier toutes les
relations a la premiere mention de l’entité référentielle trouvée dans le texte. Ce choix
résulte de tests effectués avec des étudiants de Master Linguistique, qui ont montré un
meilleur accord entre annotateurs avec cette approche. Il est en effet apparu que
l'annotation en chaine posait des problemes délicats pour le dialogue, les annotateurs se
trouvant devant des changements de locuteurs pour lesquels la notion de chaine,
pertinente dans la linéarité de l'écrit, devient beaucoup moins évidente a caractériser.
Faute de pouvoir inclure (ou exclure) systématiquement d’ une chaine les mentions faites
par des locuteurs différents, nos tests ont montré que les annotateurs se trouvaient dans
l'impossibilité de trancher de facon nette. Par ailleurs, le codage en premiere mention rend
compte des changements de genre grammatical lors de reprises successives comme dans la
séquence “j’ ai une personne qui (...) e11e te’Ie’phone (...) c’ est un étudiant de L1  e11e...
1'1..." ou toutes les entités sont coréférentes.

Des arguments d'ordre linguistique ou computationnel peuvent étre trouvés en faveur de
chaque représentation. C’ est pourquoi notre codage sera transformé également en codage
en chaine dans la distribution ﬁnale. Notons toutefois que le type de relation (directe,
indirecte, pronominale, associative) et 1’ accord dépendent du choix d’annotation effectué,
sans qu’une solution alternative ne nous paraisse envisageable.

4.3 Caractérisation des relations anaphoriques et de leurs entités

L’annotation consiste enﬁn a décrire par différents traits les entités référentielles et leurs
éventuelles relations. Pour les entités nous avons retenu les traits linguistiques suivants :

- G : Genre etN : Nombre

- POS : partie du discours — Ce trait peut prendre les valeurs P (pronom), N
(Nom) ou NULL (artefact lié a certaines disﬂuences)

- GP : inclusion dans un GP — Valeur YES (si l’entité est un GP) ou NO (si c’est un
GN)

- EN : entité nommée — Types retenus dans la campagne d’évaluation ESTERZ
(Galliano et al., 2009), a savoir FONC, LOC, PERS, ORG, PROD, TIME, AMOUNT et
EVENT. On utilise le type NO si l’entité n’est pas une entité nommée.

- DEF : définitude — cet attribut sert a distinguer le caractere déﬁni (DEF), indéﬁni
(INDEF), démonstratif (DEM) ou explétif (EXP) de l’entité.

- NEW: nouvelle entité du discours : YES (premiere mention), N0 (entité
coréférente avec une autre). Une mention isolée recevra donc toujours le type YES.
TALN-RE’CITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Les relations sont caractérisées par un type (trait TYPE). Nous distinguons le type direct
(DIR) dans le cas d’ une coréférence entre mentions de meme téte nominale (1e bus
rouge.... ce grand bus) ; indirect (IND) si les entités coréférentes ont des tétes nominales
différentes (1e cabriolet... cette de’capotabIe) ; pronomina] (PR) dans le cas particulier de
l'anaphore indirecte ou la reprise est un pronom (1e cabriolet  i1rou1ait...) et associatif
(ASSOC) si les mentions ne sont pas coréférentes mais que l’interprétation de l’ une
dépend de l’ autre (1e ViIIage  son c10cber). De méme, on retrouve un type associatif
pronominal (ASSOC_PR).

4.4 Comparaison avec d’autres schémas d’annotation

Notre modele d’annotation repose est proche de schémas proposés par plusieurs auteurs
travaillant sur le langage écrit (van Deemter et Kibble, 2000 ; Vieira et al. 2002). Nous
avons en effet le souci que nos travauX puissent étre également exploités par des personnes
travaillant sur 1’ écrit Notre typologie de relations reste relativement simple. Gardent et
Manuélian (2005) ont ainsi développé un schéma d’annotation des relations anaphoriques
(bridging) selon une typologie plus précise. Celle-ci nous semble aller au-dela des besoins
actuels du TAL. (Recasens et al., 2011) introduit de son coté la notion de quasi-identité
pour des cas décrits comme de la quasi-coréférence et qui sont considérés dans notre
schéma comme anaphores associatives. Ces propositions cherchent a réduire les
désaccords entre les anaphores associatives et les autres types, mais ne disent rien de ceux
entre nouvelle entité du discours et anaphore associative. Cette distinction est pourtant
hautement subjective (Vieira et al., 2002). (Ogrodniczuk et al. 2013) ont ainsi rencontré un
tres faible accord avec un jeu d’ annotation intégrant la quasi-identité.

Conclusion

ANCOR est a notre connaissance le premier corpus de francais parlé annoté en coréférences
diffusé librement et d’ envergure sufﬁsante pour permettre un apprentissage
automatique. Le LI travaille ainsi au développement d’ un systeme de résolution qui sera
appris sur le corpus. Ce systeme reposera sur BART, une plateforme modulaire et ouverte
utilisant le format MMAX comme format d’ échange interne (Versley et al. 2008). La
richesse d’ annotation du corpus permettra également au LLL de conduire des études
linguistiques variées sur la coréférence. ANCOR sera diffusé librement sous licence CC BY-
NC-SA a la mi-2013. Il sera récupérable sur htt : tln.li.univ-
tours.fr/Tln Corpus Ancor.html.
Références

AMSILI, P., LANDRAGIN, F., ACOSTA, A., BI'I'I‘AR, A. (2007). Résolution anaphorique : Etat d’une
réﬂexion collective, Actes journe’es d ’ Etudes de 1’ ATALA 2007, pages 1—4.

ARTSTEIN, R., POESIO, M. (2008) Inter-Coder agreement for Computational Linguistics,
Computationa] Linguistics, 34, pages 555-596

BARRAS, C., GEOFFROIS, E., WU, Z., LIBERMAN, M. (2001). Transcriber: development and use of a
tool for assisting speech corpora production. Speech Communication 33(1-2), pages 5—22.

BAUDE, 0., DUGUA, C. (2011) (Re)faire le corpus d’ Orléans quarante ans apres : quoi de
neuf, linguiste ? Corpus ,10, pages 99-118.

TALN-RECITAL 2013, 17-21 ]uin, Les Sables d’Olonne
ESHKOL-TARAVELLA, I., BAUDE, 0., MAUREL, D., HRIBA, L., DUGUA, C., TELLIER, I., (2012) Un grand
corpus oral « disponible » : le corpus d’ Orléans 1968-2012. TAL. 52(3), pages 17-46.

COHEN, ]. (1960). A coefﬁcient of agreement for nominal scales. Educationa] and
Psycho]ogica1 Measurement; 2 0, pages 37-46

CORBLIN, F. (2005) Les chaines de la conversation et les autres. In Gouvard, ].-M. (éd.), De 1a
Iangue au ster, Lyon : Presses universitaires de Lyon, pages 233—2 54.

GARDENT, C. et MANUELIAN, H. (2005). Création d’un corpus annoté de traitement des
descriptions déﬁnies. Traitemen tAutomatique des Langues, TAL, 46(1).

HENDRICKX, I. et al. (2008). A coreference corpus and resolution system for Dutch. Proc.
LREC’2008.

KRIPPENDORFF., K. (2008). Testing the reliability of content analysis data: what is involved
and why. In KRIPPENDORFF, K., ET BLOCH, M.A. (Eds) The con tentanaIysis reader. Sage Publ..

LAPPIN, S. et LEAss, H.]. (1994). An algorithm for pronominal anaphora resolution.
Computationa] Linguistics. , 20(4), pages 535-561.

MATHET, Y., WIDLOCHER, A. (2009). La plate-forme GLOZZ : environnement d’annotation et
d’exploration de corpus. Actes de TALN-2009, pages 1—10.

MATHET, Y., WIDLOCHER, A. (2011). Une approche holiste et uniﬁée de l’alignement et de la
mesure d’accord inter-annotateurs. Actes TALN 2011, Montpellier, France.

MITKOV, R. (1994). An integrated model for anaphora resolution. Proc. COLING’94, Kyoto.

MULLER, C., STRUBE, M. (2006). Multi-level annotation of linguistic data with MMAX2. In:
Braun, S., Kohn, K., Mukherjee, 1., ed., Corpus Techno]ogy and Language Pedagogy: New
Resources, New T0015, New Methods. Peter Lang, Francfort, Allemagne, pages 19 7—214.

NG, V. et CARDIE, C. (2002). Improving machine learning approaches to coreference
resolution. Proc. ACL’92.

NICOLAS, P., LETELLIER-ZARSHENAS, S., SCHADLE, I., ANTOINE, ].-Y., CAELEN, ]. (2002). Towards a
large corpus of spoken dialogue in French that will be freely available: the “Parole
Publique” project and its first realisations. LREC’2002. Las Palmas, Espagne. pp. 649-655.

OGRODNICZUK, M., ZAWISLAWSKA, M., GLOWINSKA K., SAVARY A. (2013). Interesting Linguistic
Features in Coreference Annotation of a Highly Inﬂectional Language, soumis a ACL ’ 2013.

PASSONEAU, R. (2 004) Computing reliability for Co-Reference Annotation. LREC ’ 2004.

PONZE'I'I‘O, S.P., VERSLEY, Y. (2011) Computational models of anaphora resolution: A survey.
Consulté sur : wwwusers.di.uniroma1.it/~ponzetto/pubs/poesio10a.pdf

POESIO, M., PONZE'I'I‘O, S.P., VERSLEY, Y. (2011) Computational models of anaphora resolution:
A survey. Consulté sur : wwwusers.di.uniromal.it/~ponzetto/pubs /poesi010a.pdf

RECASENS POTAU, M. (2010). Coreference: Theory, Annotation, Resolution and Evaluation.
PhD Thesis, Universitat de Barcelona, Catalunya, septembre 2010.

SOON, W.M., NG, H.T. LIM, D.C.Y. (2001). A machine learning approach to coreference
resolution in noun phrases. Computationa] Linguistics. , 27(4), pages 52 1-5 44.

SCHANG, E., BOYER, A., MUZERELLE, ]., ANTOINE, ].-Y., ESHKOL, I., MAUREL, D. (2 01 1). Coreference
and anaphoric annotations for spontaneous speech corpus in French, Proc. Discourse
Anaphora and Anaphor Resqution CoIIOqium, DAARC’2011., Faro, Portugal.
TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
SCO'I'I‘, W. (1955). Reliability of content analysis: the case of nominal scale coding. Puinc
Opinions QuaterIy. 19, pages 321-325.

VAN DEEMTER, K., KIBBLE, R. (2000). On Coreferring: Coreference in MUC and related
annotation schemes. Computationa] Linguistics, 26(4), pages 629—63 7.

VERSLEY, Y., PONZE'I'I‘O, S.P., POESIO, M., EIDELMAN, V., IERN, A., SMITH, ]., YANG, X., MOSCHI'I'I‘I, A.
(2008) BART: A Modular Toolkit for Coreference Resolution. Companion Vqume ACL ’ 08.

VIEIRA, R., SALMON-ALT, S., SCHANG, E. (2002). Multilingual corpora annotation for processing
deﬁnite descriptions. Advances in Na tura] Language Processing pages 72 1—729.


Expériences de formalisation d’un guide d’annotation :
vers l’annotation agile assistée

Bruno Guillaume1,2          Karën Fort1,3
(1) LORIA 54500 Vandœuvre-lès-Nancy
(2) Inria Nancy Grand-Est
(3) Université de Lorraine
bruno.guillaume@loria.fr, karen.fort@loria.fr

RÉSUMÉ
Nous proposons dans cet article une méthodologie, qui s’inspire du développement agile et
qui permettrait d’assister la préparation d’une campagne d’annotation . Le principe consiste à
formaliser au maximum les instructions contenues dans le guide d’annotation afin de vérifier
automatiquement si le corpus en construction est cohérent avec le guide en cours d’écriture.
Pour exprimer la partie formelle du guide, nous utilisons la réécriture de graphes, qui permet
de décrire par des motifs les constructions définies. Cette formalisation permet de repérer les
constructions prévues par le guide et, par contraste, celles qui ne sont pas cohérentes avec le
guide. En cas d’incohérence, un expert peut soit corriger l’annotation, soit mettre à jour le guide
et relancer le processus.
ABSTRACT
Formalizing an annotation guide : some experiments towards assisted agile annotation
This article presents a methodology, inspired from the agile development paradigm, that helps
preparing an annotation campaign. The idea behind the methodology is to formalize as much as
possible the instructions given in the guidelines, in order to automatically check the consistency
of the corpus being annotated with the guidelines, as they are being written. To formalize the
guidelines, we use a graph rewriting tool, that allows us to use a rich language to describe the
instructions. This formalization allows us to spot the rightfully annotated constructions and, by
contrast, those that are not consistent with the guidelines. In case of inconsistency, an expert can
either correct the annotation or update the guidelines and rerun the process.

MOTS-CLÉS : annotation, guide d’annotation, annotation agile, réécriture de graphes.
KEYWORDS: annotation, annotation guide, agile annotation, graph rewriting.
1    Introduction

Il est aujourd’hui un consensus clair, non seulement que les corpus annotés sont indispensables
aux outils de traitement automatique des langues (TAL) pour leur entraînement et leur éva-
luation, mais également que l’annotation doit être consistante pour être profitable (voir, par
exemple (Reidsma et Carletta, 2008)). Or, l’obtention d’une annotation manuelle de qualité
requiert l’utilisation d’un guide d’annotation suffisamment complet et cohérent (Nédellec et al.,

2006). La mise au point d’un tel guide est cependant, comme le soulignent Sampson (2000)
et (Scott et al., 2012), loin d’être triviale.
En outre, il est rare, une fois une campagne d’annotation terminée, que le guide d’annotation et
le corpus annoté soient complètement cohérents, ce qui n’est pas sans poser problème pour les
systèmes ou les linguistes utilisant le corpus (voir par exemple (Candito et Seddah, 2012), en ce
qui concerne le corpus arboré du français).
Une solution pour remédier à ces deux difficultés consiste à développer le guide et à annoter
le corpus selon des cycles courts de prototypage. Cette méthodologie est appelée Agile Annota-
tion (Voormann et Gut, 2008) à l’image de l’Agile Development (voir figure 1). Elle n’a, à notre
connaissance, été appliquée que dans un seul cas d’annotation réel (Alex et al., 2010).
FIGURE 1 – Phases de l’annotation traditionnelle (à gauche) et cycles de l’annotation agile (à
droite). Reproduction de la figure 2 de (Voormann et Gut, 2008)

Indépendamment de la notion d’annotation agile, nous avions utilisé la réécriture de graphes
pour rechercher des erreurs récurrentes dans le corpus Sequoia 1 (Candito et Seddah, 2012).
Cette application directe de la réécriture à la détection d’erreurs a permis d’identifier une centaine
d’erreurs d’annotation et a conduit à la publication d’une nouvelle version (3.3) du corpus en
juillet 2012.
Nous présentons ici les expériences que nous avons menées plus récemment dans le cadre de
la correction d’annotations syntaxiques, pour laquelle nous avons transformé les instructions
d’un guide d’annotation existant en règles de réécriture appliquées sur le corpus annoté. Ces
expériences ont montré l’intérêt d’une telle formalisation et nous proposons donc son intégration
dans le processus d’annotation manuelle, ce qui conduirait à la mise en place d’une annotation
agile assistée.
2     Formaliser un guide d’annotation

La méthode que nous proposons consiste à travailler de façon systématique à partir du guide
d’annotation. En effet, pour chaque type d’annotation (pour chaque relation de dépendance
syntaxique dans l’exemple utilisé plus loin) le guide énumère les cas où cette annotation doit être
réalisée. On utilise alors la réécriture de graphes pour repérer les occurrences des annotations
correspondant à chacun des cas énumérés dans le guide. Dans un deuxième temps, on liste
1. https://www.rocq.inria.fr/alpage-wiki/tiki-index.php?page=CorpusSequoia

les annotations qui n’ont pas été repérées lors de la première phase. En théorie, pour chaque
annotation identifiée par cette méthode, se présente l’un des deux cas suivants :
– l’annotation est incorrecte, on doit alors corriger le corpus ;
– l’annotation est correcte et elle correspond à un cas d’usage qui n’a pas été identifié par le
rédacteur du guide, on doit alors mettre le guide à jour.
Évidemment, dans le cas où le guide est mis à jour, il faut relancer le processus pour mettre en
évidence d’éventuelles nouvelles incohérences entre le guide et le corpus. La principale difficulté
dans la mise en place de cette méthode réside dans le passage de la version usuelle du guide à sa
version formalisée en terme de réécriture de graphes.
2.1    Guide d’annotation et implicite

Un guide d’annotation est rédigé par des humains pour des lecteurs humains. Plus précisément,
il est rédigé par des experts pour des lecteurs plus ou moins spécialistes en fonction de la tâche
d’annotation. Le guide repose donc souvent sur des informations implicites. L’introduction décrit
généralement le cadre théorique dans lequel l’annotation est réalisée. Ce cadre permet de donner
les principes généraux qui s’appliquent à l’ensemble du guide. Il faut, dans la suite du document,
qui décrit des parties plus spécifiques de l’annotation, connaître ces éléments généraux pour
interpréter les informations correctement.
Dans le guide (Candito et al., 2009), il est expliqué, d’une part que la fonction A-OBJ (figure 2)
concerne des objets indirects en « à » et, d’autre part que cette fonction peut être réalisée par
un pronom clitique. Tout lecteur francophone sait que, dans le cas de la réalisation clitique,
la préposition n’est pas présente. Cette information n’est pas dans le guide mais elle doit être
rendue explicite dans la règle. Dans cet exemple, il est facile de construire la bonne règle, mais
en général l’information implicite est plus compliquée à formaliser.
2.2    Limites de la formalisation

Il est bien évidemment impossible de formaliser complètement le guide sous forme de règles. En
effet, dans le cas contraire, cela signifierait que l’annotation peut-être faite de façon complètement
automatique sans avoir recours à un jugement humain. Par exemple, dans le cas de la fonction
A-OBJ (cf. figure 2), le guide indique qu’un objet indirect introduit par la préposition « à » peut-
être annoté par une relation A-OBJ entre le verbe et la préposition, mais peut aussi dans certains
cas être annoté comme un locatif (avec la relation P-OBJ_LOC). Le choix entre l’une des deux
annotations se fait à l’aide d’un test basé sur la cliticisation ou sur la forme interrogative. On
ne peut donc pas automatiquement détecter une erreur d’annotation qui consiste à utiliser la
relation A-OBJ au lieu de P-OBJ_LOC ou l’inverse.
3     Expériences

Nous décrivons ici une première expérience d’application de notre méthodologie sur un corpus et
le guide associé.

3.1    Corpus Sequoia

Il existe peu de ressources annotées syntaxiquement pour le français. Le corpus arboré du
français (Abeillé et al., 2003), ou French Treebank (FTB), existe depuis une dizaine d’années
mais il n’est pas librement accessible et redistribuable. L’an dernier, un corpus comparable au FTB
mais librement accessible a été proposé, le corpus Sequoia (Candito et Seddah, 2012). Celui-ci
contient environ 3 000 phrases provenant de quatre sources différentes (Wikipédia, Parlement
européen, Est Républicain et Emea). Ces phrases ont été annotées en constituants. L’annotation
en constituants a ensuite été convertie en une annotation en dépendances. L’annotation en
dépendances visée est décrite dans le guide 2 (Candito et al., 2009).
3.2    Réécriture de graphes

Pour formaliser les informations du guide, nous utilisons GREW (Guillaume et al., 2012), un
outil de réécriture de graphes spécialisé pour les applications en TAL. En effet, GREW propose
un langage de description riche qui permet de repérer automatiquement un motif de graphe
dans un ensemble de phrases. Dans un motif, on peut exprimer des combinaisons complexes de
contraintes sur les nœuds, sur les traits et sur les relations de dépendances. De plus, un motif
peut être sous-spécifié et peut également exprimer des contraintes négatives sur le contexte.
La réécriture de graphes permet, une fois qu’un motif est repéré, de modifier la structure du
graphe. Ici, on n’utilisera cette fonctionnalité que pour marquer chaque occurrence reconnue (à
l’aide de suffixes ok ou fail sur les étiquettes de dépendances).
GREW dispose également d’un mécanisme de modules qui permet d’appliquer successivement
plusieurs ensembles de règles de réécriture. Dans notre application, on utilisera deux modules :
le premier pour repérer les occurrences correctes des dépendances et un second pour mettre en
évidence les dépendances restantes et donc considérées comme incorrectes.
3.3    Un exemple de formalisation : la fonction A-OBJ

La section du guide spécifique à la relation A-OBJ est reproduite dans la figure 2, ci-dessous.
En général, quelques itérations sont nécessaires pour coder correctement les parties implicites ou
les parties décrites ailleurs dans le guide.
1. Une traduction naïve des informations du guide nous amène à définir 4 règles : une pour
chacune des réalisations possibles de l’objet indirect : un nom, un pronom clitique, un
pronom non-clitique ou une proposition infinitive.
2. Si l’objet indirect est un clitique, la préposition n’est pas présente (« Il lui parle. ») ; il faut
donc modifier la règle correspondante.
3. En cas d’élision « au », le lemme reste bien « à » mais la catégorie est P+D et non pas P ; il
faut généraliser les règles.
4. Par contre, en cas d’élision « auquel » ; le lemme n’est « à » mais « auquel » et la catégorie
P+PRO ; il faut une cinquième règle.
2. http://alpage.inria.fr/statgram/frdep/Publications/FTB-GuideDepSurface.pdf
3.5      La fonction A-OBJ
Les objets indirects en à , notés A-OBJ, sont des compléments obligatoires soit nominaux ou
pronominaux (catégorie PP), soit clitiques (CLO), soit des infinitives phrastiques (VPinf)
introduites par à.

Le test pour identifier les A-OBJ est la cliticisation par lui, leur.
(56)     Il ressemble à Martin => A-OBJ(ressemble-1,à), OBJ(à,Martin-3)
(57)     J’encourage Marie à venir => A-OBJ(encourage-1, à), OBJ(à,venir-4)
La cliticisation en y indique généralement un locatif sauf dans certains cas où on notera A-
OBJ :
(58)     Jean pense à Marie => A-OBJ(pense-1,à), OBJ(à,Marie-3)
(59)     Jean va à Paris => P-OBJ_LOC(va-1,à), OBJ(à,Paris-3)

Car on a pas Où pense Jean ? mais bien Où va Jean ?
FIGURE 2 – Extrait du guide d’annotation : la fonction A-OBJ
5. Pour les clitiques, le guide demande la catégorie clitique objet (avec le trait s=obj), mais
le corpus contient des relations A-OBJ dont le dépendant est un clitique réfléchi (avec
le trait s=refl) : « je me pose des questions » ; cette annotation est correcte ; il faut donc
mettre à jour le guide et ajouter une règle pour ce cas.
Au final, on obtient donc les six motifs suivants :
nominal                               pronominal « à » ou « au »                          pronominal « auquel »
/0123               123                              01/23                   /23                                 /0)12
!"#$%       !"#$&'&()              !"#$.          !"#$%            !"#$&'&()               !"#$&./        !"#$%             !"#$&'&()
*+,,"$-                                                *+,,"$-                                                *+,,"$"-.-+*
clitique objet                                         clitique réflexif                                        infinitif
,-./0                                                   -./01                                  12345                345
!"#$%            !"#$&'                                !"#$%             !"#$&'                !"#$%        !"#$&'&()               !"#$%
($)*+                                                  ($)*+,                              *+,,"$-                ,$./0
L’application de ces motifs sur les 3 203 phrases de Sequoia donne les résultats ci-dessous 3 :
nominal           pronominal                    pronominal          clitique     clitique           infinitif
« à » ou « au »                 « auquel »          objet       réflexif
nb d’occurrences                 476                    17                           3                 84           16                87
Il reste alors cinq occurrences de la relation A-OBJ qui ne correspondent à aucune des règles
ci-dessus. Trois de ces occurrences correspondent à une erreur d’annotation :
– une erreur de POS : « [. . .] on ne condamne pas à mort [. . .] » avec « mort » adjectif ;
– dans la construction « répondre à côté de la question », le groupe prépositionnel « à côté de la
question » est un complément circonstanciel de manière, on doit donc avoir la relation MOD ;
– utilisation de la préposition « auprès du » dans la construction « se renseigner auprès du comité » :
l’argument du verbe est un P-OBJ introduit par le préposition « auprès de » ;
3. Tous les résultats sont disponibles sur : http://wikilligramme.loria.fr/doku.php?id=taln_2013

Les deux autres occurrences sont correctes mais mériteraient de figurer d’une façon ou d’une
autre dans le guide :
– le dépendant de la préposition a un POS inattendu : par exemple ET (POS pour étiqueter les
mots d’origine étrangère) dans « [. . .] délivré à The Medecine Company [. . .] » ;
– le gouverneur de la relation A-OBJ est une coordination ;
On peut facilement imaginer le type de précisions qu’il est nécessaire d’apporter à cette partie du
guide et donc le type de modifications qu’il faudra apporter aux règles à la prochaine étape pour
tenir compte des deux derniers points.
4     Méthodologie proposée

L’expérience décrite ci-dessus a été réalisée sur un corpus et son guide figé : le guide n’a pas été
mis à jour depuis plusieurs années et l’annotation du corpus Sequoia est terminée depuis plus d’un
an. Par ailleurs, le guide n’est pas complet et il reste des sections qui ne sont pas complètement
rédigées, notamment à propos de la coordination. Le corpus n’est donc pas toujours annoté
de manière consistante, notamment en ce qui concerne les phénomènes non finalisés dans le
guide. Le corpus Sequoia, auquel nous nous intéressons, est annoté en dépendances syntaxiques,
mais l’annotation de départ et celle sur laquelle les développeurs du corpus travaillent est une
annotation en constituants, et la conversion des constituants vers les dépendances est réalisée de
manière automatique. Cela ajoute une difficulté dans la tâche de corrections du corpus : quand
une erreur d’annotation est détectée dans les dépendances, il faut retrouver l’origine de l’erreur
dans les constituants ou dans la conversion.
4.1    Intégration dans le processus d’annotation manuelle

Les expériences que nous avons menées nous ont convaincus que notre outil de réécriture de
graphes peut être un allié précieux dans la recherche de cohérence entre le guide et le corpus.
S’il est intéressant de l’utiliser sur des données statiques, nous pensons qu’il a un rôle encore plus
important à jouer sur des données en construction. Nous proposons donc d’utiliser ce type d’outil
très tôt dans le processus d’annotation, notamment au moment de la création du guide.
Dans l’idéal, chaque application de la réécriture de graphes permet de repérer des erreurs
d’annotation et des erreurs, des manques ou des imprécisions dans le guide. On peut donc
imaginer un processus comme celui décrit dans le schéma de la figure 3 qui représente un pas
du cycle de développement menant de la version i du guide et du corpus à la version i + 1 de
ceux-ci. Par souci de simplicité, le schéma ci-dessous ne fait pas intervenir de façon explicite
le travail de conversion du guide en règle de réécriture. Ce travail n’est pour autant pas trivial,
comme nous l’avons vu sur notre exemple d’annotation syntaxique.
4.2    Mise en œuvre

La méthodologie d’annotation agile décrite ci-dessus coûte cher et ne peut probablement pas
être appliquée tout au long d’une campagne de grande envergure. Cependant, il est possible (et
souhaitable) de la mettre en œuvre lors de la phase de préparation de la campagne, en particulier
!"#$%#                                                  !"#$%#56

;<3=%*#                                             ;<3=%*#56
1((%"(*    7#*%898/'"(
12)%(-              3"#$%
!(%0   1((%"(*                :(#
1((%"(*    7#*%898/'"(
4'()"*
&'()"*
+,,'-./                                                        &'()"*
+,,'-./56
!                                       !
FIGURE 3 – Une itération du processus d’annotation agile
lors de la mise au point du guide, réalisée en parallèle de l’annotation d’une mini-référence (Fort,
2012). Si la mini-référence se doit d’être représentative du corpus, sa taille va largement dépendre
des contraintes pratiques de la campagne (coût, disponibilité des experts). Il en va de même pour
le nombre d’itérations du cycle d’annotation agile.
Pendant la phase de production, durant laquelle les annotateurs travaillent sur l’ensemble du
corpus, cette méthodologie peut sans doute continuer à être utilisée, mais avec une durée de
cycle beaucoup plus longue. Le repérage d’erreurs par réécriture de graphes est alors un outil
supplémentaire (en complément d’une évaluation régulière, voir, là encore, (Fort, 2012)) pour le
gestionnaire de la campagne, qui lui permet d’être alerté au plus tôt en cas de problème dans
l’annotation.
5    Conclusion et perspectives

Nous avons proposé une méthodologie permettant d’assister l’annotation agile lors d’une cam-
pagne d’annotation, à l’aide d’un outil de réécriture de graphes. Si nous avons obtenu des résultats
intéressants lors des expériences présentées ici, il reste à vérifier l’utilisabilité du système dans le
cadre d’une campagne d’annotation réelle, c’est-à-dire de l’intégrer dans un cycle d’annotation.
Nous comptons donc appliquer cette méthodologie dans les mois qui viennent, pour la création
de la mini-référence et la mise au point du guide, dans le cadre d’une campagne d’annotation en
dépendances syntaxiques profondes du corpus Sequoia.
Pour d’autres types de campagnes d’annotation (par exemple, sémantique ou discursive), la
réécriture de graphes n’est sans doute pas l’outil le plus adapté. Pour autant, une assistance à
l’aide d’outils TAL, même frustres, pourrait profiter à l’annotation agile, dont le principal écueil
est le coût.

Remerciements

Nous tenons à remercier Florian Besnard, étudiant à l’École des Mines de Nancy, qui a participé
lors de son stage à la conversion d’une partie du guide en règles de réécriture.
Références
ABEILLÉ, A., CLÉMENT, L. et TOUSSENEL, F. (2003). Building a treebank for French. In ABEILLÉ, A.,
éditeur : Treebanks, pages 165 –187. Kluwer, Dordrecht.
ALEX, B., GROVER, C., SHEN, R. et KABADJOV, M. (2010). Agile corpus annotation in practice : An
overview of manual and automatic annotation of CVs. In Proceedings of the Fourth Linguistic
Annotation Workshop (LAW), pages 29–37, Uppsala, Suède. Association for Computational
Linguistics.
CANDITO, M., CRABBÉ, B. et FALCO, M. (2009). Dépendances syntaxiques de surface pour le
français. Rapport technique, Université Paris 7.
CANDITO, M. et SEDDAH, D. (2012). Le corpus Sequoia : annotation syntaxique et exploitation
pour l’adaptation d’analyseur par pont lexical. In Actes de Traitement Automatique des Langues
Naturelles (TALN), Grenoble, France.
FORT, K. (2012). Les ressources annotées, un enjeu pour l’analyse de contenu : vers une méthodologie
de l’annotation manuelle de corpus. Thèse de doctorat, Université Paris XIII, LIPN, INIST-CNRS.
GUILLAUME, B., B ONFANTE, G., MASSON, P., MOREY, M. et PERRIER, G. (2012). Grew : un outil de
réécriture de graphes pour le TAL. In Actes de Conférence annuelle sur le Traitement Automatique
des Langues (TALN), Grenoble, France.
NÉDELLEC, C., BESSIÈRES, P., B OSSY, R., KOTOUJANSKY, A. et MANINE, A.-P. (2006). Annotation gui-
delines for machine learning-based named entity recognition in microbiology. In et C. NÉDELLEC,
M. H., éditeur : Proceedings of the Data and text mining in integrative biology workshop, pages
40–54, Berlin, Allemagne.
REIDSMA, D. et CARLETTA, J. (2008). Reliability measurement without limits. Computational
Linguistics, 34(3):319–326.
SAMPSON, G. (2000). The role of taxonomy in language engineering. Philosophical Transactions of
the Royal Society of London. Series A :Mathematical, Physical and Engineering Sciences, 358(1769):
1339–1355.
SCOTT, D., BARONE, R. et KOELING, R. (2012). Corpus annotation as a scientific task. In
International Conference on Language Resources and Evaluation, Istanbul, Turquie.
VOORMANN, H. et GUT, U. (2008). Agile corpus creation. Corpus Linguistics and Linguistic Theory,
4(2):235–251.
TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

_ CasSys
Un systeme llbre de cascades de transducteurs

Denis Maurel Nathalie Friburger
Université Francois Rabelais Tours
denis .maurel@univ—tours . fr nathalie . friburger @univ—tours . fr

RESUME
CasSys est un systeme de création et de mise en oeuvre de cascades de transducteurs intégré
a la plateforme Unitex. Nous présentons dans cette démonstration la nouvelle version
implantée fin 2012. En particulier ont été ajoutées une interface plus conviviale et la
possibilité d'itérer un méme transducteur jusqu'a ce qu’il n’ait plus d’inﬂuence sur le texte.
Un premier exemple concernera le traitement de texte avec une gestion complexe de balises
XML et un deuxiéme présentera la cascade CasEN de reconnaissance des entités nommées.

ABSTRACT
CasSys, a free transducer cascade system.

CasSys is a free toolkit integrated in the Unitex platform to create and use transducer
cascades. We are presenting the new version implemented at the end of 2012. The system
interface has been improved and the Kleen star operation has been added: this operation
allows applying the same transducer until it no longer produces changes in the text The ﬁrst
example deals with complex XML text parsing and the second with CasEN, a free cascade for
French Named Entity Recognition.
MOTs-CLES: cascade de transducteurs, graphes Unitex, texte avec balises XML,
reconnaissance d'entités nommées.
KEYWORDS : transducer cascade, Unitex graphs, XML text, French Named Entity Recognition.
1 Présentau'on de CasSys

CasSys est un systéme de création et de mise en oeuvre de cascades de transducteurs
(Friburger, Maurel, 2004), aujourd'hui intégré a la plateforme Unitex. Il s’agit donc en fait de
cascades de graphes au sens Unitex, plus puissants que de simples transducteurs, puisqu’ils
permettent l'utilisation de variables. Une nouvelle version a été implantée en décembre
2012. En particulier une importante fonctionnalité a été ajoutée : la possibilité d’itérer un
méme transducteur jusqu’a ce qu’il n’ait plus d'inﬂuence sur le texte.

Dans cette démonstration, nous proposerons un premier exemple concernant le traitement
de texte avec une gestion complexe de balises XML (section 2) et un deuxieme présentant la
cascade CasEN de reconnaissance des entités nommées (version 1), réalisée en suivant les
consignes de la campagne Ester (section 3). Cette cascade est, elle aussi, librement accessible
et ses ressources sont ouvertes. La cascade réalisée pour la campagne Etape sera disponible
aussi, dés que les résultats officiels seront parus.
TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
2 Traitement du balisage XML

Dans le cadre du projet Régjon Centre Renom pour la recherche d'entités nommées dans des
textes de la Renaissance1, nous avons dﬁ traiter des textes 01‘1 l'ensemble de la mise en page
était indiquée sous un format XML, rendant difficile l'acces a l'analyse du texte lui-méme. Le
texte ﬁnal devait comporter a la fois les balises de mise en page et les balises désignant les
entités nommées. L'utilisation d'une cascade permet a des non-informaticiens de faire des
manipulations complexes sur le texte sans avoir a coder: par exemple, ignorer des balises
lorsqu'elles ne sont pas nécessaires a l'analyse (lettrines, début de ligne...), rétablir les mots
coupés par un saut de ligne ou bas de page, choisir la forme corrigée et non la forme
originale lorsque des corrections sont ajoutées (en général, des apostrophes absentes du
texte original). Par exemple :

<p> Pour la REN, on veut
Coupure <hi rend="larger">E</hi>Nceste mesme heure Gargan disposer de ,

<lb rend="hyphen"/>tua [...]

enlﬁn <lb/> fut adverty [...] comment Picrocho- gfcifcrllgll:
de llgne <lb rend="hyphen"/>le sestoit rempare a la Rocheclermaud [...]
la Rocheclermaud
</p>
<p> [...]
<lb/>[...] sainct et

Al out <lb/>Thomas
d' apos- <choice><orig>Langloys </orig><reg> L'angloys</reg></choice>
trophe voulut bien pour I
<lb/>yceulx mourlr, [...]
</p>

sainct Thomas L'angloys

3 La cascade CasEN, version 1

La cascade CasEN, réalisée pour la campagne Ester, dans le cadre le cadre du projet ANR
Variling et du projet FEDER Régjon Centre Entités nommées et nommables, est disponible
librement et en ressources ouvertes2. Elle permet la reconnaissance d'entités nommées.
Cette cascade, décrite dans (Maurel et al., 2011), est composée de 56 graphes et sera
commentée lors de la démonstration (en particulier l'ordre des graphes qui n'est pas
anodin !) . Un exemple de reconnaissance est donné ci-dessous :

« Au pire de la crise, <ENT type:"time.date.rel">a l'aubomne dernier</ENT>,

nous avons détenu jusqu’a 20 % de liquidités dans notre portefeuille », indique

<ENT type:"pers.hum"><ENT type:"pers.hum"><forename>Denis

</forename> <surname>Remacle</surname>, <ENT type:"job">gérant

d'<ENT type:"org.com">Amplitude Paciﬁque</ENT></ ENT> </ENT>, une
sicav de <ENT type:"org.com">La Poste</ENT>.

Références

FRIBURGER N., MAUREL D. (2004), Finite-state transducer cascade to extract named entities in
texts, Theoretical ComputerScience, vol. 313, 94-104.

MAUREL D., FRIBURGER N., ANTOINE ].-Y., ESHKOL—TARAVELLAI., NOUVEL D. (2011). Cascades de
transducteurs autour de la reconnaissance des entités nommées. Traitement automatique
des langues, 52 (1) : 69 -96.
1 Ici, GargantIIa de Rabelais, dans sa version originale

2 http: ( (tln.li.univ-bours.fr( I In CasEN.html

Fouille de règles d’annotation partielles
pour la reconnaissance des entités nommées

Damien Nouvel 1, 2           Jean-Yves Antoine 1           Nathalie.Friburger 1
Arnaud.Soulet 1
(1) LI, 3 place Jean Jaurès, 41000 Blois
(2) Alpage, INRIA & Université Paris-Diderot, 75013 Paris
{prenom.nom}@univ-tours.fr

RÉSUMÉ
Ces dernières décennies, l’accroissement des volumes de données a rendu disponible une diversité
toujours plus importante de types de contenus échangés (texte, image, audio, vidéo, SMS, tweet,
données statistiques, spatiales, etc.). En conséquence, de nouvelles problématiques ont vu le
jour, dont la recherche d’information au sein de données potentiellement bruitées. Dans cet
article, nous nous penchons sur la reconnaissance d’entités nommées au sein de transcriptions
(manuelles ou automatiques) d’émissions radiodiffusées et télévisuelles. À cet effet, nous mettons
en œuvre une approche originale par fouille de données afin d’extraire des motifs, que nous
nommons règles d’annotation. Au sein d’un modèle, ces règles réalisent l’annotation automatique
de transcriptions. Dans le cadre de la campagne d’évaluation Etape, nous mettons à l’épreuve
le système implémenté, mXS, étudions les règles extraites et rapportons les performances du
système. Il obtient de bonnes performances, en particulier lorsque les transcriptions sont bruitées.
ABSTRACT
Mining Partial Annotation Rules for Named Entity Recognition
During the last decades, the unremitting increase of numeric data available has led to a more
and more urgent need for efficient solution of information retrieval (IR). This paper concerns a
problematic of first importance for the IR on linguistic data : the recognition of named entities
(NE) on speech transcripts issued from radio or TV broadcasts. We present an original approach for
named entity recognition which is based on data mining techniques. More precisely, we propose to
adapt hierarchical sequence mining techniques to extract automatically from annotated corpora
intelligible rules of NE detection. This research was carried out in the framework of the Etape NER
evaluation campaign, where mXS, our text-mining based system has shown good performances
challenging the best symbolic or data-driven systems
MOTS-CLÉS : Entités nommées, Fouille de données, Règles d’annotation.
KEYWORDS: Named Entities, Data Mining, Annotation Rules.
1    Introduction

Ces dernières décennies, le développement considérable des technologies de l’information et de la
communication a modifié la manière dont nous accédons et manipulons les connaissances. Nous
constatons une diversité toujours plus importante des types de contenus échangés (texte, image,
audio, vidéo, SMS, tweet, données statistiques, spatiales, etc.), ce qui nécessite de résoudre
de nombreuses problématiques, parmi lesquelles la recherche d’information, qui a intéressé la
communauté du TALN dès les années 90 avec les campagnes d’évaluation MUC (Grishman et
Sundheim, 1996). Les travaux sur le sujet ont porté une attention particulière aux noms propres
de personnes, de lieux et d’organisations, appelés entités nommées (EN). Au gré des besoins,
celles-ci ont été étendues aux dates, aux expressions numériques, aux marques ou aux fonctions,
avant de recouvrir un large spectre d’expressions linguistiques.

De nombreux systèmes ont été élaborés pour réaliser la reconnaissance d’entités nommées (REN),
selon des approches orientées connaissances ou orientées données. Les premières ont générale-
ment une grande précision mais nécessitent un coup humain de développement important, ce
qui se traduit généralement par une couverture (et donc un rappel) perfectible. Les approches
orientées données, par ajustement automatique de paramètres d’un modèle numérique, per-
mettent d’obtenir de bonnes performances, avec un coup d’entrée limité, du moment où l’on
dispose d’une base d’apprentissage de taille suffisante. Ils sont également réputés présenter une
dégradation graduelle de leurs performances sur des données bruitées. Cependant, l’aspect “boîte
noire” des algorithmes d’apprentissage rend difficile l’amélioration ciblée de leurs performances.

Ces constats ont été vérifiés par de nombreuses campagnes d’évaluation. À titre d’exemple, lors
de la campagne d’évaluation francophone Ester2 (Galliano et al., 2009), portant sur le traitement
de transcriptions de parole radio ou télédiffusée, les deux meilleurs systèmes travaillant sur
des transcriptions manuelles étaient des systèmes à base de connaissance, tandis que les tests
effectués sur des sorties de reconnaissance de la parole ont été dominés par un système orienté
données.

Les travaux que nous présentons dans cet article ont été menés dans le cadre de la campagne
Etape (qui a fait suite à Ester2) qui visait notamment à évaluer des systèmes de REN sur des flux
de parole conversationnelle. Nous y proposons une approche novatrice pour la REN : l’utilisation
de méthodes de fouille de données séquentielle hiérarchique. À nos yeux, ces travaux présentent
plusieurs originalités du point de vue du TALN :
(i) nous élaborons un moyen-terme entre les approches orientées données et orientées connais-
sances reposant sur la recherche, à partir de données d’apprentissage, de motifs pour la REN :
cette technique centrée données permet l’extraction de connaissances interprétables ;
(ii) la stratégie de détection des entités nommées est originale, par la recherche séparée du
début et de la fin des entités, en nous appuyant sur le contexte immédiat pour placer les balises
d’annotation : cela présente l’intérêt de conserver une certaine robustesse en cas de disfluence
ou d’erreur de reconnaissance au sein de l’entité nommée.

Cet article porte sur l’élaboration, l’implémentation et l’évaluation d’une telle approche. En partie
2, nous faisons un état de l’art des approches pour la REN. La partie 3 présente le formalisme de
fouille pour l’extraction de règles d’annotation et leur utilisation pour reconnaître des entités
nommées. En partie 4, nous décrivons le jeu de données utilisé et les résultats obtenus lors de
l’évaluation dans le cadre de la campagne Etape.
2      Approches pour la reconnaissance d’EN structurées

2.1      Approches orientées connaissances

Les approches orientées connaissances sont basées sur la description de règles décrivant les
entités nommées et leur contexte à l’aide d’indices linguistiques fournis par le texte lui-même et
des ressources externes (dictionnaires). Généralement, les textes sont étiquetés syntaxiquement
(éventuellement sémantiquement) grâce aux dictionnaires, puis un ensemble de règles, qui
prennent en compte les indices morphologiques (présence de majuscule, ponctuation), morpho-
syntaxiques et sémantique, permettent de repérer les ENs. Les règles utilisent ces éléments, soit
comme preuves internes de la présence d’une entité nommée, soit par description de son contexte
d’apparition (McDonald, 1996; Friburger et Maurel, 2004). Une preuve interne sera, par exemple,
la présence d’un prénom avant un mot commençant par une majuscule ; ce prénom indiquera un
nom de personne (ex : ’François Hollande’). Nous voyons que c’est la “connaissance” qui guide
cette approche, celle de l’expert qui créé les règles, selon les informations à sa disposition (dont
les ressources externes).

Dès les années 1990, un certain nombre de systèmes (Stephens, 1993; Hobbs J. R. et Tyson, 1996)
mettent en œuvre cette approche orientée connaissances. Les automates sont particulièrement
adaptés à l’élaboration et l’utilisation des règles. De plus, l’utilisation de transducteurs 1 permet
de produire très intuitivement une annotation à l’aide de balises (‘<pers>’, ‘</pers>’, ‘<org>’,
‘</org>’, etc.), ils sont donc largement utilisés pour ce type de tâche (Friburger et Maurel, 2004;
Brun et Ehrmann, 2010; Béchet et al., 2011). Enfin, les transducteurs peuvent être organisés
sous forme de cascades, chaque transducteur permettant de lever des ambiguïtés et de mettre à
disposition des reconnaissances pour les transducteurs suivants (ce qui permet de reconnaître des
imbrications). L’ordre dans lequel sont appliqués les transducteurs a alors une grande importance.

Étant donné les traitements qu’elles mettent en œuvre, les approches orientées connaissances
insèrent au sein des séquences de mots ce que nous appelons des marqueurs, comme le montre la
figure 1 pour l’expression ‘fondation Cartier’.

ORG
PERS
La           Fondation        Cartier              pour     l’   art   contemporain       .
<org>                                                                            </org>
<pers>          </pers>
FIGURE 1 – Annotation par balises

Les approches orientées connaissances peuvent être utilisées et adaptées à des textes sans
apprentissage préalable. Leur limitation est liée au fait que les ressources utilisées sont rarement
exhaustives (par exemple, les noms propres forment une classe “ouverte”) : il semble illusoire de
bâtir ce type d’approche sur l’hypothèse d’un lexique complet des entités nommées existantes.

1. Automates qui modifient le texte fourni en entrée par insertion de balises

2.2    Approches orientées données

Les approches orientées données paramètrent un modèle automatiquement grâce à un apprentis-
sage sur un corpus d’entraînement. Ce corpus d’entrainement, créé par des experts, fournit de
nombreux exemples de données : le système apprend sur ces exemples puis prédit l’étiquette
d’une nouvelle donnée, selon son modèle. Le corpus d’entrainement est constitué d’un ensemble
de textes annotés en entités nommées par des experts. L’apprentissage automatique sera chargé
d’ajuster les paramètres disponibles, cette procédure étant guidée à chaque itération par les
erreurs que commet le système sur les jeux de données disponibles. Une fois l’apprentissage
réalisé, le système est en mesure d’annoter de nouveaux textes en entités nommées selon les
paramètres de son modèle. Traditionnellement, l’apprentissage automatique se rapproche plutôt
d’une classification (attribution d’une classe à un mot) que d’une annotation (délimitation d’une
expression linguistique).
Pour la REN, le format BIO 2 s’est imposé. La figure 2 présente la classification par mots réalisée
pour l’énoncé ‘<org> fondation <pers> Cartier </pers> </org>’. Signalons qu’en partie
3, nous présentons une approche orientée donnée, mais qui est apparentée à un mécanisme de
transduction (à l’aide d’indices locaux) plutôt que de classification.

B-ORG
PERS
La      Fondation       Cartier    pour         l’     art    contemporain   .
O         I-ORG         I-ORG      I-ORG      I-ORG   I-ORG      I-ORG       O
B-PERS
FIGURE 2 – Annotation par classification

Généralement, ces approches estiment la probabilité des classes selon les tokens et les informa-
tions qui y sont associées. Parmi les modèles numériques adaptés, figurent les modèles bayésiens,
la régression logistique (ou maximum d’entropie), les machines à vecteur de support (SVM) ,
etc. La régression logistique a démontré son efficacité pour la reconnaissance d’entités nommées
(Mikheev et al., 1999; Ekbala et al., 2010), permettant de prendre en compte de multiples
traits discriminants (morphologiques, morpho-syntaxiques, lexicaux) interdépendants. D’autres
modèles tirent parti de la séquentialité, comme les HMM (Bikel et al., 1999), par modélisation
des transitions entre états (types d’entités nommées) et des générations d’observations (mots).
Pour prendre en compte simultanément la multiplicité des indices locaux et les aspects séquentiels
au sein d’un modèle unifié, les MEMM 3 (McCallum et al., 2000) puis les CRF 4 (Raymond
et Fayolle, 2010; Zidouni et al., 2010) sont les modèles réputés les plus adéquats à ce jour.
L’inconvénient est qu’ils restent difficiles à interpréter : les traits découverts sont généralement
composites et exhibent des dépendances complexes dont il est difficile d’affirmer qu’elles sont
nécessaires ou suffisantes pour déterminer les entités nommées.
A ce jour, les approches orientées données se basent majoritairement sur une représentation
“plate” des entités nommées. Comme nous le verrons en partie 4, nous cherchons à réaliser la
2. Begin, Inside, Outside
3. Modèles markoviens à maximum d’entropie
4. Champs aléatoires conditionnels

REN structurée (avec imbrications). Notons que quelques travaux (Finkel et Manning, 2005;
Dinarelli et Rosset, 2011) ont adapté avec un certain succès des méthodes orientées données à la
reconnaissance de structure.

De manière générale, nous remarquons que les approches automatiques nécessitent un travail
préalable conséquent (préparation des jeux de données, implémentation du modèle, des procé-
dures d’apprentissage et d’estimation, sélection des traits et dépendances pertinents, etc.) avant
d’être en mesure de paramétrer les modèles, et qu’il reste difficile de les utiliser pour extraire des
connaissances ou pour étudier des phénomènes particuliers.
2.3    Proposition : les marqueurs d’annotation

Nous le voyons, les approches guidées par les données s’appuient sur des indices locaux variés.
La nature “locale” de la structuration en entités nommées est alors un atout. Les systèmes
orientés connaissances ont l’avantage de modéliser la structure interne des entités nommées.
Ainsi un système à base de connaissances aura plus de facilité à analyser l’encapsulation d’entités
nommées comme dans l’exemple suivant (issu d’Etape) : ‘le député UMP de Haute-Saône’ où
l’entité nommée globale est construite à l’aide de l’entité ‘UMP’, de type organisation, et de l’entité
‘Haute-Saône’, de type division géographique administrative.

Cependant, ces dernières approches utilisent une connaissance dont la construction est coûteuse et
délicate. Aussi avons-nous souhaité développer une approche permettant l’extraction automatique
sur corpus de motifs se rapprochant des règles de reconnaissance mises en œuvre par la REN
symbolique. La fouille hiérarchique séquentielle de données est adéquate à cet effet.

Par ailleurs, les systèmes orientés connaissances sont aujourd’hui contraints à modéliser inté-
gralement la structure des entités, voire de ses contextes d’introduction. Ce choix est discutable
et met à l’épreuve la robustesse des systèmes lorsqu’ils traitent de la parole spontanée. Une
erreur de reconnaissance sur un seul mot de l’entité (dûe par exemple à une disfluence) empêche
l’application de la règle de détection.

Afin de répondre à cette insuffisance, nous proposons de séparer la détection du début et de
la fin de l’entité, pour ensuite chercher à associer une marque de début et de fin d’entité. Notre
hypothèse est que l’on dispose de suffisamment d’indices locaux pour caractériser précisément le
début ou la fin d’une entité.

Considérons pas exemple l’énoncé annoté suivant ‘En <date> <num> 1969 </num> </date>
<pers> <prenom> Georges </prenom> <famille> Pompidou </famille> </pers> dirige la
<org> <loc> France </loc> </org>’. Notre hypothèse est que chacune des marques d’annota-
tion (‘<pers>’, ‘<prenom>’, ‘</prenom>’, ‘</pers>’, etc.) est détectable séparément. De plus,
la détection d’une entité encapsulée telle que ‘<prenom>’ peut guider la détection de l’entité
englobante. Il s’agira, pour le système, d’extraire des règles d’annotation, d’estimer localement
les marqueurs probables, puis de déterminer, par leurs combinaisons, l’annotation la plus vrai-
semblable. Nous implémentons un système de reconnaissance d’entités nommées, mXS, selon
cette approche originale. Grâce à ce procédé, notre système reconnaît par exemple le montant
‘deux cent ça compte mille’ (erreur de transcription pour deux cent cinquante mille), alors qu’un
système symbolique sera mis en difficulté.
3      Extraction de règles d’annotation pour la REN

3.1     Enrichissement ambigu des données
axe paradigmatique

PRENOM                                      BAT          BAT        CELEB
Sémantique

NP       VER       VER      DET           NC                 NP
Morpho-syntaxe
axe structurel

avoir    visiter                                                      Lemmatisation

Pierre      a      visité     le          Centre          Pompidou               Tokenisation
FIGURE 3 – Représentation des structures à fouiller

L’approche que nous mettons en œuvre repose sur des analyses fréquemment conduites pour
traiter le langage naturel (morpho-syntaxe, lexiques). Pour la fouille, ces traitements sont
interprétés comme autant d’enrichissements des données, à utiliser pour rechercher des motifs
généralisés dans les données. La figure 3 présente de manière schématique, sur l’exemple ‘Pierre
a visité le Centre Georges Pompidou’, la manière dont se superposent ces enrichissements.
La fouille de données devra alors tenir compte de deux axes : paradigmatique, pour la superposi-
tion d’enrichissements, et structurel, pour l’examen des contigüités entre items. Comme nous le
verrons par la suite, ce processus est flexible : les enrichissements peuvent être plus ou moins
profonds selon les éléments considérés. Nous pouvons moduler à volonté l’axe paradigmatique
selon les éléments observés et la tâche d’annotation à réaliser.
3.1.1    Morpho-syntaxe

Nous réalisons conjointement la tokenisation, la lemmatisation et l’étiquetage morpho-syntaxique
avec TreeTagger (Schmid, 1994). De surcroît, nous en adaptons la sortie comme suit :
– Déterminants : les déterminants définis (‘le’, ‘la’, ‘les’, ‘l”) sont sous-catégorisés en ‘DET/DEF’.
– Prépositions : la sous-catégorie ‘PRP:det’ (‘au’, ‘du’, ‘des’) forme une catégorie ‘PRPDET’.
– Nombres : les nombres sont sous-catégorisés selon leur nombre de chiffres 5 .
– Noms propres et abréviations : ces deux catégories se généralisent en ‘NAMABR’.
– Nom propres, abréviations, noms, verbes : ces éléments sont sous-catégorisés par le suffixe
des trois derniers caractères (‘NOM/SUFF:ier’, ‘NAMABR/NAM/SUFF:ges’, ‘VER/SUFF:vre’).
– Verbes : les sous-catégories relatives au mode et temps du verbe sont supprimées.
Pour le processus de fouille de données, nous omettons les variations surfaciques (majuscules)
et flexionnelles (déclinaisons et conjugaisons) : nous ne conservons pas les items lexicaux eux-
mêmes et faisons reposer la recherche de motifs sur les lemmes proposés par TreeTagger. Par
exemple, le ‘En 1970 les socialistes [...]’ donnera la séquence :
‘PRP/en NUM/DIGITS:4/PREF:19/1970 DET/DEF/le NOM/SUFF:ste/socialiste’.
5. Ce nombre est précisé s’il est inférieur ou égal à quatre le préfixe est utilisé dans ce dernier cas :
‘NUM/DIGITS:MANY’, ‘NUM/DIGITS:4/PREF:20’ . . ., ‘NUM/DIGITS:1’)

3.1.2    Lexiques

Les lexiques nous permettent d’ajouter un niveau sémantique aux hiérarchies. Nous exploitons
des ressources diverses, dont certaines sont importées à partir des dictionnaires et motifs du
système CasEN 6 . Nous y ajoutons quelques listes, constituées manuellement, en particulier
pour les fonctions, lieux, organisations, quantités et dates. Ces ressources contiennent 221
547 expressions distinctes qui produisent 443 112 catégorisations sémantiques 7 . Une large
part est dédiée à la reconnaissance des personnes et des lieux. Signalons qu’une partie de
ces ressources est générée à partir d’automates (transducteurs CasEN) qui reconnaissent des
expressions linguistiques utiles à la REN.
Ces ressources sont utilisées telles quelles pour produire les enrichissements. Ceux-ci peuvent
alors être sémantiquement ambigus, ce que nous notons comme une disjonction exclusive
⊕. Par exemple, au nom propre Washington seront affectées les catégories sémantiques
‘CELEB⊕TOPO⊕ORG-LOC-GOV⊕PREN⊕VILLE’. Notons ici que nous considérons que les noms
propres forment une classe ouverte et qu’ils n’ont pas vocation à être utilisés lexicalisés au
sein des motifs extraits : lorsqu’ils ont donné lieu à des enrichissements sémantiques, les items
lexicaux sont omis afin que la fouille de données ne repose que sur les catégories sémantiques.
3.2     Exploration de règles d’annotation de segments

Les données ainsi enrichies forment le langage � r et ont vocation a être fouillées afin d’y
rechercher des motifs séquentiels d’intérêt (Fischer et al., 2005; Cellier et Charnois, 2010) pour
la REN.
Le langage des motifs � p+ comprend celui des données enrichies et toutes leurs généralisa-
tions. Un élément de motif (item) couvre une donnée, notée ≤ci , lorsqu’il s’y trouve en tenant
compte des disjonctions ⊕. Par exemple, l’item ‘TOPO/Washington’ couvre la donnée enrichie
‘CELEB/Washington⊕TOPO/Washington’. Dès lors, nous nous inspirons de travaux intégrant
des hiérarchies aux séquences (Srikant et Agrawal, 1996), en y ajoutant la notion de segment 8
particulièrement adaptée au traitement de structures au sein desquelles des items se répètent
(comme des syntagmes sémantiquement catégorisés).
Couverture d’un motif de segments sur des données : soient un motif de segments P =
p1 p2 . . . pn ∈ � r et une séquence de la base de données enrichie I = i1 i2 . . . i p ∈ � p+ , alors P
couvre les segments de I, noté P ≤c + I, s’il existe une fonction discrète croissante S() définie de
[1, p] vers [1, n] telle que, pour tout j ∈ [1, p], alors p j ≤ci iS( j)
Ce même mécanisme sera pris en compte lorsqu’il s’agit de généraliser selon l’axe paradigmatique :
l’objectif est que, par exemple, ‘CELEB’ couvre indifféremment ‘Pompidou’ et ‘Valery Giscard
d’Estaing’. Plus généralement, nous définissons trois relations de généralisation entre motifs :
– Généralisation hiérarchique entre motifs de segments : soient deux motifs de segments
P = p1 p2 . . . pn ∈ � p+ et Q = q1 q2 . . . q p ∈ � p+ , alors P généralise hiérarchiquement les segments
de Q, noté P ≤ g Q, s’il existe une une fonction discrète croissante S() définie de [1, p] vers
[1, n] telle que, pour tout j ∈ [1, p], alors p j ≤ci qS( j) .
6. http ://tln.li.univ-tours.fr/Tln_CasEN.html
7. Il est fréquent que plusieurs catégories sémantiques soient associées aux entrées
8. Pour respecter l’anti-monotonie, deux items contigus ne peuvent être identiques ou parents l’un de l’autre

– Généralisation par affixation entre motifs : soient deux motifs P = p1 p2 . . . pn ∈ � p+ et
Q = q1 q2 . . . q p ∈ � p+ , alors P généralise par affixation Q, noté P ≤ g Q, si p ≥ n et s’il existe au
moins un k ∈ [0, p − n] tel que, pour tout j ∈ [1, n], alors q j+k = p j .
– Généralisation sur marqueurs entre motifs : soient deux motifs P = p1 p2 . . . pn ∈ � p+ et
Q = q1 q2 . . . q p ∈ � p+ , alors P généralise sur marqueurs Q, noté P ≤ g Q, si p ≥ n et s’il existe
une fonction discrète strictement croissante C() définie de [1, n] vers [1, p] telle que, pour tout
j ∈ [1, n], alors p j = qC( j) et, pour tout k ∈ [1, p] tel que k �∈ {C( j), j ∈ [1, n]}, alors qk ∈ Σm .

Ces généralisations nous permettent de rechercher des motifs dans lesquels apparaissent les
marqueurs d’entités nommées. Par exemple, au sein de l’énoncé ‘Le <fonc> président </fonc>
<pers> Georges Pompidou </pers> débattait souvent.’, nous relevons, par relations de couverture
et de généralisation, une occurrence pour les motifs ‘NOM/président <pers> CELEB </pers>’
ou ‘NOM/président CELEB </pers> VERB/débattre’, par exemple.

Finalement, La notion de règle d’annotation partielle découle de celle de motif de segments :

Règle d’annotation partielle une règle d’annotation partielle est un motif de segments P ∈ � p+
contenant au moins un élément de Σ r et un élément de Σm .

Notons qu’à ce stade les règles d’annotation contiennent un nombre indéterminé de marqueurs.
Il conviendra de filtrer au besoin lors de l’extraction des motifs et de s’assurer que l’on utilise ces
règles de manière adéquate afin de produire une annotation.
3.3    Filtrage et extraction de règles d’annotations partielles

La combinatoire du langage � p+ étant importante, il est nécessaire de filtrer les règles. Pour cela,
nous déterminons la fréquence et la confiance des règles, afin d’éliminer celles qui n’ont que peu
d’intérêt. À l’aide de la couverture et des généralisations définies ci-dessus, nous déterminons la
fréquence F r eq(P, �) d’une règle P comme son nombre d’occurrences au sein du corpus �. La
confiance d’une règle d’annotation P estime la proportion de phrases où la règle est appliquée
avec justesse :
F r eq(P, �)
C on f (P, �) =                      (la fonction Ret m (P) retire les marqueurs de P)
F r eq(Ret m (P), �)
Même en fixant des seuils de support et confiance sélectifs, les règles d’annotation peuvent être
trop nombreuses à cause des combinaisons possibles au travers de la hiérarchie. Afin de contenir
cette abondance de règles, nous proposons de grouper les règles, puis d’éliminer celles qui ne sont
pas informatives, à l’instar de (Pasquier et al., 1999). L’idée forte est que deux motifs qui couvrent
les mêmes exemples sont redondants car ils appartiennent à la même classe d’équivalence :

Équivalence de motifs au regard d’une base de données : soient P et Q deux motifs et � une
base de données, alors P est équivalent à Q au regard de �, notée P ≡� Q, si P ≤ g Q ou Q ≤ g P
et F r eq(P, �) = F r eq(Q, �)

Dans la suite, plutôt que d’extraire toutes les règles d’une même classe d’équivalence, nous nous
contentrerons des motifs les plus spécifiques car ils sont porteurs de plus de corrélations. Par
ailleurs, nous étendons cette équivalence par une marge de tolérance lors de la comparaison des
fréquences à δ%, ce que nous appellons alors filtrage δ.

3.4       Annotation automatique à partir des règles d’annotation

Les règles d’annotation sont utilisées par mXS pour réaliser l’annotation en entités nommées.
Pour une position j d’un texte, de nombreuses règles peuvent proposer des marqueurs. Nous
estimons la probabilité d’insérer des marqueurs en M j (transductions) par régression logistique,
ce qui nous permet de tenir compte de la multiplicité des règles P ∈ � j selon la formule :
1
P(m ∈ M j |� j ) = Z(� )
· exp P∈� j λ P,m
j
Dans une annotation (et plus particulièrement si elle est structurée), plusieurs marqueurs peuvent
se trouver à une position donnée. Il nous faut être en mesure de faire le lien entre la probabilité
d’insérer un marqueur individuel et celle d’insérer une séquence de marqueurs. Pour cela, nous
tenons compte des statistiques issues du corpus sous forme de probabilités conditionnelles 9 :
1 �p
P(M j = m1 m2 . . . m p ) = ·    P(mk ∈ Mk |�k )P(m1 . . . m p |mk )
p k=1
Lorsque les probabilités de séquences de marqueurs P(Mi ) sont estimées, nous les utilisons afin
de déterminer quelle est, pour un énoncé donné, l’annotation la plus vraisemblable parmi les
annotations valides. Une hypothèse d’indépendance entre marqueurs au sein d’un énoncé nous
permet de résoudre la recherche de l’annotation par programmation dynamique.
4        Expériences sur le corpus Etape

4.1       Données

Corpus    Sources(nombre de fichiers)                         Tokens     Énoncés     EN
Etape-Train     BFMTV (5), France Inter (16), LCP (23)             355 975     14 989    46 259
Etape-Dev     BFMTV (1), France Inter (6), LCP(6), TV8 (2)       115 530     5 724     14 112
Etape-Test     BFMTV (1), France Inter (6), LCP (5), TV8 (2)      123 221     6 770     13 055
Total   74 enregistrements                                 594 726     27 483    73 426
Etape-Quaero     France Classique (1), France Culture (1), France   1 596 427   43 828    279 797
Inter (62), France Info (13), RFI (14), RTM (97)

TABLE 1 – Caractéristiques du corpus Etape

Le travail a été réalisé dans le contexte de la campagne d’évaluation Etape 10 , en interaction avec
le programme Quaero 11 . Cette campagne a porté sur le traitement d’émissions radiodiffusées et
télévisuelles, donc orales et en partie spontanées. L’objectif est d’annoter les entités nommées
structurées, tant sur les transcriptions manuelles qu’en sortie de systèmes de reconnaissance
de la parole. La table 1 indique les parties à disposition. Le corpus Etape-Test étant en
cours d’adjudication, nous ne l’utilisons pas pour mener nos expériences. Etape-Quaero 12
est volumineux et reste difficile à exploiter par la fouille. En conséquence, nous n’utilisons que
Etape-Train (extraction des règles et paramétrage du modèle) et Etape-Dev (évaluation).
9.   Ces probabilités sont normalisées a posteriori
10.   Évaluations en Traitement Automatique de la Parole (2011-2012)
11.   http://www.quaero.org (2008-2013)
12.   Adaptation du corpus Ester au format Etape

Les types principaux d’entités nommées sont les personnes (pers)), fonctions (fonc), organisa-
tions (org), lieux (loc), productions humaines (prod), points dans le temps (time), quantités
(amount) et évènements (event). À granularité fine (sur laquelle est réalisée l’évaluation), ils
sont répartis en 34 sous-types. La figure 4 indique leur répartition au sein du corpus Etape.
Notons que les entités nommées sont étendues à des expressions construites à partir de noms
communs, ce qui amène à considérer une large gamme d’expressions linguistiques.
13%                      10%                     12%
13%
11%          11%                      11%         14%
12%         prod
8%
amount
7%
15%
7%
15%                                               fonc
13%                      loc
13%                      14%                      12%       org
30%                     30%                       29%               pers
time

(a) Etape-Train          (b) Etape-Dev                    (c) Etape-Test
FIGURE 4 – Répartition des types principaux d’entités

En plus des entités nommées, leurs composants sont annotés, soit spécifiques à certains types
(jour, mois, etc. pour une date) ou transverses (valeur, unité, qualificateur, etc.). Ces éléments
permettent de mieux décrire les entités lors de leur annotation (Rosset et al., 2011).
Le nombre d’entités nommées rapporté au nombre de tokens du corpus est de 12,3%, dont 4,8%
pour les entités et 7,5% pour les composants. Globalement, ce corpus, quoiqu’assez volumineux,
est bien équilibré pour les types principaux d’entités et de composants. Notons que nous réalisons
l’exploration des données sur un corpus qui contient des disfluences, répétitions, etc.
4.2     Extraction de règles d’annotation

Pour implémenter la fouille de données, nous construisons un arbre des préfixes communs
par niveaux, le processus est optimisé en exploitant la propriété d’anti-monotonie (Agrawal et
Srikant, 1995) et les hiérarchies (Wang et Han, 2004). De plus, nous poussons deux contraintes
supplémentaires pour l’extraction des règles d’annotation :
– Nombre de marqueurs : une règle d’annotation partielle ne contient qu’un marqueur.
– Niveaux : le nombre d’itérations de l’algorithme par niveaux est limité à 7.
L’approche que nous adoptons nous permet d’explorer exhaustivement les motifs fréquents et
confiants. Les seuils minimaux sont fixés à 3 en fréquence et 5% en confiance. Le système extrait
alors 143 205 règles d’annotation partielles 13 . La figure 5 montre que la longueur des règles varie
autour de trois éléments, et leur profondeur d’items 14 se situe autour de quatre. Ces statistiques
confirment que les règles d’annotation sont explorées sur les deux axes que nous avons définis.
Nous voyons aussi que la répartition des règles d’annotation par types d’EN est diversement
corrélée au corpus. Les types time et amount sont moins représentés : il y a moins de descripteurs
pour ces types, il pourrait alors être assez homogène dans les données. Inversement, le type
prod, est sur-représenté et nous faisons l’hypothèse qu’il est assez hétérogène.
13. En 15 minutes, sur un seul cœur, en consommant 1,5Go de RAM
14. Somme sur les items des spécialisations au delà de la racine

·104                                            ·104
4
100 ≤ F                                          100 ≤ F                                     6%
10%
6                          30 ≤ F < 100                                     30 ≤ F < 100
3 ≤ F < 30          3                            3 ≤ F < 30                  12%                     19%
prod
4                                                                                                                                              amount
2                                                                                              fonc
NB
NB
4%      loc
16%                                   org
2                                               1                                                                                              pers
33%                      time
0                                               0
2         4        6                     0           5            10              15

(a) Longueur des règles                        (b) Profondeur des règles                                 (c) Types d’entités des règles
FIGURE 5 – Caractéristiques des règles d’annotation extraites
4.3         Reconnaissance d’entités nommées

Nous utilisons l’outil scikit-learn 15 (Pedregosa et al., 2011) pour réaliser la régression logistique.
La figure 6 présente les résultats obtenus en SER 16 et les taux par types d’erreurs (Galibert et al.,
2011). Ces graphiques confirment que le système réduit graduellement ses erreurs à mesure que
les seuils de fréquence et de confiance sont abaissés.
100                                                                     100
F   ≥ 21                                                             Insertions
80                                       F   ≥ 42                       80                                     Délétion
F   ≥ 63                                                            Substitutions
Taux d’erreurs
60                                       F   ≥ 84                       60
SER
40                                                                      40

20                                                                      20

0                                                                       0
80        60        40          20                                       80     60             40            20
C                                                                           C
(a) Performances                                                       (b) Erreurs, F ≥ 21
FIGURE 6 – Performances (SER) et erreurs selon la Fréquence (F) et la Confiance (C)

Nous menons des expériences supplémentaires , dont les résultats sont reportés dans le tableau 2
pour les configurations suivantes :

–    Logit : système par défaut
–    Logit-Dicos : désactivation des ressources lexicales
–    Logit+Test : apprentissage en fusionnant les corpus Etape-Train et Etape-Dev
–    Logit-D25 : filtrage δ à 25%,
–    Logit-D50 : filtrage δ à 50%,
–    Logit-D75 : filtrage δ à 75%,

15. http://scikit-learn.org
16. Slot Error Rate, taux d’erreur pondéré

Le système donne des résultats satisfaisants, étant donné la difficulté de la tâche. Sans surprise, la
désactivation des dictionnaires dégrade considérablement les performances. Lorsque les données
comportent les données d’évaluation (Logit+Test), le surapprentissage est modéré, ce qui est
lié au fait que les règles d’annotation ne sont pas lexicalisées. Les expériences Logit-DXX nous
montrent clairement que le système obtient encore des performances très acceptables lorsque
l’on réduit significativement le nombre de règles extraites à l’aide du filtrage δ.

Approche     Règles      SER     I          D      S      P       R     Fm
Logit       143 205     35,9   5,6        24,2   10,8   79,8   64,9    71,6
Logit-Dicos    80 231      45,2   5,9        30,2   16,3   70,7   53,5    60,9
Logit+Test     141 550     26,3   3,2        18,6   8,1    86,6   73,3    79,4
Logit-D25     100 027     36,2   5,6        24,6   10,9   79,7   64,6    71,3
Logit-D50     73 332      36,7   5,4        25,2   11,0   79,5   63,8    70,8
Logit-D75     50 408      39,0   5,4        27,0   11,7   78,2   61,3    68,7

TABLE 2 – Performances (SER), erreurs d’Insertion (I), de Délétion (D), de Substitution (S),
Précision (P), Rappel (R), F-mesure (Fm) des approches

Nous menons des évaluations séparées des types primaires (sans sous-types) d’entités nommées
et de composants. La figure 7 en donne les résultats. Les entités nommées sont moins bien
reconnues que les composants et plusieurs types (en particulier les expressions de temps) posent
encore problème. Ceci dit, le système équilibre relativement bien sa précision et son rappel et la
reconnaissance d’entités nommées selon l’approche présentée donne des résultats.

100
80
Types        SER     P      R        Fm             60
Entités       38,9   76,4   62,3      68,6           40
Composants      33,0   86,4   68,5      76,4           20
Tous        35,9   79,8   64,9      71,6            0
pro amo     f        loc    org     per    tim
d    unt unc                        s      e
FIGURE 7 – SER, précision (gauche) et rappel (droite) par types primaires et composants

La phase d’adjudication de la campagne d’évaluation ETAPE n’est pas achevée à l’heure de
la rédaction de cet article. Nous avons cependant été autorisés à reporter en table 3 les per-
formances anonymes des systèmes avant adjudication. Les SER présentés sont donnés sur les
transcriptions manuelles et sur les sorties de différents systèmes de reconnaissance, pour lesquels
sont mentionnés les WER 17 .
Parmi les autres systèmes participants, le système 3 utilise des CRF (binarisés, un par type), le
système 6/7/8 utilise un CRF pour les composants et un PCFG pour reconstituer les entités,
CasEN utilise des transducteurs. De manière générale, mXS affiche de bonnes performances (entre
la 1 ère et la 3 ème position). Les taux d’erreurs élevés sont liés à la difficulté de la tâche (parole
spontanée, imbrications, typologie fine). Sans surprise, les performances sont dégradées sur
les données bruitées par la reconnaissance de parole. Nous voyons que mXS et résiste bien aux
erreurs de reconnaissance de la parole.
17. Word Error Rate

Part.      Type      Man     Rover    WER23     WER24    WER25     WER30     WER35
1         OC       84.8     98,1    100,7      94,2     98,9      98,4     100,9
2         OC       172.0   147,4    178,8     160,4    168,0     163,9     168,2
3        CRF       33.8     57,2     59,3      64,7     62,0      61,7      71,8
4         OC       55.6     88,0     98,8      76,8     92,8      94,9      99,6
5        CRF       43.6     69,7     73,8      72,1     73,7      74,8      86,0
6     CRF+PCFG      na      79,2     79,5      66,8     80,8      80,0      87,0
7     CRF+PCFG      na      67,8     68,4      67,6     70,9      69,9      85,2
8     CRF+PCFG     36.4      na       na        na       na        na        na
9        CRF       62.8     75,8     79,2      76,9     79,8      80,5      90,5
10         OC       42.9     65,0     69,9      66,3     70,5      69,9      87,0
CasEN        OC       49.3      na       na       68,4      na        na        na
mXS       Règles     41.0     63,7     67,5      64,1     69,1      68,6      80,4

TABLE 3 – SER de la campagne Etape par système (OC=Orienté Connaissances) sur les transcrip-
tions avant adjudication (manuel : Man, transcription automatiques : Rover et WERXX, dont
WER24 avec majuscules)
5    Conclusion

La reconnaissance d’entités nommées structurées sur de la parole spontanée nécessite de mettre
au point des systèmes robustes. Dans cet article, nous présentons une approche originale à base
de fouille de données, qui extrait des règles d’annotation partielles et paramètre un modèle
numérique les utilisant.
Les résultats obtenus dans le cadre de la campagne Etape indiquent que notre approche novatrice
fait jeu égal avec les systèmes état de l’art. Pour éviter tout biais méthodologique, nous restons
toutefois en attente d’une référence débarrassée de toute erreur d’annotation : c’est l’objectif de
la phase d’adjudication en cours. Notre objectif à court terme est de mieux caractériser les points
forts et limitations du modèle (détection séparée du début et de la fin des annotations). Nous
comptons également mettre à l’épreuve le système sur d’autres tâches qui pourraient bénéficier
de l’extraction de motifs de segments.

Remerciements

Ces travaux ont été réalisés dans le cadre du projet ANR Etape. Merci en particulier à Olivier
Galibert (LNE), Matthieu Carré (ELDA) et Guillaume Gravier (IRISA).

Références
AGRAWAL, R. et SRIKANT, R. (1995). Mining sequential patterns. In International Conference on Data
Engineering (ICDE’95), pages 3–14.
BIKEL, D., SCHWARTZ, R. et WEISCHEDEL, R. M. (1999). An algorithm that learns what’s in a name. Machine
Learning, 34:211–231.
BRUN, C. et EHRMANN, M. (2010). Un système de détection d’entités nommées adapté pour la campagne
d’évaluation ester 2. In Traitement Automatique du Langage Naturel (TALN’10).
BÉCHET, F., SAGOT, B. et STERN, R. (2011). Coopération de méthodes statistiques et symboliques pour
l’adaptation non-supervisée d’un système d’étiquetage en entités nommées. In Traitement Automatique des
Langues Naturelles (TALN’11).

CELLIER, P. et CHARNOIS, T. (2010). Fouille de données séquentielles d’itemsets pour l’apprentissage de
patrons linguistiques. In Traitement Automatique des Langues Naturelles (TALN’10).
DINARELLI, M. et ROSSET, S. (2011). Models cascade for tree-structured named entity detection. In
International Joint Conference on Natural Language Processing (IJCNLP’11).
EKBALA, A., SOURJIKOVA, E., FRANK, A. et PONZETTO, S. P. (2010). Assessing the challenge of fine-grained
named entity recognition and classification. In Annual Meeting of the Association for Computational
Linguistics (ACL’10) - Named Entities Workshop, pages 93–101, Uppsala, Sweden.
FINKEL, J. R. et MANNING, C. D. (2005). Nested named entity recognition. In Conference on Empirical
Methods in Natural Language Processing (EMNLP’09).
FISCHER, J., HEUN, V. et KRAMER, S. (2005). Fast frequent string mining using suffix arrays. In 5th IEEE
International Conference on Data Mining (ICDM’05), pages 609–612.
FRIBURGER, N. et MAUREL, D. (2004). Finite-state transducer cascades to extract named entities in texts.
Theoretical Computer Sciences (TCS), 313:93–104.
GALIBERT, O., ROSSET, S., GROUIN, C., ZWEIGENBAUM, P. et QUINTARD, L. (2011). Structured and extended
named entity evaluation in automatic speech transcriptions. In International Joint Conference on Natural
Language Processing (IJCNLP’11).
GALLIANO, S., GRAVIER, G. et CHAUBARD, L. (2009). The ester 2 evaluation campaign for the rich transcription
of french radio broadcasts. In International Speech Communication Association (INTERSPEECH’09).
GRISHMAN, R. et SUNDHEIM, B. (1996). Message undersrtanding conference - 6 : A brief history. In
International Conference on Computational Linguistics (COLING’96), pages 466–471, Copenhagen, Denmark.
HOBBS J. R., Appelt D., B. J. I. D. K. M. S. M. et TYSON, M. (1996). FASTUS : A Cascaded Finite-State
Transducer for Extracting Information from Natural-Language Text, pages 383–406.
MCCALLUM, A., FREITAG, D. et PEREIRA, F. (2000). Maximum entropy markov models for information
extraction and segmentation. In International Conference on Machine Learning (ICML’00), pages 591–598.
MCDONALD, D. D. (1996). Internal and External Evidence in the Identification and Semantic Categorisation
of Proper Names, pages 32–43.
MIKHEEV, A., MOENS, M. et GROVER, C. (1999). Named entity recognition without gazetteers. In Proc. of
the Ninth Conference of the European Chapter of the Association for Computational Linguistics, pages 1–8.
PASQUIER, N., BASTIDE, Y., TAOUIL, R. et LAKHAL, L. (1999). Efficient mining of association rules using closed
itemset lattices. INF. SYST., 24(1):25–46.
PEDREGOSA, F., VAROQUAUX, G., GRAMFORT, A., MICHEL, V., THIRION, B., GRISEL, O., BLONDEL, M., PRETTEN-
HOFER , P., W EISS , R., D UBOURG , V., VANDERPLAS , J., PASSOS , A., C OURNAPEAU , D., B RUCHER , M., P ERROT , M.
et Édouard DUCHESNAY (2011). Scikit-learn : Machine learning in python. Journal of Machine Learning
Research, 12:2825–2830.
RAYMOND, C. et FAYOLLE, J. (2010). Reconnaissance robuste d’entités nommées sur de la parole transcrite
automatiquement. In Traitement Automatique des Langues Naturelles (TALN’10).
ROSSET, S., GROUIN, C. et ZWEIGENBAUM, P. (2011). Entité nommées structurées : guide d’annotation quaero.
Rapport technique, LIMSI (2011-04).
SCHMID, H. (1994). Probabilistic pos tagging using decision trees. In New Meth. in Lang. Proc. (NEMLP’94).
SRIKANT, R. et AGRAWAL, R. (1996). Mining sequential patterns : Generalizations and performance
improvements. In International Conference on Extending Database Technology (EDBT’96), pages 3–17.
STEPHENS, C. S. (1993). The analysis and acquisition of proper names for the understanding of free text.
Computers and the Humanities, 26:441–456.
WANG, J. et HAN, J. (2004). Bide : Efficient mining of frequent closed sequences. In International Conference
on Data Engineering (ICDE’04).
ZIDOUNI, A., ROSSET, S. et GLOTIN, H. (2010). Efficient combined approach for named entity recognition in
spoken language. In Conference of the International Speech Communication Association (INTERSPEECH’10).


Fouille de règles d’annotation partielles
pour la reconnaissance des entités nommées

Damien Nouvel 1, 2           Jean-Yves Antoine 1           Nathalie.Friburger 1
Arnaud.Soulet 1
(1) LI, 3 place Jean Jaurès, 41000 Blois
(2) Alpage, INRIA & Université Paris-Diderot, 75013 Paris
{prenom.nom}@univ-tours.fr

RÉSUMÉ
Ces dernières décennies, l’accroissement des volumes de données a rendu disponible une diversité
toujours plus importante de types de contenus échangés (texte, image, audio, vidéo, SMS, tweet,
données statistiques, spatiales, etc.). En conséquence, de nouvelles problématiques ont vu le
jour, dont la recherche d’information au sein de données potentiellement bruitées. Dans cet
article, nous nous penchons sur la reconnaissance d’entités nommées au sein de transcriptions
(manuelles ou automatiques) d’émissions radiodiffusées et télévisuelles. À cet effet, nous mettons
en œuvre une approche originale par fouille de données afin d’extraire des motifs, que nous
nommons règles d’annotation. Au sein d’un modèle, ces règles réalisent l’annotation automatique
de transcriptions. Dans le cadre de la campagne d’évaluation Etape, nous mettons à l’épreuve
le système implémenté, mXS, étudions les règles extraites et rapportons les performances du
système. Il obtient de bonnes performances, en particulier lorsque les transcriptions sont bruitées.
ABSTRACT
Mining Partial Annotation Rules for Named Entity Recognition
During the last decades, the unremitting increase of numeric data available has led to a more
and more urgent need for efficient solution of information retrieval (IR). This paper concerns a
problematic of first importance for the IR on linguistic data : the recognition of named entities
(NE) on speech transcripts issued from radio or TV broadcasts. We present an original approach for
named entity recognition which is based on data mining techniques. More precisely, we propose to
adapt hierarchical sequence mining techniques to extract automatically from annotated corpora
intelligible rules of NE detection. This research was carried out in the framework of the Etape NER
evaluation campaign, where mXS, our text-mining based system has shown good performances
challenging the best symbolic or data-driven systems
MOTS-CLÉS : Entités nommées, Fouille de données, Règles d’annotation.
KEYWORDS: Named Entities, Data Mining, Annotation Rules.
1    Introduction

Ces dernières décennies, le développement considérable des technologies de l’information et de la
communication a modifié la manière dont nous accédons et manipulons les connaissances. Nous
constatons une diversité toujours plus importante des types de contenus échangés (texte, image,
audio, vidéo, SMS, tweet, données statistiques, spatiales, etc.), ce qui nécessite de résoudre
de nombreuses problématiques, parmi lesquelles la recherche d’information, qui a intéressé la
communauté du TALN dès les années 90 avec les campagnes d’évaluation MUC (Grishman et
Sundheim, 1996). Les travaux sur le sujet ont porté une attention particulière aux noms propres
de personnes, de lieux et d’organisations, appelés entités nommées (EN). Au gré des besoins,
celles-ci ont été étendues aux dates, aux expressions numériques, aux marques ou aux fonctions,
avant de recouvrir un large spectre d’expressions linguistiques.

De nombreux systèmes ont été élaborés pour réaliser la reconnaissance d’entités nommées (REN),
selon des approches orientées connaissances ou orientées données. Les premières ont générale-
ment une grande précision mais nécessitent un coup humain de développement important, ce
qui se traduit généralement par une couverture (et donc un rappel) perfectible. Les approches
orientées données, par ajustement automatique de paramètres d’un modèle numérique, per-
mettent d’obtenir de bonnes performances, avec un coup d’entrée limité, du moment où l’on
dispose d’une base d’apprentissage de taille suffisante. Ils sont également réputés présenter une
dégradation graduelle de leurs performances sur des données bruitées. Cependant, l’aspect “boîte
noire” des algorithmes d’apprentissage rend difficile l’amélioration ciblée de leurs performances.

Ces constats ont été vérifiés par de nombreuses campagnes d’évaluation. À titre d’exemple, lors
de la campagne d’évaluation francophone Ester2 (Galliano et al., 2009), portant sur le traitement
de transcriptions de parole radio ou télédiffusée, les deux meilleurs systèmes travaillant sur
des transcriptions manuelles étaient des systèmes à base de connaissance, tandis que les tests
effectués sur des sorties de reconnaissance de la parole ont été dominés par un système orienté
données.

Les travaux que nous présentons dans cet article ont été menés dans le cadre de la campagne
Etape (qui a fait suite à Ester2) qui visait notamment à évaluer des systèmes de REN sur des flux
de parole conversationnelle. Nous y proposons une approche novatrice pour la REN : l’utilisation
de méthodes de fouille de données séquentielle hiérarchique. À nos yeux, ces travaux présentent
plusieurs originalités du point de vue du TALN :
(i) nous élaborons un moyen-terme entre les approches orientées données et orientées connais-
sances reposant sur la recherche, à partir de données d’apprentissage, de motifs pour la REN :
cette technique centrée données permet l’extraction de connaissances interprétables ;
(ii) la stratégie de détection des entités nommées est originale, par la recherche séparée du
début et de la fin des entités, en nous appuyant sur le contexte immédiat pour placer les balises
d’annotation : cela présente l’intérêt de conserver une certaine robustesse en cas de disfluence
ou d’erreur de reconnaissance au sein de l’entité nommée.

Cet article porte sur l’élaboration, l’implémentation et l’évaluation d’une telle approche. En partie
2, nous faisons un état de l’art des approches pour la REN. La partie 3 présente le formalisme de
fouille pour l’extraction de règles d’annotation et leur utilisation pour reconnaître des entités
nommées. En partie 4, nous décrivons le jeu de données utilisé et les résultats obtenus lors de
l’évaluation dans le cadre de la campagne Etape.
2      Approches pour la reconnaissance d’EN structurées

2.1      Approches orientées connaissances

Les approches orientées connaissances sont basées sur la description de règles décrivant les
entités nommées et leur contexte à l’aide d’indices linguistiques fournis par le texte lui-même et
des ressources externes (dictionnaires). Généralement, les textes sont étiquetés syntaxiquement
(éventuellement sémantiquement) grâce aux dictionnaires, puis un ensemble de règles, qui
prennent en compte les indices morphologiques (présence de majuscule, ponctuation), morpho-
syntaxiques et sémantique, permettent de repérer les ENs. Les règles utilisent ces éléments, soit
comme preuves internes de la présence d’une entité nommée, soit par description de son contexte
d’apparition (McDonald, 1996; Friburger et Maurel, 2004). Une preuve interne sera, par exemple,
la présence d’un prénom avant un mot commençant par une majuscule ; ce prénom indiquera un
nom de personne (ex : ’François Hollande’). Nous voyons que c’est la “connaissance” qui guide
cette approche, celle de l’expert qui créé les règles, selon les informations à sa disposition (dont
les ressources externes).

Dès les années 1990, un certain nombre de systèmes (Stephens, 1993; Hobbs J. R. et Tyson, 1996)
mettent en œuvre cette approche orientée connaissances. Les automates sont particulièrement
adaptés à l’élaboration et l’utilisation des règles. De plus, l’utilisation de transducteurs 1 permet
de produire très intuitivement une annotation à l’aide de balises (‘<pers>’, ‘</pers>’, ‘<org>’,
‘</org>’, etc.), ils sont donc largement utilisés pour ce type de tâche (Friburger et Maurel, 2004;
Brun et Ehrmann, 2010; Béchet et al., 2011). Enfin, les transducteurs peuvent être organisés
sous forme de cascades, chaque transducteur permettant de lever des ambiguïtés et de mettre à
disposition des reconnaissances pour les transducteurs suivants (ce qui permet de reconnaître des
imbrications). L’ordre dans lequel sont appliqués les transducteurs a alors une grande importance.

Étant donné les traitements qu’elles mettent en œuvre, les approches orientées connaissances
insèrent au sein des séquences de mots ce que nous appelons des marqueurs, comme le montre la
figure 1 pour l’expression ‘fondation Cartier’.

ORG
PERS
La           Fondation        Cartier              pour     l’   art   contemporain       .
<org>                                                                            </org>
<pers>          </pers>
FIGURE 1 – Annotation par balises

Les approches orientées connaissances peuvent être utilisées et adaptées à des textes sans
apprentissage préalable. Leur limitation est liée au fait que les ressources utilisées sont rarement
exhaustives (par exemple, les noms propres forment une classe “ouverte”) : il semble illusoire de
bâtir ce type d’approche sur l’hypothèse d’un lexique complet des entités nommées existantes.

1. Automates qui modifient le texte fourni en entrée par insertion de balises

2.2    Approches orientées données

Les approches orientées données paramètrent un modèle automatiquement grâce à un apprentis-
sage sur un corpus d’entraînement. Ce corpus d’entrainement, créé par des experts, fournit de
nombreux exemples de données : le système apprend sur ces exemples puis prédit l’étiquette
d’une nouvelle donnée, selon son modèle. Le corpus d’entrainement est constitué d’un ensemble
de textes annotés en entités nommées par des experts. L’apprentissage automatique sera chargé
d’ajuster les paramètres disponibles, cette procédure étant guidée à chaque itération par les
erreurs que commet le système sur les jeux de données disponibles. Une fois l’apprentissage
réalisé, le système est en mesure d’annoter de nouveaux textes en entités nommées selon les
paramètres de son modèle. Traditionnellement, l’apprentissage automatique se rapproche plutôt
d’une classification (attribution d’une classe à un mot) que d’une annotation (délimitation d’une
expression linguistique).
Pour la REN, le format BIO 2 s’est imposé. La figure 2 présente la classification par mots réalisée
pour l’énoncé ‘<org> fondation <pers> Cartier </pers> </org>’. Signalons qu’en partie
3, nous présentons une approche orientée donnée, mais qui est apparentée à un mécanisme de
transduction (à l’aide d’indices locaux) plutôt que de classification.

B-ORG
PERS
La      Fondation       Cartier    pour         l’     art    contemporain   .
O         I-ORG         I-ORG      I-ORG      I-ORG   I-ORG      I-ORG       O
B-PERS
FIGURE 2 – Annotation par classification

Généralement, ces approches estiment la probabilité des classes selon les tokens et les informa-
tions qui y sont associées. Parmi les modèles numériques adaptés, figurent les modèles bayésiens,
la régression logistique (ou maximum d’entropie), les machines à vecteur de support (SVM) ,
etc. La régression logistique a démontré son efficacité pour la reconnaissance d’entités nommées
(Mikheev et al., 1999; Ekbala et al., 2010), permettant de prendre en compte de multiples
traits discriminants (morphologiques, morpho-syntaxiques, lexicaux) interdépendants. D’autres
modèles tirent parti de la séquentialité, comme les HMM (Bikel et al., 1999), par modélisation
des transitions entre états (types d’entités nommées) et des générations d’observations (mots).
Pour prendre en compte simultanément la multiplicité des indices locaux et les aspects séquentiels
au sein d’un modèle unifié, les MEMM 3 (McCallum et al., 2000) puis les CRF 4 (Raymond
et Fayolle, 2010; Zidouni et al., 2010) sont les modèles réputés les plus adéquats à ce jour.
L’inconvénient est qu’ils restent difficiles à interpréter : les traits découverts sont généralement
composites et exhibent des dépendances complexes dont il est difficile d’affirmer qu’elles sont
nécessaires ou suffisantes pour déterminer les entités nommées.
A ce jour, les approches orientées données se basent majoritairement sur une représentation
“plate” des entités nommées. Comme nous le verrons en partie 4, nous cherchons à réaliser la
2. Begin, Inside, Outside
3. Modèles markoviens à maximum d’entropie
4. Champs aléatoires conditionnels

REN structurée (avec imbrications). Notons que quelques travaux (Finkel et Manning, 2005;
Dinarelli et Rosset, 2011) ont adapté avec un certain succès des méthodes orientées données à la
reconnaissance de structure.

De manière générale, nous remarquons que les approches automatiques nécessitent un travail
préalable conséquent (préparation des jeux de données, implémentation du modèle, des procé-
dures d’apprentissage et d’estimation, sélection des traits et dépendances pertinents, etc.) avant
d’être en mesure de paramétrer les modèles, et qu’il reste difficile de les utiliser pour extraire des
connaissances ou pour étudier des phénomènes particuliers.
2.3    Proposition : les marqueurs d’annotation

Nous le voyons, les approches guidées par les données s’appuient sur des indices locaux variés.
La nature “locale” de la structuration en entités nommées est alors un atout. Les systèmes
orientés connaissances ont l’avantage de modéliser la structure interne des entités nommées.
Ainsi un système à base de connaissances aura plus de facilité à analyser l’encapsulation d’entités
nommées comme dans l’exemple suivant (issu d’Etape) : ‘le député UMP de Haute-Saône’ où
l’entité nommée globale est construite à l’aide de l’entité ‘UMP’, de type organisation, et de l’entité
‘Haute-Saône’, de type division géographique administrative.

Cependant, ces dernières approches utilisent une connaissance dont la construction est coûteuse et
délicate. Aussi avons-nous souhaité développer une approche permettant l’extraction automatique
sur corpus de motifs se rapprochant des règles de reconnaissance mises en œuvre par la REN
symbolique. La fouille hiérarchique séquentielle de données est adéquate à cet effet.

Par ailleurs, les systèmes orientés connaissances sont aujourd’hui contraints à modéliser inté-
gralement la structure des entités, voire de ses contextes d’introduction. Ce choix est discutable
et met à l’épreuve la robustesse des systèmes lorsqu’ils traitent de la parole spontanée. Une
erreur de reconnaissance sur un seul mot de l’entité (dûe par exemple à une disfluence) empêche
l’application de la règle de détection.

Afin de répondre à cette insuffisance, nous proposons de séparer la détection du début et de
la fin de l’entité, pour ensuite chercher à associer une marque de début et de fin d’entité. Notre
hypothèse est que l’on dispose de suffisamment d’indices locaux pour caractériser précisément le
début ou la fin d’une entité.

Considérons pas exemple l’énoncé annoté suivant ‘En <date> <num> 1969 </num> </date>
<pers> <prenom> Georges </prenom> <famille> Pompidou </famille> </pers> dirige la
<org> <loc> France </loc> </org>’. Notre hypothèse est que chacune des marques d’annota-
tion (‘<pers>’, ‘<prenom>’, ‘</prenom>’, ‘</pers>’, etc.) est détectable séparément. De plus,
la détection d’une entité encapsulée telle que ‘<prenom>’ peut guider la détection de l’entité
englobante. Il s’agira, pour le système, d’extraire des règles d’annotation, d’estimer localement
les marqueurs probables, puis de déterminer, par leurs combinaisons, l’annotation la plus vrai-
semblable. Nous implémentons un système de reconnaissance d’entités nommées, mXS, selon
cette approche originale. Grâce à ce procédé, notre système reconnaît par exemple le montant
‘deux cent ça compte mille’ (erreur de transcription pour deux cent cinquante mille), alors qu’un
système symbolique sera mis en difficulté.
3      Extraction de règles d’annotation pour la REN

3.1     Enrichissement ambigu des données
axe paradigmatique

PRENOM                                      BAT          BAT        CELEB
Sémantique

NP       VER       VER      DET           NC                 NP
Morpho-syntaxe
axe structurel

avoir    visiter                                                      Lemmatisation

Pierre      a      visité     le          Centre          Pompidou               Tokenisation
FIGURE 3 – Représentation des structures à fouiller

L’approche que nous mettons en œuvre repose sur des analyses fréquemment conduites pour
traiter le langage naturel (morpho-syntaxe, lexiques). Pour la fouille, ces traitements sont
interprétés comme autant d’enrichissements des données, à utiliser pour rechercher des motifs
généralisés dans les données. La figure 3 présente de manière schématique, sur l’exemple ‘Pierre
a visité le Centre Georges Pompidou’, la manière dont se superposent ces enrichissements.
La fouille de données devra alors tenir compte de deux axes : paradigmatique, pour la superposi-
tion d’enrichissements, et structurel, pour l’examen des contigüités entre items. Comme nous le
verrons par la suite, ce processus est flexible : les enrichissements peuvent être plus ou moins
profonds selon les éléments considérés. Nous pouvons moduler à volonté l’axe paradigmatique
selon les éléments observés et la tâche d’annotation à réaliser.
3.1.1    Morpho-syntaxe

Nous réalisons conjointement la tokenisation, la lemmatisation et l’étiquetage morpho-syntaxique
avec TreeTagger (Schmid, 1994). De surcroît, nous en adaptons la sortie comme suit :
– Déterminants : les déterminants définis (‘le’, ‘la’, ‘les’, ‘l”) sont sous-catégorisés en ‘DET/DEF’.
– Prépositions : la sous-catégorie ‘PRP:det’ (‘au’, ‘du’, ‘des’) forme une catégorie ‘PRPDET’.
– Nombres : les nombres sont sous-catégorisés selon leur nombre de chiffres 5 .
– Noms propres et abréviations : ces deux catégories se généralisent en ‘NAMABR’.
– Nom propres, abréviations, noms, verbes : ces éléments sont sous-catégorisés par le suffixe
des trois derniers caractères (‘NOM/SUFF:ier’, ‘NAMABR/NAM/SUFF:ges’, ‘VER/SUFF:vre’).
– Verbes : les sous-catégories relatives au mode et temps du verbe sont supprimées.
Pour le processus de fouille de données, nous omettons les variations surfaciques (majuscules)
et flexionnelles (déclinaisons et conjugaisons) : nous ne conservons pas les items lexicaux eux-
mêmes et faisons reposer la recherche de motifs sur les lemmes proposés par TreeTagger. Par
exemple, le ‘En 1970 les socialistes [...]’ donnera la séquence :
‘PRP/en NUM/DIGITS:4/PREF:19/1970 DET/DEF/le NOM/SUFF:ste/socialiste’.
5. Ce nombre est précisé s’il est inférieur ou égal à quatre le préfixe est utilisé dans ce dernier cas :
‘NUM/DIGITS:MANY’, ‘NUM/DIGITS:4/PREF:20’ . . ., ‘NUM/DIGITS:1’)

3.1.2    Lexiques

Les lexiques nous permettent d’ajouter un niveau sémantique aux hiérarchies. Nous exploitons
des ressources diverses, dont certaines sont importées à partir des dictionnaires et motifs du
système CasEN 6 . Nous y ajoutons quelques listes, constituées manuellement, en particulier
pour les fonctions, lieux, organisations, quantités et dates. Ces ressources contiennent 221
547 expressions distinctes qui produisent 443 112 catégorisations sémantiques 7 . Une large
part est dédiée à la reconnaissance des personnes et des lieux. Signalons qu’une partie de
ces ressources est générée à partir d’automates (transducteurs CasEN) qui reconnaissent des
expressions linguistiques utiles à la REN.
Ces ressources sont utilisées telles quelles pour produire les enrichissements. Ceux-ci peuvent
alors être sémantiquement ambigus, ce que nous notons comme une disjonction exclusive
⊕. Par exemple, au nom propre Washington seront affectées les catégories sémantiques
‘CELEB⊕TOPO⊕ORG-LOC-GOV⊕PREN⊕VILLE’. Notons ici que nous considérons que les noms
propres forment une classe ouverte et qu’ils n’ont pas vocation à être utilisés lexicalisés au
sein des motifs extraits : lorsqu’ils ont donné lieu à des enrichissements sémantiques, les items
lexicaux sont omis afin que la fouille de données ne repose que sur les catégories sémantiques.
3.2     Exploration de règles d’annotation de segments

Les données ainsi enrichies forment le langage � r et ont vocation a être fouillées afin d’y
rechercher des motifs séquentiels d’intérêt (Fischer et al., 2005; Cellier et Charnois, 2010) pour
la REN.
Le langage des motifs � p+ comprend celui des données enrichies et toutes leurs généralisa-
tions. Un élément de motif (item) couvre une donnée, notée ≤ci , lorsqu’il s’y trouve en tenant
compte des disjonctions ⊕. Par exemple, l’item ‘TOPO/Washington’ couvre la donnée enrichie
‘CELEB/Washington⊕TOPO/Washington’. Dès lors, nous nous inspirons de travaux intégrant
des hiérarchies aux séquences (Srikant et Agrawal, 1996), en y ajoutant la notion de segment 8
particulièrement adaptée au traitement de structures au sein desquelles des items se répètent
(comme des syntagmes sémantiquement catégorisés).
Couverture d’un motif de segments sur des données : soient un motif de segments P =
p1 p2 . . . pn ∈ � r et une séquence de la base de données enrichie I = i1 i2 . . . i p ∈ � p+ , alors P
couvre les segments de I, noté P ≤c + I, s’il existe une fonction discrète croissante S() définie de
[1, p] vers [1, n] telle que, pour tout j ∈ [1, p], alors p j ≤ci iS( j)
Ce même mécanisme sera pris en compte lorsqu’il s’agit de généraliser selon l’axe paradigmatique :
l’objectif est que, par exemple, ‘CELEB’ couvre indifféremment ‘Pompidou’ et ‘Valery Giscard
d’Estaing’. Plus généralement, nous définissons trois relations de généralisation entre motifs :
– Généralisation hiérarchique entre motifs de segments : soient deux motifs de segments
P = p1 p2 . . . pn ∈ � p+ et Q = q1 q2 . . . q p ∈ � p+ , alors P généralise hiérarchiquement les segments
de Q, noté P ≤ g Q, s’il existe une une fonction discrète croissante S() définie de [1, p] vers
[1, n] telle que, pour tout j ∈ [1, p], alors p j ≤ci qS( j) .
6. http ://tln.li.univ-tours.fr/Tln_CasEN.html
7. Il est fréquent que plusieurs catégories sémantiques soient associées aux entrées
8. Pour respecter l’anti-monotonie, deux items contigus ne peuvent être identiques ou parents l’un de l’autre

– Généralisation par affixation entre motifs : soient deux motifs P = p1 p2 . . . pn ∈ � p+ et
Q = q1 q2 . . . q p ∈ � p+ , alors P généralise par affixation Q, noté P ≤ g Q, si p ≥ n et s’il existe au
moins un k ∈ [0, p − n] tel que, pour tout j ∈ [1, n], alors q j+k = p j .
– Généralisation sur marqueurs entre motifs : soient deux motifs P = p1 p2 . . . pn ∈ � p+ et
Q = q1 q2 . . . q p ∈ � p+ , alors P généralise sur marqueurs Q, noté P ≤ g Q, si p ≥ n et s’il existe
une fonction discrète strictement croissante C() définie de [1, n] vers [1, p] telle que, pour tout
j ∈ [1, n], alors p j = qC( j) et, pour tout k ∈ [1, p] tel que k �∈ {C( j), j ∈ [1, n]}, alors qk ∈ Σm .

Ces généralisations nous permettent de rechercher des motifs dans lesquels apparaissent les
marqueurs d’entités nommées. Par exemple, au sein de l’énoncé ‘Le <fonc> président </fonc>
<pers> Georges Pompidou </pers> débattait souvent.’, nous relevons, par relations de couverture
et de généralisation, une occurrence pour les motifs ‘NOM/président <pers> CELEB </pers>’
ou ‘NOM/président CELEB </pers> VERB/débattre’, par exemple.

Finalement, La notion de règle d’annotation partielle découle de celle de motif de segments :

Règle d’annotation partielle une règle d’annotation partielle est un motif de segments P ∈ � p+
contenant au moins un élément de Σ r et un élément de Σm .

Notons qu’à ce stade les règles d’annotation contiennent un nombre indéterminé de marqueurs.
Il conviendra de filtrer au besoin lors de l’extraction des motifs et de s’assurer que l’on utilise ces
règles de manière adéquate afin de produire une annotation.
3.3    Filtrage et extraction de règles d’annotations partielles

La combinatoire du langage � p+ étant importante, il est nécessaire de filtrer les règles. Pour cela,
nous déterminons la fréquence et la confiance des règles, afin d’éliminer celles qui n’ont que peu
d’intérêt. À l’aide de la couverture et des généralisations définies ci-dessus, nous déterminons la
fréquence F r eq(P, �) d’une règle P comme son nombre d’occurrences au sein du corpus �. La
confiance d’une règle d’annotation P estime la proportion de phrases où la règle est appliquée
avec justesse :
F r eq(P, �)
C on f (P, �) =                      (la fonction Ret m (P) retire les marqueurs de P)
F r eq(Ret m (P), �)
Même en fixant des seuils de support et confiance sélectifs, les règles d’annotation peuvent être
trop nombreuses à cause des combinaisons possibles au travers de la hiérarchie. Afin de contenir
cette abondance de règles, nous proposons de grouper les règles, puis d’éliminer celles qui ne sont
pas informatives, à l’instar de (Pasquier et al., 1999). L’idée forte est que deux motifs qui couvrent
les mêmes exemples sont redondants car ils appartiennent à la même classe d’équivalence :

Équivalence de motifs au regard d’une base de données : soient P et Q deux motifs et � une
base de données, alors P est équivalent à Q au regard de �, notée P ≡� Q, si P ≤ g Q ou Q ≤ g P
et F r eq(P, �) = F r eq(Q, �)

Dans la suite, plutôt que d’extraire toutes les règles d’une même classe d’équivalence, nous nous
contentrerons des motifs les plus spécifiques car ils sont porteurs de plus de corrélations. Par
ailleurs, nous étendons cette équivalence par une marge de tolérance lors de la comparaison des
fréquences à δ%, ce que nous appellons alors filtrage δ.

3.4       Annotation automatique à partir des règles d’annotation

Les règles d’annotation sont utilisées par mXS pour réaliser l’annotation en entités nommées.
Pour une position j d’un texte, de nombreuses règles peuvent proposer des marqueurs. Nous
estimons la probabilité d’insérer des marqueurs en M j (transductions) par régression logistique,
ce qui nous permet de tenir compte de la multiplicité des règles P ∈ � j selon la formule :
1
P(m ∈ M j |� j ) = Z(� )
· exp P∈� j λ P,m
j
Dans une annotation (et plus particulièrement si elle est structurée), plusieurs marqueurs peuvent
se trouver à une position donnée. Il nous faut être en mesure de faire le lien entre la probabilité
d’insérer un marqueur individuel et celle d’insérer une séquence de marqueurs. Pour cela, nous
tenons compte des statistiques issues du corpus sous forme de probabilités conditionnelles 9 :
1 �p
P(M j = m1 m2 . . . m p ) = ·    P(mk ∈ Mk |�k )P(m1 . . . m p |mk )
p k=1
Lorsque les probabilités de séquences de marqueurs P(Mi ) sont estimées, nous les utilisons afin
de déterminer quelle est, pour un énoncé donné, l’annotation la plus vraisemblable parmi les
annotations valides. Une hypothèse d’indépendance entre marqueurs au sein d’un énoncé nous
permet de résoudre la recherche de l’annotation par programmation dynamique.
4        Expériences sur le corpus Etape

4.1       Données

Corpus    Sources(nombre de fichiers)                         Tokens     Énoncés     EN
Etape-Train     BFMTV (5), France Inter (16), LCP (23)             355 975     14 989    46 259
Etape-Dev     BFMTV (1), France Inter (6), LCP(6), TV8 (2)       115 530     5 724     14 112
Etape-Test     BFMTV (1), France Inter (6), LCP (5), TV8 (2)      123 221     6 770     13 055
Total   74 enregistrements                                 594 726     27 483    73 426
Etape-Quaero     France Classique (1), France Culture (1), France   1 596 427   43 828    279 797
Inter (62), France Info (13), RFI (14), RTM (97)

TABLE 1 – Caractéristiques du corpus Etape

Le travail a été réalisé dans le contexte de la campagne d’évaluation Etape 10 , en interaction avec
le programme Quaero 11 . Cette campagne a porté sur le traitement d’émissions radiodiffusées et
télévisuelles, donc orales et en partie spontanées. L’objectif est d’annoter les entités nommées
structurées, tant sur les transcriptions manuelles qu’en sortie de systèmes de reconnaissance
de la parole. La table 1 indique les parties à disposition. Le corpus Etape-Test étant en
cours d’adjudication, nous ne l’utilisons pas pour mener nos expériences. Etape-Quaero 12
est volumineux et reste difficile à exploiter par la fouille. En conséquence, nous n’utilisons que
Etape-Train (extraction des règles et paramétrage du modèle) et Etape-Dev (évaluation).
9.   Ces probabilités sont normalisées a posteriori
10.   Évaluations en Traitement Automatique de la Parole (2011-2012)
11.   http://www.quaero.org (2008-2013)
12.   Adaptation du corpus Ester au format Etape

Les types principaux d’entités nommées sont les personnes (pers)), fonctions (fonc), organisa-
tions (org), lieux (loc), productions humaines (prod), points dans le temps (time), quantités
(amount) et évènements (event). À granularité fine (sur laquelle est réalisée l’évaluation), ils
sont répartis en 34 sous-types. La figure 4 indique leur répartition au sein du corpus Etape.
Notons que les entités nommées sont étendues à des expressions construites à partir de noms
communs, ce qui amène à considérer une large gamme d’expressions linguistiques.
13%                      10%                     12%
13%
11%          11%                      11%         14%
12%         prod
8%
amount
7%
15%
7%
15%                                               fonc
13%                      loc
13%                      14%                      12%       org
30%                     30%                       29%               pers
time

(a) Etape-Train          (b) Etape-Dev                    (c) Etape-Test
FIGURE 4 – Répartition des types principaux d’entités

En plus des entités nommées, leurs composants sont annotés, soit spécifiques à certains types
(jour, mois, etc. pour une date) ou transverses (valeur, unité, qualificateur, etc.). Ces éléments
permettent de mieux décrire les entités lors de leur annotation (Rosset et al., 2011).
Le nombre d’entités nommées rapporté au nombre de tokens du corpus est de 12,3%, dont 4,8%
pour les entités et 7,5% pour les composants. Globalement, ce corpus, quoiqu’assez volumineux,
est bien équilibré pour les types principaux d’entités et de composants. Notons que nous réalisons
l’exploration des données sur un corpus qui contient des disfluences, répétitions, etc.
4.2     Extraction de règles d’annotation

Pour implémenter la fouille de données, nous construisons un arbre des préfixes communs
par niveaux, le processus est optimisé en exploitant la propriété d’anti-monotonie (Agrawal et
Srikant, 1995) et les hiérarchies (Wang et Han, 2004). De plus, nous poussons deux contraintes
supplémentaires pour l’extraction des règles d’annotation :
– Nombre de marqueurs : une règle d’annotation partielle ne contient qu’un marqueur.
– Niveaux : le nombre d’itérations de l’algorithme par niveaux est limité à 7.
L’approche que nous adoptons nous permet d’explorer exhaustivement les motifs fréquents et
confiants. Les seuils minimaux sont fixés à 3 en fréquence et 5% en confiance. Le système extrait
alors 143 205 règles d’annotation partielles 13 . La figure 5 montre que la longueur des règles varie
autour de trois éléments, et leur profondeur d’items 14 se situe autour de quatre. Ces statistiques
confirment que les règles d’annotation sont explorées sur les deux axes que nous avons définis.
Nous voyons aussi que la répartition des règles d’annotation par types d’EN est diversement
corrélée au corpus. Les types time et amount sont moins représentés : il y a moins de descripteurs
pour ces types, il pourrait alors être assez homogène dans les données. Inversement, le type
prod, est sur-représenté et nous faisons l’hypothèse qu’il est assez hétérogène.
13. En 15 minutes, sur un seul cœur, en consommant 1,5Go de RAM
14. Somme sur les items des spécialisations au delà de la racine

·104                                            ·104
4
100 ≤ F                                          100 ≤ F                                     6%
10%
6                          30 ≤ F < 100                                     30 ≤ F < 100
3 ≤ F < 30          3                            3 ≤ F < 30                  12%                     19%
prod
4                                                                                                                                              amount
2                                                                                              fonc
NB
NB
4%      loc
16%                                   org
2                                               1                                                                                              pers
33%                      time
0                                               0
2         4        6                     0           5            10              15

(a) Longueur des règles                        (b) Profondeur des règles                                 (c) Types d’entités des règles
FIGURE 5 – Caractéristiques des règles d’annotation extraites
4.3         Reconnaissance d’entités nommées

Nous utilisons l’outil scikit-learn 15 (Pedregosa et al., 2011) pour réaliser la régression logistique.
La figure 6 présente les résultats obtenus en SER 16 et les taux par types d’erreurs (Galibert et al.,
2011). Ces graphiques confirment que le système réduit graduellement ses erreurs à mesure que
les seuils de fréquence et de confiance sont abaissés.
100                                                                     100
F   ≥ 21                                                             Insertions
80                                       F   ≥ 42                       80                                     Délétion
F   ≥ 63                                                            Substitutions
Taux d’erreurs
60                                       F   ≥ 84                       60
SER
40                                                                      40

20                                                                      20

0                                                                       0
80        60        40          20                                       80     60             40            20
C                                                                           C
(a) Performances                                                       (b) Erreurs, F ≥ 21
FIGURE 6 – Performances (SER) et erreurs selon la Fréquence (F) et la Confiance (C)

Nous menons des expériences supplémentaires , dont les résultats sont reportés dans le tableau 2
pour les configurations suivantes :

–    Logit : système par défaut
–    Logit-Dicos : désactivation des ressources lexicales
–    Logit+Test : apprentissage en fusionnant les corpus Etape-Train et Etape-Dev
–    Logit-D25 : filtrage δ à 25%,
–    Logit-D50 : filtrage δ à 50%,
–    Logit-D75 : filtrage δ à 75%,

15. http://scikit-learn.org
16. Slot Error Rate, taux d’erreur pondéré

Le système donne des résultats satisfaisants, étant donné la difficulté de la tâche. Sans surprise, la
désactivation des dictionnaires dégrade considérablement les performances. Lorsque les données
comportent les données d’évaluation (Logit+Test), le surapprentissage est modéré, ce qui est
lié au fait que les règles d’annotation ne sont pas lexicalisées. Les expériences Logit-DXX nous
montrent clairement que le système obtient encore des performances très acceptables lorsque
l’on réduit significativement le nombre de règles extraites à l’aide du filtrage δ.

Approche     Règles      SER     I          D      S      P       R     Fm
Logit       143 205     35,9   5,6        24,2   10,8   79,8   64,9    71,6
Logit-Dicos    80 231      45,2   5,9        30,2   16,3   70,7   53,5    60,9
Logit+Test     141 550     26,3   3,2        18,6   8,1    86,6   73,3    79,4
Logit-D25     100 027     36,2   5,6        24,6   10,9   79,7   64,6    71,3
Logit-D50     73 332      36,7   5,4        25,2   11,0   79,5   63,8    70,8
Logit-D75     50 408      39,0   5,4        27,0   11,7   78,2   61,3    68,7

TABLE 2 – Performances (SER), erreurs d’Insertion (I), de Délétion (D), de Substitution (S),
Précision (P), Rappel (R), F-mesure (Fm) des approches

Nous menons des évaluations séparées des types primaires (sans sous-types) d’entités nommées
et de composants. La figure 7 en donne les résultats. Les entités nommées sont moins bien
reconnues que les composants et plusieurs types (en particulier les expressions de temps) posent
encore problème. Ceci dit, le système équilibre relativement bien sa précision et son rappel et la
reconnaissance d’entités nommées selon l’approche présentée donne des résultats.

100
80
Types        SER     P      R        Fm             60
Entités       38,9   76,4   62,3      68,6           40
Composants      33,0   86,4   68,5      76,4           20
Tous        35,9   79,8   64,9      71,6            0
pro amo     f        loc    org     per    tim
d    unt unc                        s      e
FIGURE 7 – SER, précision (gauche) et rappel (droite) par types primaires et composants

La phase d’adjudication de la campagne d’évaluation ETAPE n’est pas achevée à l’heure de
la rédaction de cet article. Nous avons cependant été autorisés à reporter en table 3 les per-
formances anonymes des systèmes avant adjudication. Les SER présentés sont donnés sur les
transcriptions manuelles et sur les sorties de différents systèmes de reconnaissance, pour lesquels
sont mentionnés les WER 17 .
Parmi les autres systèmes participants, le système 3 utilise des CRF (binarisés, un par type), le
système 6/7/8 utilise un CRF pour les composants et un PCFG pour reconstituer les entités,
CasEN utilise des transducteurs. De manière générale, mXS affiche de bonnes performances (entre
la 1 ère et la 3 ème position). Les taux d’erreurs élevés sont liés à la difficulté de la tâche (parole
spontanée, imbrications, typologie fine). Sans surprise, les performances sont dégradées sur
les données bruitées par la reconnaissance de parole. Nous voyons que mXS et résiste bien aux
erreurs de reconnaissance de la parole.
17. Word Error Rate

Part.      Type      Man     Rover    WER23     WER24    WER25     WER30     WER35
1         OC       84.8     98,1    100,7      94,2     98,9      98,4     100,9
2         OC       172.0   147,4    178,8     160,4    168,0     163,9     168,2
3        CRF       33.8     57,2     59,3      64,7     62,0      61,7      71,8
4         OC       55.6     88,0     98,8      76,8     92,8      94,9      99,6
5        CRF       43.6     69,7     73,8      72,1     73,7      74,8      86,0
6     CRF+PCFG      na      79,2     79,5      66,8     80,8      80,0      87,0
7     CRF+PCFG      na      67,8     68,4      67,6     70,9      69,9      85,2
8     CRF+PCFG     36.4      na       na        na       na        na        na
9        CRF       62.8     75,8     79,2      76,9     79,8      80,5      90,5
10         OC       42.9     65,0     69,9      66,3     70,5      69,9      87,0
CasEN        OC       49.3      na       na       68,4      na        na        na
mXS       Règles     41.0     63,7     67,5      64,1     69,1      68,6      80,4

TABLE 3 – SER de la campagne Etape par système (OC=Orienté Connaissances) sur les transcrip-
tions avant adjudication (manuel : Man, transcription automatiques : Rover et WERXX, dont
WER24 avec majuscules)
5    Conclusion

La reconnaissance d’entités nommées structurées sur de la parole spontanée nécessite de mettre
au point des systèmes robustes. Dans cet article, nous présentons une approche originale à base
de fouille de données, qui extrait des règles d’annotation partielles et paramètre un modèle
numérique les utilisant.
Les résultats obtenus dans le cadre de la campagne Etape indiquent que notre approche novatrice
fait jeu égal avec les systèmes état de l’art. Pour éviter tout biais méthodologique, nous restons
toutefois en attente d’une référence débarrassée de toute erreur d’annotation : c’est l’objectif de
la phase d’adjudication en cours. Notre objectif à court terme est de mieux caractériser les points
forts et limitations du modèle (détection séparée du début et de la fin des annotations). Nous
comptons également mettre à l’épreuve le système sur d’autres tâches qui pourraient bénéficier
de l’extraction de motifs de segments.

Remerciements

Ces travaux ont été réalisés dans le cadre du projet ANR Etape. Merci en particulier à Olivier
Galibert (LNE), Matthieu Carré (ELDA) et Guillaume Gravier (IRISA).

Références
AGRAWAL, R. et SRIKANT, R. (1995). Mining sequential patterns. In International Conference on Data
Engineering (ICDE’95), pages 3–14.
BIKEL, D., SCHWARTZ, R. et WEISCHEDEL, R. M. (1999). An algorithm that learns what’s in a name. Machine
Learning, 34:211–231.
BRUN, C. et EHRMANN, M. (2010). Un système de détection d’entités nommées adapté pour la campagne
d’évaluation ester 2. In Traitement Automatique du Langage Naturel (TALN’10).
BÉCHET, F., SAGOT, B. et STERN, R. (2011). Coopération de méthodes statistiques et symboliques pour
l’adaptation non-supervisée d’un système d’étiquetage en entités nommées. In Traitement Automatique des
Langues Naturelles (TALN’11).

CELLIER, P. et CHARNOIS, T. (2010). Fouille de données séquentielles d’itemsets pour l’apprentissage de
patrons linguistiques. In Traitement Automatique des Langues Naturelles (TALN’10).
DINARELLI, M. et ROSSET, S. (2011). Models cascade for tree-structured named entity detection. In
International Joint Conference on Natural Language Processing (IJCNLP’11).
EKBALA, A., SOURJIKOVA, E., FRANK, A. et PONZETTO, S. P. (2010). Assessing the challenge of fine-grained
named entity recognition and classification. In Annual Meeting of the Association for Computational
Linguistics (ACL’10) - Named Entities Workshop, pages 93–101, Uppsala, Sweden.
FINKEL, J. R. et MANNING, C. D. (2005). Nested named entity recognition. In Conference on Empirical
Methods in Natural Language Processing (EMNLP’09).
FISCHER, J., HEUN, V. et KRAMER, S. (2005). Fast frequent string mining using suffix arrays. In 5th IEEE
International Conference on Data Mining (ICDM’05), pages 609–612.
FRIBURGER, N. et MAUREL, D. (2004). Finite-state transducer cascades to extract named entities in texts.
Theoretical Computer Sciences (TCS), 313:93–104.
GALIBERT, O., ROSSET, S., GROUIN, C., ZWEIGENBAUM, P. et QUINTARD, L. (2011). Structured and extended
named entity evaluation in automatic speech transcriptions. In International Joint Conference on Natural
Language Processing (IJCNLP’11).
GALLIANO, S., GRAVIER, G. et CHAUBARD, L. (2009). The ester 2 evaluation campaign for the rich transcription
of french radio broadcasts. In International Speech Communication Association (INTERSPEECH’09).
GRISHMAN, R. et SUNDHEIM, B. (1996). Message undersrtanding conference - 6 : A brief history. In
International Conference on Computational Linguistics (COLING’96), pages 466–471, Copenhagen, Denmark.
HOBBS J. R., Appelt D., B. J. I. D. K. M. S. M. et TYSON, M. (1996). FASTUS : A Cascaded Finite-State
Transducer for Extracting Information from Natural-Language Text, pages 383–406.
MCCALLUM, A., FREITAG, D. et PEREIRA, F. (2000). Maximum entropy markov models for information
extraction and segmentation. In International Conference on Machine Learning (ICML’00), pages 591–598.
MCDONALD, D. D. (1996). Internal and External Evidence in the Identification and Semantic Categorisation
of Proper Names, pages 32–43.
MIKHEEV, A., MOENS, M. et GROVER, C. (1999). Named entity recognition without gazetteers. In Proc. of
the Ninth Conference of the European Chapter of the Association for Computational Linguistics, pages 1–8.
PASQUIER, N., BASTIDE, Y., TAOUIL, R. et LAKHAL, L. (1999). Efficient mining of association rules using closed
itemset lattices. INF. SYST., 24(1):25–46.
PEDREGOSA, F., VAROQUAUX, G., GRAMFORT, A., MICHEL, V., THIRION, B., GRISEL, O., BLONDEL, M., PRETTEN-
HOFER , P., W EISS , R., D UBOURG , V., VANDERPLAS , J., PASSOS , A., C OURNAPEAU , D., B RUCHER , M., P ERROT , M.
et Édouard DUCHESNAY (2011). Scikit-learn : Machine learning in python. Journal of Machine Learning
Research, 12:2825–2830.
RAYMOND, C. et FAYOLLE, J. (2010). Reconnaissance robuste d’entités nommées sur de la parole transcrite
automatiquement. In Traitement Automatique des Langues Naturelles (TALN’10).
ROSSET, S., GROUIN, C. et ZWEIGENBAUM, P. (2011). Entité nommées structurées : guide d’annotation quaero.
Rapport technique, LIMSI (2011-04).
SCHMID, H. (1994). Probabilistic pos tagging using decision trees. In New Meth. in Lang. Proc. (NEMLP’94).
SRIKANT, R. et AGRAWAL, R. (1996). Mining sequential patterns : Generalizations and performance
improvements. In International Conference on Extending Database Technology (EDBT’96), pages 3–17.
STEPHENS, C. S. (1993). The analysis and acquisition of proper names for the understanding of free text.
Computers and the Humanities, 26:441–456.
WANG, J. et HAN, J. (2004). Bide : Efficient mining of frequent closed sequences. In International Conference
on Data Engineering (ICDE’04).
ZIDOUNI, A., ROSSET, S. et GLOTIN, H. (2010). Efficient combined approach for named entity recognition in
spoken language. In Conference of the International Speech Communication Association (INTERSPEECH’10).

[P-C.2]
21ème Traitement Automatique des Langues Naturelles, Marseille, 2014
Annotation de la temporalité en corpus : contribution à l'amélioration de la
norme TimeML

Anaïs Lefeuvre1, Jean-Yves Antoine1, Agata Savary1, Emmanuel Schang2, Lotfi Abouda2, Denis Maurel1,
Iris Eshkol2
(1) Université François Rabelais de Tours, laboratoire LI
(2) Université d’Orléans, laboratoire LLL, UMR 7270

Résumé.        Cet article propose une analyse critique de la norme TimeML à la lumière de l’expérience d’annotation
temporelle d’un corpus de français parlé. Il montre que certaines adaptations de la norme seraient conseillées pour
répondre aux besoins du TAL et des sciences du langage. Sont étudiées ici les questions de séparation des niveaux
d’annotation, de délimitation des éventualités dans le texte et de l’ajout d’une relation temporelle de type associative.

Abstract. This paper reports a critical analysis of the TimeML standard, in the light of a temporal annotation that
was conducted on spoken French. It shows that the norm suffers from weaknesses that must be corrected to fit the needs
of NLP and corpus linguistics. These limitations concern mainly 1) the separation of different levels of linguistic
annotation, 2) the delimitation in the text of the events, and 3) the absence of a bridging temporal relation in the norm.
Mots-clés :        annotation temporelle, TimeML, éventualités, relations temporelles, expressions temporelles
Keywords:          temporal annotation, TimeML, eventualities, temporal relations, time expressions
1     Introduction : normalisation des ressources linguistique en TAL
La lecture des actes de la première édition de la conférence TALN, organisée il y a 20 ans à Marseille, nous renseigne
sur l'évolution de la discipline, marquée par le développement des approches par apprentissage : seules 13% des
communications de l’époque évoquaient des approches centrées sur les données. Cette évolution fut celle du passage de
la linguistique computationnelle à l’ingénierie des langues. D’un point de vue méthodologique, elle s’est traduite par le
recours de plus en plus systématique aux corpus. Avec l’émergence d’Internet et du Big Data, une masse sans cesse
accrue d’information textuelle est désormais disponible. La question qui se pose n’est plus l’accès au matériau
linguistique mais à son annotation, description enrichie essentielle à de nombreuses applications. L’annotation manuelle
de corpus étant coûteuse, la création de telles ressources reste limitée. Pour palier cette difficulté, certains s’en remettent
à une annotation automatique dont les erreurs sont compensées par la masse de données enrichies (Clark et Lappin
2010). La diffusion d’annotations manuelles d’envergure et de qualité reste néanmoins une question cruciale. Afin de
favoriser la réutilisabilité des ressources, il est nécessaire de définir des normes d’annotation acceptées de tous. Cet
effort de normalisation reste toutefois insuffisant. La reconnaissance des entités nommées est symptomatique de cette
situation. La notion d’entité nommée n’a cessé d’évoluer au fil des campagnes d’évaluation sans arriver à un réel
standard consensuel. D’où l’existence de corpus reposant sur des typologies peu compatibles.

L’annotation en temporalité semble avoir évité cet écueil. Elle a fait l’objet de la publication d’une norme, TimeML
proposée par le comité TC37de l’ISO (2008). Elle a été adoptée par de multiples corpus, avec parfois des adaptations
comme ce fut le cas pour le French TimeBank (Bittar and al., 2011). Cette norme présente toutefois à nos yeux comme à
d’autres (Battistelli, 2009) des insuffisances qu’il convient de circonscrire. Après un état de l’art sur l’annotation en
temporalité, cet article présente une analyse critique de certaines faiblesses de la norme ISO TimeML et tente d’y
apporter des réponses. Cette réflexion est portée par un consortium dont l’ambition est de réaliser un corpus de grande
envergure annoté en temporalité, comme il l’a déjà fait pour l’annotation en coréférence avec le corpus ANCOR_Centre
(Muzerelle et al. 2013, Muzerelle et al. 2014). Ce texte ne constitue pas la proposition d’une nouvelle norme, mais le
fruit de réflexions issues d’ateliers d’annotation que nous portons auprès de la communauté TALN pour alimenter le
début d’une nécessaire évolution de la norme TimeML.
562

[P-C.2]
A. LEFEUVRE, J-Y. ANTOINE, A. SAVARY, E. SCHANG, L. ABOUDA, D. MAUREL, I. ESHKOL
2      Annotation en temporalité
L’annotation de la temporalité linguistique demande dans un premier lieu de définir ce qu’est un évènement, élément
constitutif de la temporalité du discours. On retiendra deux définitions pertinentes, la première de Mani et al. (2005) :

“We consider “events” a cover term for situations that happen or occur. Events can be punctual or last for a
period of time. We also consider as events those predicates describing states or circumstances in which
something obtains or holds true.” (Mani et al., 2005)

“Nous utiliserons (...) le terme d’éventualité (...) comme terme générique pour désigner quelque chose qui se
produit, qui a lieu ou qui est vrai sur une période donnée de temps.” (De Saussure, 1997)

Ces deux définitions rendent compte de la diversité des évènements évoqués dans le discours. Diversité que nous
chercherons à représenter par le terme générique d'éventualité (Bach, 1981), et non d’évènement. Pour analyser
temporellement un énoncé, il est également nécessaire de détecter les relations entre ces entités du discours. (Allen,
1983) en propose un modèle standard reposant sur 13 relations minimales entre intervalles.
(1) Tu restes la semaine à Paris, j’y serai à partir de mercredi.
(2) Je reste la semaine à Paris d’où je pars pour Londres.
L’exemple (1) illustre la relation d’overlap, qui suppose l’existence d’une intersection entre les intervalles mobilisés par
“tu restes la semaine à Paris” et “j’y serai à partir de mercredi”. L’exemple (2) illustre la relation meet. Celle-ci décrit
le fait qu’une éventualité, (“je pars pour Londres”) succède immédiatement à une autre (“je reste la semaine à Paris“).

L’annotation de la temporalité a été initialement motivée par des visées applicatives, notamment en extraction
d’information. Des campagnes d’évaluation ont défini des tâches bien délimitées, de complexité croissante, chacune
étant associée à un schéma d’annotation spécifique. Ainsi, MUC-7 et CoNLL 2002-2003 ont proposé les tâches de
repérage d’entités nommées incluant des expressions temporelles. ACE 2005-2007 a complété cette tâche par une phase
de normalisation d’expressions temporelles et a introduit la tâche de repérage et de caractérisation d’évènements.
TempEval à son tour a proposé une tâche d’identification automatique des relations temporelles à partir des expressions
temporelles et événements déjà identifiés dans le texte. Un des résultats de ces initiatives est la construction progressive
d’un standard d’annotation qui a donné lieu à la norme TimeML, dont nous donnons un aperçu à la section suivante.

Un des challenges récents dans la reconnaissance d’entités nommées, y compris temporelles, est celui du repérage des
entités imbriquées (Savary et al. 2010, Gravier et al., 2012), comme par exemple dans l’entité nommée [conseil général
[du Morbihan]]. L’imbrication permet de mettre directement en évidence certaines relations inter-entités, mais
complexifie la tâche des systèmes de reconnaissance automatique. Certains travaux tentent de contourner la difficulté du
repérage des frontières de ces entités/mentions en identifiant seulement leurs têtes lexicales. Comme nous l’expliquerons
dans la section 3.2, nous nous plaçons du côté des approches qui délimitent la portée textuelle complète des
entités/mentions (Muzerelle et al. 2013, Ogrodniczuk et al. 2013).

Notons enfin, que l’intérêt de la prise en compte de l’information temporelle est partagée par d’autres domaines. Ainsi,
les spécialistes de représentation de connaissances, et notamment des Linked Open Data, ont identifié le besoin de
raisonnement automatique de nature spatio-temporelle, et ont pour cela proposé des extensions aux ontologies
existantes, telles que YAGO2 (Hoffart et al., 2011), et aux formalismes associés (RDF, SPARQL, etc.), en y incluant
notamment des dimensions temporelles (par addition d’intervalles temporels pour caractériser l’existence des entités).

3      TimeML

Dans le domaine du TALN, la norme ISO TimeML s'est imposée comme un stantard de facto pour l'annotation de la
temporalité. Des corpus annotés suivant le format ISO TimeML existent ainsi pour l'anglais, le français, l'italien, le
portugais, le coréen, le roumain et le chinois. TimeML distingue tout d'abord trois types d'unités:
EVENT, reprenant globalement la notion d’éventualité typée (state, occurrence, etc.) et attachée à la tête du
syntagme (verbal, nominal, adjectival ou prépositionnel) correspondant. Un EVENT est assorti d’un ou plusieurs
MAKEINSTANCE qui permettent d’annoter les différentes instances du même EVENT en donnant plusieurs traits à
ceux-ci (part-of-speech, tense, aspect, etc.). Par exemple, l’énoncé suivant sera annoté avec un EVENT attaché à
“enseigné” et deux objets MAKEINSTANCE, un pour chacune des instances de cette éventualité.

(3) Jean a enseigné deux fois lundi
563

[P-C.2]
ANNOTATION DE LA TEMPORALITE EN CORPUS
TIMEX : les expressions temporelles expriment une date calendaire, un horaire, une durée, un ensemble d’horaires
(date, time, duration, set). Pour l’exemple (3), “lundi” sera annoté TIMEX.
SIGNAL : termes linguistiques qui marquent l'existence d'une relation temporelle entre deux éventualités.

Enfin, 3 relations sont proposées :
TLINK : relation entre deux EVENTs, un EVENT et un TIMEX, ou encore entre deux TIMEXs, suivant la
typologie de Allen. TimeML ajoute une relation identity pour la coréférence temporelle.
ALINK : relation aspectuelle entre EVENTs, permet d’annoter l’opération d’un EVENT sur un second telle que la
sélection de sa phase initiale, finale, son apogée etc. Dans l’exemple suivant, “démarré” et “cours” sont deux
EVENTs reliés par une relation ALINK dont le type est initiates :

(4) Jean a démarré son cours à 14h.

SLINK : relation de subordination entre deux EVENTs, marquant l’influence du premier sur la factivité du second.
Dans l’exemple suivant “oublié” et “était à Paris” sont liés par un SLINK de type factive.

(5) Jean a oublié qu’il était à Paris ce jour là.

4      Limites de TimeML

Dans le cadre du projet TEMPORAL, financé par la MSH Val de Loire, nous comptons ajouter une couche d’annotation
temporelle au corpus ANCOR_Centre (Muzerelle et al. 2013, 2014). Couvrant un large éventail de relations
anaphoriques ou de coréférence, ce grand corpus oral ne recense toutefois pas les anaphores abstraites (Dipper &
Zinsmeister, 2010) telles que :
(6) L1 : Pierre a encore cassé sa voiture.
L2 : Venant de lui, ça ne m’étonne pas.

Ici, le pronom anaphorique “ça” a pour antécédent l’éventualité décrite par l’ensemble de l’énoncé de L1. TEMPORAL
avait pour objectif initial de rendre compte de ce genre de situations avec la norme TimeML. Une démarche
méthodologique consistant à réaliser à intervalles réguliers des ateliers d’annotation entre experts nous a toutefois révélé
certaines limitations pénalisantes de la norme, qui sont l’objet du présent article.

4.1      Confusion entre les niveaux d’annotation
ISO TimeML cherche visiblement à intégrer dans une seule couche d’annotation tous les éléments nécessaires à la
résolution des références temporelles. Ce choix conduit à des corpora auto-suffisants mais induit des incohérences du
fait de la juxtaposition d’annotations de niveaux linguistiques différents. C’est ainsi le cas de la distinction entre
évènement et instance (EVENT/MAKEINSTANCE). EVENT est en effet directement lié à un observable linguistique,
marqueur de l’éventualité dans l’énoncé, tandis que l’instance réfère à la réalisation effective de cette éventualité.
Paradoxalement, TimeML confère à l’instance l’ensemble des traits qui décrivent l’éventualité, alors que ceux-ci sont
clairement liés à sa réalisation linguistique en contexte (tense, aspect, etc.).

(7) Jean a enseigné deux fois lundi.

Ainsi, l’exemple (7), transposé du guide d’annotation TimeML, montre une répétition d’éventualités. La norme propose
de définir un EVENT pour décrire une éventualité traduite par le verbe “a enseigné”, puis deux instances correspondant
aux deux réalisations successives de cette éventualité. Il serait préférable de définir des niveaux d’annotation séparés, ce
que permet le format déporté d’ISO TimeML. Nous conseillons même de ne conserver que les objets EVENT en leur
intégrant les attributs de MAKEINSTANCE comme dans le French TimeBank (Bittar et al. 2011), nous limitant à une
annotation objective purement linguistique. L’annotation combinée des deux niveaux pose en effet des questions en
termes de fiabilité des données. Considérons maintenant l’énoncé (8) :
(8) Jean a enseigné aux L1 le lundi et Marc aux L2 le mardi.

Doit-on encore considérer comme TimeML qu’il n’y a qu’une seule éventualité et plusieurs instances, alors que les
actions diffèrent, puisque les arguments du verbe varient ? Dès lors, où s’arrête la distinction entre éventualités et
instances ? Dans le cadre du projet TEMPORAL qui réunit nos deux laboratoires, nous proposons de nous limiter à une
annotation purement linguistique qui limite ce genre de questionnement. L’annotation pourrait ensuite être complétée
par d’autres couches pour les communautés travaillant sur la recherche d’information ou les systèmes question/réponse.
564

[P-C.2]
A. LEFEUVRE, J-Y. ANTOINE, A. SAVARY, E. SCHANG, L. ABOUDA, D. MAUREL, I. ESHKOL
4.2      Délimitation des éventualités
La remarque précédente pose la question de la délimitation des éventualités. ISO TimeML a choisi de caractériser les
éventualités uniquement par la tête lexicale de leur observable linguistique, en ignorant leurs arguments. Ce choix est
motivé par le souci de simplifier la tâche des annotateurs (Bittar, 2008). Cette question a préocuppé Pustejovsky et ses
collègues, qui ont proposé d’annoter à part les arguments des têtes lexicales portant une éventualité (Pustejovsky et al.
2006). Cette proposition n’a toutefois pas été retenue dans la norme TimeML.

Nos expériences d’annotation n’ont pas démontré de difficulté à délimiter des éventualités à large empan. Il nous semble
donc préférable d’annoter l’ensemble de l’unité linguistique décrivant l’éventualité. En effet, le calcul de la temporalité
d’un énoncé se fait à partir de tous ses éléments pertinents, ce dont doit rendre compte le balisage dû à l’annotation.
Cette approche permet de se dispenser des relations artefactuelles auxquelles TimeML est condamné dans le cas
d'encapsulation d'éventualités comme dans l'exemple (9) :

(9)        [vous voulez [venir avec votre chien] ]

Ce choix est cohérent avec les pratiques observées pour l’annotation des entités nommées. Il permet d’établir une
procédure homogène et syntaxiquement cohérente avec le traitement des éventualités nominales. L’exemple ci-dessous
fait ainsi un parallèle entre les délimitations d’entités imbriquées proposées pour notre annotation temporelle (10a) et
pour une annotation en entités nommées (10b) telle qu’envisagée par exemple dans la campagne ETAPE.

(10a)      [le début de [la seconde phase de [colonisation de l’archipel] ] ]
(10b)      [le président de [la première réunion de [la nouvelle assemblée constituante]]]

L’intérêt de ce choix est manifeste dans le cas des anaphores abstraites ou de l’encapsulation d’éventualités. Dans
l’exemple (11), l’éventualité ancrée par vouloir a une portée sur les deux autres éventualités, ce qui est aisément capturé
par l’encapsulation. TimeML peut représenter ce type d’énoncé, mais d’une manière moins élégante avec des relations.

(11)       [vous voulez [venir avec votre chien] et [faire l’intégralité de la promenade] ]

Il n’en reste pas moins qu’une annotation à large empan peut conduire à de nouvelles difficultés que nous avons-nous-
même expérimentées. Tout d’abord, les relations entre les têtes lexicales et leurs arguments peuvent s’étendre sur de
grandes longueurs. Nous proposons de nous limiter à l’inclusion des éléments sous-catégorisés (arguments de valence
pour un verbe, par exemple). D’autre part, les éventualités ainsi délimitées peuvent être discontinues. C’est le cas à
l’oral où des disfluences telles que les incises peuvent découper l’éventualité. On peut formellement décrire ces
discontinuités (notion de schéma sous la plateforme Glozz, par exemple (Mathet & Widlöcher, 2009)) mais cela
complique la tâche d’annotation. C’est pourquoi notre réflexion s’oriente à l’heure actuelle vers une annotation sur
treebank, inspirée de l'annotation des expressions polylexicales dans le Prague Dependency Treebank (Bejček &
Straňák, 2010): puisqu'une éventualité est représentée dans un treebank par le nœud de l’arbre syntaxique qui
correspond à la tête lexicale tout en couvrant la partie de l’énoncé concerné, identifier cette éventualité se réduirait à
pointer ce noeud dans l'arbre tout en lui assignant des attributs spécifiques à la temporalité.

Des incohérences plus anecdotiques de délimitation peuvent également se rencontrer. TimeML distingue ainsi deux
types d’entités qui modifient ou précisent la portée temporelle des éventualités ou des expressions temporelles :
▪     les signaux, qui indiquent la présence d’une relation temporelle (TLINK) entre deux éventualités ou expressions
temporelles. Ainsi, la préposition avant sera annotée comme un signal dans l’énoncé je suis parti avant midi.
▪     les modifieurs (MOD), qui altèrent ou précisent une portée temporelle. Par exemple, la locution en fin de sera annoté
comme un modifieur dans l’énoncé je suis parti en fin de matinée.

Ces entités jouent des rôles relativement proches mais donnent lieu à des conventions d’annotation radicalement
différentes. Les signaux sont des objets d’annotation propres, délimités par un balisage spécifique (<SIGNAL> avant
</SIGNAL>) alors que les modifieurs conduisent simplement à l’ajout d’un attribut MOD dans l’élément qu’ils
caractérisent. ISO TimeML donnera ainsi sur notre exemple l’annotation (simplifiée) suivante : en fin de <EVENT …
MOD=’END’> matinée </EVENT>. Une description homogène rendrait plus fiable le processus d’annotation.
4.3      Expressions temporelles
L’annotation des expressions temporelles dans TimeML dérive du tag TIMEX2 utilisé dans TIDES (Ferro 2005). Cette
norme répond plus aux besoins de la RI que du TAL. Les expressions temporelles y sont en effet intégralement évaluées
temporellement en contexte. Ainsi, “le mois dernier” sera par exemple annotée 2014-05. Cette convention pose des
565

[P-C.2]
ANNOTATION DE LA TEMPORALITE EN CORPUS
problèmes de cohérence : le contexte d’évaluation ne sera en effet pas toujours connu pour permettre cette résolution
temporelle. Cela renforce par ailleurs la part de subjectivité laissée à l’annotateur, qui ne cherchera pas toujours
l’information nécessaire à cette évaluation. Ici encore, TimeML mélange deux niveaux d’annotation : celui du signifiant
linguistique et celui de son interprétation qui relève d’un raisonnement purement temporel. Nous proposons donc
d’annoter “le mois dernier” sous la forme -1M (mois antérieur) et de laisser à une autre couche d’annotation la question
de la résolution complète de l’expression temporelle.

4.4        Attributs liés aux éventualités
Enfin, certaines limitations concernent les attributs liés aux éventualités. L’une d’entre elles est spécifique l’adaptation
au français de TimeML (Bittar et al. 2011). Le French TimeBank ajoute, pour le français, des temps dans le système de
valeurs de l’attribut TimeML tense. On retrouve un temps past mais également un temps imperfect. Ce choix
manque de pertinence linguistique : le terme imperfect étant traditionnellement attaché à une notion d'aspect, les valeurs
du trait ici croisent d'un côté une notion de temps linguistique (tense en anglais et qui pourrait justifier imperfect) et
de l'autre une représentation du temps extralinguistique en 3 parties past, present, future.

D’autres limitations correspondent à des manques de la norme. Par exemple ISO TimeML prévoit un attribut neg qui
permet de représenter la négation d’une éventualité (exemple : il ne pleut pas). Dans cet esprit, il nous semble manquer
un attribut pour marquer l'occurrence de l’éventualité dans une question. Cette information peut par exemple avoir une
utilité pour l’identification des actes de langage liés à l’éventualité. Trois valeurs peuvent être portées par cet attribut :
-     YEST    si la question porte sur la temporalité de l’évènement. Par exemple : quand viendras-tu ?
-     YES     si la question porte sur l'occurrence ou non de l’éventualité. Par exemple : viendras-tu ?
-     NO      lorsqu’il n’y a pas d’une question ou qu’elle porte sur une autre dimension. Par exemple : Où vas-tu ?

4.5        Relations temporelles
Le dernier élément important de la norme concerne les relations temporelles. ISO TimeML intègre un type de relation
(TLINK) qui peut concerner de manière indifférente les éventualités ou les expressions temporelles, alors que les types
ALINK et SLINK ne concernent que les éventualités. Nous regrettons là encore le groupement sous la même annotation
d’objets décrivent des réalités différentes (cf § 3.1.). Il serait plus cohérent de réserver le type TLINK aux seules
relations entre éventualités, et d’ajouter un type spécifique pour les relations d’ancrage liant une expression temporelle à
une éventualité. Pustejovsky suit d'ailleurs cette logique lorsqu'il propose (séminaire invité, Paris 2014) de créer une
relation MLINK pour représenter une relation entre un TIMEX et un EVENT dont le premier donne la durée du second.

On relève en outre que le type TLINK se limite aux relations d’ordonnancement (égalité, précédence, post-occurence...)
de Allen (1983). Cette convention ignore l’existence de correspondances plus complexes, comme par exemple des
relations d’inclusion temporelle ontologiquement décrites. Considérons l’exemple suivant :
(12) Je suis né à Noël, précisément, le 25 décembre 1966. Cette année-là il a fait très froid durant tout l’hiver

TimeML représente 25 décembre 1966 comme une date mais ne peut exprimer la relation d’inclusion qu’elle partage
ontologiquement avec l’expression englobante cette année-là ? On se retrouve ici avec une relation de type
meronymique (partie_de) au niveau temporel. Celle-ci s’apparente par analogie avec les anaphores associatives que
l’on retrouve lorsque l’on s’interesse à la référence (Webber 1988). L’attribut reltype de TLINK doit décrire ce type
de relation. Par analogie, on propose de rajouter la valeur bridging au système de valeur associé à cet attribut.

5         Conclusion

Cet article a cherché à caractériser certaines limitations de la norme ISO TimeML qui demandent à être corrigées pour
atteindre une annotation qui réponde réellement aux besoins du TALN, des sciences du langage mais également de
l’ingénierie des connaissances. Certaines réponses à ces observations ne requièrent qu’une modification à la marge de la
norme. La question de la délimitation des éventualités étant plus problématique de notre point de vue. Nous proposons
de ne pas se limiter à la tête lexicale mais bien de l’étendre à tout le signifiant linguistique correspondant. La taille des
ressources annotées avec la norme TimeML reste encore limitée. Il nous semble donc plus qu’urgent de la remettre
partiellement en question, sur des points qui, comme nous l'avons rappelé, ont souvent été déjà questionnés.
566

[P-C.2]
A. LEFEUVRE, J-Y. ANTOINE, A. SAVARY, E. SCHANG, L. ABOUDA, D. MAUREL, I. ESHKOL
Références
ALLEN J. F. (1983). Maintaining knowledge about temporal intervals. Communications of the ACM, 26(11), 832–843.
BACH E. W. (1981). Time, Tense, and Aspect : An Essay in English Metaphysics . In Radical Pragmatics, 63–81.
BATTISTELLI D. (2009). La temporalité linguistique: circonscrire un objet d'analyse ainsi que des finalités à cette
analyse Habilitation à diriger des Recherches, Université de Nanterre-Paris X.
BEJČEK E. & STRAŇÁK P. (2010). Annotation of multiword expressions in the Prague Dependency Treebank.
Language Resources and Evaluation, 44(1–2). 7–21.
BITTAR A. (2008) Annotation des informations temporelles dans des textes en français. Actes RECITAL’2008.
BITTAR A., AMSILI P., DENIS P., DANLOS L. (2011). French TimeBank: An ISO-TimeML Annotated Reference
Corpus. Proc. ACL’2011, Portland, Oregon : États-Unis.
CLARK A. & LAPPIN S. (2010). Unsupervised learning and grammar induction. The Handbook of Computational
Linguistics and Natural Language Processing, 57.
DE SAUSSURE L. (1997). Le temps chez Beauzée : algorithmes de repérage et comparaison avec Reichenbach.
Cahiers Ferdinand de Saussure, (49):171–195.
DIPPER S., ZINSMEISTER H. (2010). Towards a standard for annotating abstract anaphora. In LREC 2010 Workshop
on Language Resources and Language Technology Standards, 54-59.
FERRO L. (2005). TIDES 2005 standard for the annotation of temporal expressions. MITRE Corp. Tech. Report.
GRAVIER G., ADDA G., PAULSON N., CARRÉ M., GIRAUDEL A. & GALIBERT O.(2012). The ETAPE corpus
for the evaluation of speech-based TV content processing in the French language. In International Conference on
Language Resources, Evaluation and Corpora, na. Turquie.
ISO (2008). ISO DIS 24617-1: 2008 Language resource management - Semantic annotation framework - Part 1: Time
and Events. International Organization for Standardization, ISO Central Secretariat, Genève, Suisse.
HOFFART J., SUCHANEK F. M., BERBERICH K., LEWIS-KELHAM E., DE MELO G. & WEIKUM G. (2011).
YAGO2: exploring and querying world knowledge in time, space, context, and many languages, in Proc. 20th
International Conference on World Wide Web, WWW 2011, Hyderabad, India (Companion Volume), 229–232.
MANI I., PUSTEJOVSKY J., GAIZAUSKAS R., SAURI R., CASTANO J., LITTMAN J., SETZER A., & KATZ G.
(2005). The specification language TimeML. In Mani I., Pustejovsky J., Gaizauskas R. (Eds.), The language of time : a
reader, chapter 27. Oxford.
MATHET, Y., WIDLÖCHER, A. (2009). La plate-forme GLOZZ : environnement d’annotation et d’exploration de
corpus. Actes de TALN-2009, pages 1–10
MUZERELLE J., LEFEUVRE A., ANTOINE J.-Y, SCHANG E, MAUREL D., VILLANEAU J., ESHKOL I. (2013).
ANCOR, premier corpus de français parlé d’envergure annoté en coréférence et distribué librement. Actes TALN’2013,
Les Sables d’Olonnes.
MUZERELLE J., LEFEUVRE A., SCHANG E., ANTOINE J.-Y., PELLETIER A., MAUREL D., ESHKOL I.,
VILLANEAU J. (2014). ANCOR_Centre, a large free spoken french coreference corpus. Proc. LREC’2014, Reykjavik.
OGRODNICZUK M., ZAWISŁAWSKA M., SAVARY A., GŁOWŃSKA K. (2013). Coreference Annotation Schema
for an Inflectional Language, Proc. CICLING’2013, Samos, Greece, In. LNCS 7816, Springer Verlag, 394-407.
PUSTEJOVSKY J., LITTMAN J., SAURI R. (2006). Argument Structure in TimeML. In Katz G., Pustejovsky J.,
Schilder F. (Eds.) Annotating, Extracting and Reasoning about Time and Events, Dagstuhl Seminar Proc., Schloss
Dagstuhl, Allemagne.
SAVARY A., WASZCZUK J., PRZEPIÓRKOWSKI A. (2010). Towards the Annotation of Named Entities in the
National Corpus of Polish. in Proc. LREC'10, Valetta, Malta.
WEBBER B. L. (1988). Tense as discourse anaphor. Computational Linguistics, 14(2), 61-73.
567
[P-C.2]
21ème Traitement Automatique des Langues Naturelles, Marseille, 2014
Annotation de la temporalité en corpus : contribution à l'amélioration de la
norme TimeML

Anaïs Lefeuvre1, Jean-Yves Antoine1, Agata Savary1, Emmanuel Schang2, Lotfi Abouda2, Denis Maurel1,
Iris Eshkol2
(1) Université François Rabelais de Tours, laboratoire LI
(2) Université d’Orléans, laboratoire LLL, UMR 7270

Résumé.        Cet article propose une analyse critique de la norme TimeML à la lumière de l’expérience d’annotation
temporelle d’un corpus de français parlé. Il montre que certaines adaptations de la norme seraient conseillées pour
répondre aux besoins du TAL et des sciences du langage. Sont étudiées ici les questions de séparation des niveaux
d’annotation, de délimitation des éventualités dans le texte et de l’ajout d’une relation temporelle de type associative.

Abstract. This paper reports a critical analysis of the TimeML standard, in the light of a temporal annotation that
was conducted on spoken French. It shows that the norm suffers from weaknesses that must be corrected to fit the needs
of NLP and corpus linguistics. These limitations concern mainly 1) the separation of different levels of linguistic
annotation, 2) the delimitation in the text of the events, and 3) the absence of a bridging temporal relation in the norm.
Mots-clés :        annotation temporelle, TimeML, éventualités, relations temporelles, expressions temporelles
Keywords:          temporal annotation, TimeML, eventualities, temporal relations, time expressions
1     Introduction : normalisation des ressources linguistique en TAL
La lecture des actes de la première édition de la conférence TALN, organisée il y a 20 ans à Marseille, nous renseigne
sur l'évolution de la discipline, marquée par le développement des approches par apprentissage : seules 13% des
communications de l’époque évoquaient des approches centrées sur les données. Cette évolution fut celle du passage de
la linguistique computationnelle à l’ingénierie des langues. D’un point de vue méthodologique, elle s’est traduite par le
recours de plus en plus systématique aux corpus. Avec l’émergence d’Internet et du Big Data, une masse sans cesse
accrue d’information textuelle est désormais disponible. La question qui se pose n’est plus l’accès au matériau
linguistique mais à son annotation, description enrichie essentielle à de nombreuses applications. L’annotation manuelle
de corpus étant coûteuse, la création de telles ressources reste limitée. Pour palier cette difficulté, certains s’en remettent
à une annotation automatique dont les erreurs sont compensées par la masse de données enrichies (Clark et Lappin
2010). La diffusion d’annotations manuelles d’envergure et de qualité reste néanmoins une question cruciale. Afin de
favoriser la réutilisabilité des ressources, il est nécessaire de définir des normes d’annotation acceptées de tous. Cet
effort de normalisation reste toutefois insuffisant. La reconnaissance des entités nommées est symptomatique de cette
situation. La notion d’entité nommée n’a cessé d’évoluer au fil des campagnes d’évaluation sans arriver à un réel
standard consensuel. D’où l’existence de corpus reposant sur des typologies peu compatibles.

L’annotation en temporalité semble avoir évité cet écueil. Elle a fait l’objet de la publication d’une norme, TimeML
proposée par le comité TC37de l’ISO (2008). Elle a été adoptée par de multiples corpus, avec parfois des adaptations
comme ce fut le cas pour le French TimeBank (Bittar and al., 2011). Cette norme présente toutefois à nos yeux comme à
d’autres (Battistelli, 2009) des insuffisances qu’il convient de circonscrire. Après un état de l’art sur l’annotation en
temporalité, cet article présente une analyse critique de certaines faiblesses de la norme ISO TimeML et tente d’y
apporter des réponses. Cette réflexion est portée par un consortium dont l’ambition est de réaliser un corpus de grande
envergure annoté en temporalité, comme il l’a déjà fait pour l’annotation en coréférence avec le corpus ANCOR_Centre
(Muzerelle et al. 2013, Muzerelle et al. 2014). Ce texte ne constitue pas la proposition d’une nouvelle norme, mais le
fruit de réflexions issues d’ateliers d’annotation que nous portons auprès de la communauté TALN pour alimenter le
début d’une nécessaire évolution de la norme TimeML.
562

[P-C.2]
A. LEFEUVRE, J-Y. ANTOINE, A. SAVARY, E. SCHANG, L. ABOUDA, D. MAUREL, I. ESHKOL
2      Annotation en temporalité
L’annotation de la temporalité linguistique demande dans un premier lieu de définir ce qu’est un évènement, élément
constitutif de la temporalité du discours. On retiendra deux définitions pertinentes, la première de Mani et al. (2005) :

“We consider “events” a cover term for situations that happen or occur. Events can be punctual or last for a
period of time. We also consider as events those predicates describing states or circumstances in which
something obtains or holds true.” (Mani et al., 2005)

“Nous utiliserons (...) le terme d’éventualité (...) comme terme générique pour désigner quelque chose qui se
produit, qui a lieu ou qui est vrai sur une période donnée de temps.” (De Saussure, 1997)

Ces deux définitions rendent compte de la diversité des évènements évoqués dans le discours. Diversité que nous
chercherons à représenter par le terme générique d'éventualité (Bach, 1981), et non d’évènement. Pour analyser
temporellement un énoncé, il est également nécessaire de détecter les relations entre ces entités du discours. (Allen,
1983) en propose un modèle standard reposant sur 13 relations minimales entre intervalles.
(1) Tu restes la semaine à Paris, j’y serai à partir de mercredi.
(2) Je reste la semaine à Paris d’où je pars pour Londres.
L’exemple (1) illustre la relation d’overlap, qui suppose l’existence d’une intersection entre les intervalles mobilisés par
“tu restes la semaine à Paris” et “j’y serai à partir de mercredi”. L’exemple (2) illustre la relation meet. Celle-ci décrit
le fait qu’une éventualité, (“je pars pour Londres”) succède immédiatement à une autre (“je reste la semaine à Paris“).

L’annotation de la temporalité a été initialement motivée par des visées applicatives, notamment en extraction
d’information. Des campagnes d’évaluation ont défini des tâches bien délimitées, de complexité croissante, chacune
étant associée à un schéma d’annotation spécifique. Ainsi, MUC-7 et CoNLL 2002-2003 ont proposé les tâches de
repérage d’entités nommées incluant des expressions temporelles. ACE 2005-2007 a complété cette tâche par une phase
de normalisation d’expressions temporelles et a introduit la tâche de repérage et de caractérisation d’évènements.
TempEval à son tour a proposé une tâche d’identification automatique des relations temporelles à partir des expressions
temporelles et événements déjà identifiés dans le texte. Un des résultats de ces initiatives est la construction progressive
d’un standard d’annotation qui a donné lieu à la norme TimeML, dont nous donnons un aperçu à la section suivante.

Un des challenges récents dans la reconnaissance d’entités nommées, y compris temporelles, est celui du repérage des
entités imbriquées (Savary et al. 2010, Gravier et al., 2012), comme par exemple dans l’entité nommée [conseil général
[du Morbihan]]. L’imbrication permet de mettre directement en évidence certaines relations inter-entités, mais
complexifie la tâche des systèmes de reconnaissance automatique. Certains travaux tentent de contourner la difficulté du
repérage des frontières de ces entités/mentions en identifiant seulement leurs têtes lexicales. Comme nous l’expliquerons
dans la section 3.2, nous nous plaçons du côté des approches qui délimitent la portée textuelle complète des
entités/mentions (Muzerelle et al. 2013, Ogrodniczuk et al. 2013).

Notons enfin, que l’intérêt de la prise en compte de l’information temporelle est partagée par d’autres domaines. Ainsi,
les spécialistes de représentation de connaissances, et notamment des Linked Open Data, ont identifié le besoin de
raisonnement automatique de nature spatio-temporelle, et ont pour cela proposé des extensions aux ontologies
existantes, telles que YAGO2 (Hoffart et al., 2011), et aux formalismes associés (RDF, SPARQL, etc.), en y incluant
notamment des dimensions temporelles (par addition d’intervalles temporels pour caractériser l’existence des entités).

3      TimeML

Dans le domaine du TALN, la norme ISO TimeML s'est imposée comme un stantard de facto pour l'annotation de la
temporalité. Des corpus annotés suivant le format ISO TimeML existent ainsi pour l'anglais, le français, l'italien, le
portugais, le coréen, le roumain et le chinois. TimeML distingue tout d'abord trois types d'unités:
EVENT, reprenant globalement la notion d’éventualité typée (state, occurrence, etc.) et attachée à la tête du
syntagme (verbal, nominal, adjectival ou prépositionnel) correspondant. Un EVENT est assorti d’un ou plusieurs
MAKEINSTANCE qui permettent d’annoter les différentes instances du même EVENT en donnant plusieurs traits à
ceux-ci (part-of-speech, tense, aspect, etc.). Par exemple, l’énoncé suivant sera annoté avec un EVENT attaché à
“enseigné” et deux objets MAKEINSTANCE, un pour chacune des instances de cette éventualité.

(3) Jean a enseigné deux fois lundi
563

[P-C.2]
ANNOTATION DE LA TEMPORALITE EN CORPUS
TIMEX : les expressions temporelles expriment une date calendaire, un horaire, une durée, un ensemble d’horaires
(date, time, duration, set). Pour l’exemple (3), “lundi” sera annoté TIMEX.
SIGNAL : termes linguistiques qui marquent l'existence d'une relation temporelle entre deux éventualités.

Enfin, 3 relations sont proposées :
TLINK : relation entre deux EVENTs, un EVENT et un TIMEX, ou encore entre deux TIMEXs, suivant la
typologie de Allen. TimeML ajoute une relation identity pour la coréférence temporelle.
ALINK : relation aspectuelle entre EVENTs, permet d’annoter l’opération d’un EVENT sur un second telle que la
sélection de sa phase initiale, finale, son apogée etc. Dans l’exemple suivant, “démarré” et “cours” sont deux
EVENTs reliés par une relation ALINK dont le type est initiates :

(4) Jean a démarré son cours à 14h.

SLINK : relation de subordination entre deux EVENTs, marquant l’influence du premier sur la factivité du second.
Dans l’exemple suivant “oublié” et “était à Paris” sont liés par un SLINK de type factive.

(5) Jean a oublié qu’il était à Paris ce jour là.

4      Limites de TimeML

Dans le cadre du projet TEMPORAL, financé par la MSH Val de Loire, nous comptons ajouter une couche d’annotation
temporelle au corpus ANCOR_Centre (Muzerelle et al. 2013, 2014). Couvrant un large éventail de relations
anaphoriques ou de coréférence, ce grand corpus oral ne recense toutefois pas les anaphores abstraites (Dipper &
Zinsmeister, 2010) telles que :
(6) L1 : Pierre a encore cassé sa voiture.
L2 : Venant de lui, ça ne m’étonne pas.

Ici, le pronom anaphorique “ça” a pour antécédent l’éventualité décrite par l’ensemble de l’énoncé de L1. TEMPORAL
avait pour objectif initial de rendre compte de ce genre de situations avec la norme TimeML. Une démarche
méthodologique consistant à réaliser à intervalles réguliers des ateliers d’annotation entre experts nous a toutefois révélé
certaines limitations pénalisantes de la norme, qui sont l’objet du présent article.

4.1      Confusion entre les niveaux d’annotation
ISO TimeML cherche visiblement à intégrer dans une seule couche d’annotation tous les éléments nécessaires à la
résolution des références temporelles. Ce choix conduit à des corpora auto-suffisants mais induit des incohérences du
fait de la juxtaposition d’annotations de niveaux linguistiques différents. C’est ainsi le cas de la distinction entre
évènement et instance (EVENT/MAKEINSTANCE). EVENT est en effet directement lié à un observable linguistique,
marqueur de l’éventualité dans l’énoncé, tandis que l’instance réfère à la réalisation effective de cette éventualité.
Paradoxalement, TimeML confère à l’instance l’ensemble des traits qui décrivent l’éventualité, alors que ceux-ci sont
clairement liés à sa réalisation linguistique en contexte (tense, aspect, etc.).

(7) Jean a enseigné deux fois lundi.

Ainsi, l’exemple (7), transposé du guide d’annotation TimeML, montre une répétition d’éventualités. La norme propose
de définir un EVENT pour décrire une éventualité traduite par le verbe “a enseigné”, puis deux instances correspondant
aux deux réalisations successives de cette éventualité. Il serait préférable de définir des niveaux d’annotation séparés, ce
que permet le format déporté d’ISO TimeML. Nous conseillons même de ne conserver que les objets EVENT en leur
intégrant les attributs de MAKEINSTANCE comme dans le French TimeBank (Bittar et al. 2011), nous limitant à une
annotation objective purement linguistique. L’annotation combinée des deux niveaux pose en effet des questions en
termes de fiabilité des données. Considérons maintenant l’énoncé (8) :
(8) Jean a enseigné aux L1 le lundi et Marc aux L2 le mardi.

Doit-on encore considérer comme TimeML qu’il n’y a qu’une seule éventualité et plusieurs instances, alors que les
actions diffèrent, puisque les arguments du verbe varient ? Dès lors, où s’arrête la distinction entre éventualités et
instances ? Dans le cadre du projet TEMPORAL qui réunit nos deux laboratoires, nous proposons de nous limiter à une
annotation purement linguistique qui limite ce genre de questionnement. L’annotation pourrait ensuite être complétée
par d’autres couches pour les communautés travaillant sur la recherche d’information ou les systèmes question/réponse.
564

[P-C.2]
A. LEFEUVRE, J-Y. ANTOINE, A. SAVARY, E. SCHANG, L. ABOUDA, D. MAUREL, I. ESHKOL
4.2      Délimitation des éventualités
La remarque précédente pose la question de la délimitation des éventualités. ISO TimeML a choisi de caractériser les
éventualités uniquement par la tête lexicale de leur observable linguistique, en ignorant leurs arguments. Ce choix est
motivé par le souci de simplifier la tâche des annotateurs (Bittar, 2008). Cette question a préocuppé Pustejovsky et ses
collègues, qui ont proposé d’annoter à part les arguments des têtes lexicales portant une éventualité (Pustejovsky et al.
2006). Cette proposition n’a toutefois pas été retenue dans la norme TimeML.

Nos expériences d’annotation n’ont pas démontré de difficulté à délimiter des éventualités à large empan. Il nous semble
donc préférable d’annoter l’ensemble de l’unité linguistique décrivant l’éventualité. En effet, le calcul de la temporalité
d’un énoncé se fait à partir de tous ses éléments pertinents, ce dont doit rendre compte le balisage dû à l’annotation.
Cette approche permet de se dispenser des relations artefactuelles auxquelles TimeML est condamné dans le cas
d'encapsulation d'éventualités comme dans l'exemple (9) :

(9)        [vous voulez [venir avec votre chien] ]

Ce choix est cohérent avec les pratiques observées pour l’annotation des entités nommées. Il permet d’établir une
procédure homogène et syntaxiquement cohérente avec le traitement des éventualités nominales. L’exemple ci-dessous
fait ainsi un parallèle entre les délimitations d’entités imbriquées proposées pour notre annotation temporelle (10a) et
pour une annotation en entités nommées (10b) telle qu’envisagée par exemple dans la campagne ETAPE.

(10a)      [le début de [la seconde phase de [colonisation de l’archipel] ] ]
(10b)      [le président de [la première réunion de [la nouvelle assemblée constituante]]]

L’intérêt de ce choix est manifeste dans le cas des anaphores abstraites ou de l’encapsulation d’éventualités. Dans
l’exemple (11), l’éventualité ancrée par vouloir a une portée sur les deux autres éventualités, ce qui est aisément capturé
par l’encapsulation. TimeML peut représenter ce type d’énoncé, mais d’une manière moins élégante avec des relations.

(11)       [vous voulez [venir avec votre chien] et [faire l’intégralité de la promenade] ]

Il n’en reste pas moins qu’une annotation à large empan peut conduire à de nouvelles difficultés que nous avons-nous-
même expérimentées. Tout d’abord, les relations entre les têtes lexicales et leurs arguments peuvent s’étendre sur de
grandes longueurs. Nous proposons de nous limiter à l’inclusion des éléments sous-catégorisés (arguments de valence
pour un verbe, par exemple). D’autre part, les éventualités ainsi délimitées peuvent être discontinues. C’est le cas à
l’oral où des disfluences telles que les incises peuvent découper l’éventualité. On peut formellement décrire ces
discontinuités (notion de schéma sous la plateforme Glozz, par exemple (Mathet & Widlöcher, 2009)) mais cela
complique la tâche d’annotation. C’est pourquoi notre réflexion s’oriente à l’heure actuelle vers une annotation sur
treebank, inspirée de l'annotation des expressions polylexicales dans le Prague Dependency Treebank (Bejček &
Straňák, 2010): puisqu'une éventualité est représentée dans un treebank par le nœud de l’arbre syntaxique qui
correspond à la tête lexicale tout en couvrant la partie de l’énoncé concerné, identifier cette éventualité se réduirait à
pointer ce noeud dans l'arbre tout en lui assignant des attributs spécifiques à la temporalité.

Des incohérences plus anecdotiques de délimitation peuvent également se rencontrer. TimeML distingue ainsi deux
types d’entités qui modifient ou précisent la portée temporelle des éventualités ou des expressions temporelles :
▪     les signaux, qui indiquent la présence d’une relation temporelle (TLINK) entre deux éventualités ou expressions
temporelles. Ainsi, la préposition avant sera annotée comme un signal dans l’énoncé je suis parti avant midi.
▪     les modifieurs (MOD), qui altèrent ou précisent une portée temporelle. Par exemple, la locution en fin de sera annoté
comme un modifieur dans l’énoncé je suis parti en fin de matinée.

Ces entités jouent des rôles relativement proches mais donnent lieu à des conventions d’annotation radicalement
différentes. Les signaux sont des objets d’annotation propres, délimités par un balisage spécifique (<SIGNAL> avant
</SIGNAL>) alors que les modifieurs conduisent simplement à l’ajout d’un attribut MOD dans l’élément qu’ils
caractérisent. ISO TimeML donnera ainsi sur notre exemple l’annotation (simplifiée) suivante : en fin de <EVENT …
MOD=’END’> matinée </EVENT>. Une description homogène rendrait plus fiable le processus d’annotation.
4.3      Expressions temporelles
L’annotation des expressions temporelles dans TimeML dérive du tag TIMEX2 utilisé dans TIDES (Ferro 2005). Cette
norme répond plus aux besoins de la RI que du TAL. Les expressions temporelles y sont en effet intégralement évaluées
temporellement en contexte. Ainsi, “le mois dernier” sera par exemple annotée 2014-05. Cette convention pose des
565

[P-C.2]
ANNOTATION DE LA TEMPORALITE EN CORPUS
problèmes de cohérence : le contexte d’évaluation ne sera en effet pas toujours connu pour permettre cette résolution
temporelle. Cela renforce par ailleurs la part de subjectivité laissée à l’annotateur, qui ne cherchera pas toujours
l’information nécessaire à cette évaluation. Ici encore, TimeML mélange deux niveaux d’annotation : celui du signifiant
linguistique et celui de son interprétation qui relève d’un raisonnement purement temporel. Nous proposons donc
d’annoter “le mois dernier” sous la forme -1M (mois antérieur) et de laisser à une autre couche d’annotation la question
de la résolution complète de l’expression temporelle.

4.4        Attributs liés aux éventualités
Enfin, certaines limitations concernent les attributs liés aux éventualités. L’une d’entre elles est spécifique l’adaptation
au français de TimeML (Bittar et al. 2011). Le French TimeBank ajoute, pour le français, des temps dans le système de
valeurs de l’attribut TimeML tense. On retrouve un temps past mais également un temps imperfect. Ce choix
manque de pertinence linguistique : le terme imperfect étant traditionnellement attaché à une notion d'aspect, les valeurs
du trait ici croisent d'un côté une notion de temps linguistique (tense en anglais et qui pourrait justifier imperfect) et
de l'autre une représentation du temps extralinguistique en 3 parties past, present, future.

D’autres limitations correspondent à des manques de la norme. Par exemple ISO TimeML prévoit un attribut neg qui
permet de représenter la négation d’une éventualité (exemple : il ne pleut pas). Dans cet esprit, il nous semble manquer
un attribut pour marquer l'occurrence de l’éventualité dans une question. Cette information peut par exemple avoir une
utilité pour l’identification des actes de langage liés à l’éventualité. Trois valeurs peuvent être portées par cet attribut :
-     YEST    si la question porte sur la temporalité de l’évènement. Par exemple : quand viendras-tu ?
-     YES     si la question porte sur l'occurrence ou non de l’éventualité. Par exemple : viendras-tu ?
-     NO      lorsqu’il n’y a pas d’une question ou qu’elle porte sur une autre dimension. Par exemple : Où vas-tu ?

4.5        Relations temporelles
Le dernier élément important de la norme concerne les relations temporelles. ISO TimeML intègre un type de relation
(TLINK) qui peut concerner de manière indifférente les éventualités ou les expressions temporelles, alors que les types
ALINK et SLINK ne concernent que les éventualités. Nous regrettons là encore le groupement sous la même annotation
d’objets décrivent des réalités différentes (cf § 3.1.). Il serait plus cohérent de réserver le type TLINK aux seules
relations entre éventualités, et d’ajouter un type spécifique pour les relations d’ancrage liant une expression temporelle à
une éventualité. Pustejovsky suit d'ailleurs cette logique lorsqu'il propose (séminaire invité, Paris 2014) de créer une
relation MLINK pour représenter une relation entre un TIMEX et un EVENT dont le premier donne la durée du second.

On relève en outre que le type TLINK se limite aux relations d’ordonnancement (égalité, précédence, post-occurence...)
de Allen (1983). Cette convention ignore l’existence de correspondances plus complexes, comme par exemple des
relations d’inclusion temporelle ontologiquement décrites. Considérons l’exemple suivant :
(12) Je suis né à Noël, précisément, le 25 décembre 1966. Cette année-là il a fait très froid durant tout l’hiver

TimeML représente 25 décembre 1966 comme une date mais ne peut exprimer la relation d’inclusion qu’elle partage
ontologiquement avec l’expression englobante cette année-là ? On se retrouve ici avec une relation de type
meronymique (partie_de) au niveau temporel. Celle-ci s’apparente par analogie avec les anaphores associatives que
l’on retrouve lorsque l’on s’interesse à la référence (Webber 1988). L’attribut reltype de TLINK doit décrire ce type
de relation. Par analogie, on propose de rajouter la valeur bridging au système de valeur associé à cet attribut.

5         Conclusion

Cet article a cherché à caractériser certaines limitations de la norme ISO TimeML qui demandent à être corrigées pour
atteindre une annotation qui réponde réellement aux besoins du TALN, des sciences du langage mais également de
l’ingénierie des connaissances. Certaines réponses à ces observations ne requièrent qu’une modification à la marge de la
norme. La question de la délimitation des éventualités étant plus problématique de notre point de vue. Nous proposons
de ne pas se limiter à la tête lexicale mais bien de l’étendre à tout le signifiant linguistique correspondant. La taille des
ressources annotées avec la norme TimeML reste encore limitée. Il nous semble donc plus qu’urgent de la remettre
partiellement en question, sur des points qui, comme nous l'avons rappelé, ont souvent été déjà questionnés.
566

[P-C.2]
A. LEFEUVRE, J-Y. ANTOINE, A. SAVARY, E. SCHANG, L. ABOUDA, D. MAUREL, I. ESHKOL
Références
ALLEN J. F. (1983). Maintaining knowledge about temporal intervals. Communications of the ACM, 26(11), 832–843.
BACH E. W. (1981). Time, Tense, and Aspect : An Essay in English Metaphysics . In Radical Pragmatics, 63–81.
BATTISTELLI D. (2009). La temporalité linguistique: circonscrire un objet d'analyse ainsi que des finalités à cette
analyse Habilitation à diriger des Recherches, Université de Nanterre-Paris X.
BEJČEK E. & STRAŇÁK P. (2010). Annotation of multiword expressions in the Prague Dependency Treebank.
Language Resources and Evaluation, 44(1–2). 7–21.
BITTAR A. (2008) Annotation des informations temporelles dans des textes en français. Actes RECITAL’2008.
BITTAR A., AMSILI P., DENIS P., DANLOS L. (2011). French TimeBank: An ISO-TimeML Annotated Reference
Corpus. Proc. ACL’2011, Portland, Oregon : États-Unis.
CLARK A. & LAPPIN S. (2010). Unsupervised learning and grammar induction. The Handbook of Computational
Linguistics and Natural Language Processing, 57.
DE SAUSSURE L. (1997). Le temps chez Beauzée : algorithmes de repérage et comparaison avec Reichenbach.
Cahiers Ferdinand de Saussure, (49):171–195.
DIPPER S., ZINSMEISTER H. (2010). Towards a standard for annotating abstract anaphora. In LREC 2010 Workshop
on Language Resources and Language Technology Standards, 54-59.
FERRO L. (2005). TIDES 2005 standard for the annotation of temporal expressions. MITRE Corp. Tech. Report.
GRAVIER G., ADDA G., PAULSON N., CARRÉ M., GIRAUDEL A. & GALIBERT O.(2012). The ETAPE corpus
for the evaluation of speech-based TV content processing in the French language. In International Conference on
Language Resources, Evaluation and Corpora, na. Turquie.
ISO (2008). ISO DIS 24617-1: 2008 Language resource management - Semantic annotation framework - Part 1: Time
and Events. International Organization for Standardization, ISO Central Secretariat, Genève, Suisse.
HOFFART J., SUCHANEK F. M., BERBERICH K., LEWIS-KELHAM E., DE MELO G. & WEIKUM G. (2011).
YAGO2: exploring and querying world knowledge in time, space, context, and many languages, in Proc. 20th
International Conference on World Wide Web, WWW 2011, Hyderabad, India (Companion Volume), 229–232.
MANI I., PUSTEJOVSKY J., GAIZAUSKAS R., SAURI R., CASTANO J., LITTMAN J., SETZER A., & KATZ G.
(2005). The specification language TimeML. In Mani I., Pustejovsky J., Gaizauskas R. (Eds.), The language of time : a
reader, chapter 27. Oxford.
MATHET, Y., WIDLÖCHER, A. (2009). La plate-forme GLOZZ : environnement d’annotation et d’exploration de
corpus. Actes de TALN-2009, pages 1–10
MUZERELLE J., LEFEUVRE A., ANTOINE J.-Y, SCHANG E, MAUREL D., VILLANEAU J., ESHKOL I. (2013).
ANCOR, premier corpus de français parlé d’envergure annoté en coréférence et distribué librement. Actes TALN’2013,
Les Sables d’Olonnes.
MUZERELLE J., LEFEUVRE A., SCHANG E., ANTOINE J.-Y., PELLETIER A., MAUREL D., ESHKOL I.,
VILLANEAU J. (2014). ANCOR_Centre, a large free spoken french coreference corpus. Proc. LREC’2014, Reykjavik.
OGRODNICZUK M., ZAWISŁAWSKA M., SAVARY A., GŁOWŃSKA K. (2013). Coreference Annotation Schema
for an Inflectional Language, Proc. CICLING’2013, Samos, Greece, In. LNCS 7816, Springer Verlag, 394-407.
PUSTEJOVSKY J., LITTMAN J., SAURI R. (2006). Argument Structure in TimeML. In Katz G., Pustejovsky J.,
Schilder F. (Eds.) Annotating, Extracting and Reasoning about Time and Events, Dagstuhl Seminar Proc., Schloss
Dagstuhl, Allemagne.
SAVARY A., WASZCZUK J., PRZEPIÓRKOWSKI A. (2010). Towards the Annotation of Named Entities in the
National Corpus of Polish. in Proc. LREC'10, Valetta, Malta.
WEBBER B. L. (1988). Tense as discourse anaphor. Computational Linguistics, 14(2), 61-73.
567
[P-C.4]
21ème Traitement Automatique des Langues Naturelles, Marseille, 2014
Un schéma d’annotation en dépendances syntaxiques profondes
pour le français ∗

Guy Perrier 1 Marie Candito 2 Bruno Guillaume 3
Corentin Ribeyre 2 Karën Fort 1 Djamé Seddah 4
(1) Université de Lorraine/LORIA    (2) Université Paris Diderot/INRIA
(3) Inria Nancy Grand-Est/LORIA     (4) Université Paris Sorbonne/INRIA

Résumé.         À partir du schéma d’annotation en dépendances syntaxiques de surface du corpus Sequoia, nous propo-
sons un schéma en dépendances syntaxiques profondes qui en est une abstraction exprimant les relations grammaticales
entre mots sémantiquement pleins. Quand ces relations grammaticales sont partie prenante de diathèses verbales, ces
diathèses sont vues comme le résultat de redistributions à partir d’une diathèse canonique et c’est cette dernière qui est
retenue dans notre schéma d’annotation syntaxique profonde.
Abstract.         We describe in this article an annotation scheme for deep dependency syntax, built from the surface
annotation scheme of the Sequoia corpus, abstracting away from it and expressing the grammatical relations between
content words. When these grammatical relations take part into verbal diatheses, we consider the diatheses as resulting
from redistributions from the canonical diathesis, which we retain in our annotation scheme.
Mots-clés :             schéma d’annotation, syntaxe profonde, grammaires de dépendance.

Keywords:               annotation scheme, deep syntax, dependency grammar.

1     Introduction
Les corpus annotés en dépendances syntaxiques présentent un intérêt croissant par rapport aux corpus annotés en syn-
tagmes dans la mesure où ils permettent plus directement d’extraire les relations prédicat-argument constitutives d’une
représentation sémantique. Cette extraction reste cependant non triviale, la syntaxe offrant une grande variabilité dans la
façon d’exprimer ces relations. Une solution consiste à définir un niveau intermédiaire, la syntaxe profonde, qui est une
abstraction de la syntaxe de surface et qui vise à se rapprocher du niveau sémantique.
La Théorie Sens-Texte (TST)(Mel’ˇcuk, 1988), parmi les différents niveaux de la langue qu’elle définit, propose un niveau
de syntaxe profonde, où seuls les mots sémantiquement pleins sont présents. Ce niveau de syntaxe profonde est lié à la
sémantique dans la mesure où les mots présents renvoient aux entrées d’un lexique sémantique.
Comme le pointe Zabokrtsky (2005), la TST est très proche du modèle de la Description Générative Fonctionnelle ou
Functional Generative Description (FGD) (Sgall et al., 1986), sur lequel s’appuie l’annotation du Prague Dependency
Treebank (Hajic et al., 2006). La FGD définit un niveau tectogrammatical qui s’apparente au niveau de syntaxe profonde
de la TST avec une différence dans son rapport à la sémantique : l’absence de lien avec un lexique sémantique est
compensée par un étiquetage des dépendances prédicat-argument avec des rôles thématiques.
Contrairement à ces deux modèles, le schéma de syntaxe profonde que nous proposons ne marque aucun engagement
sémantique, la sémantique ne servant qu’à lever les ambiguïtés de rattachement. Nous ne retenons que les mots sémanti-
quement pleins mais les relations entre ceux-ci expriment des fonctions grammaticales. Les diathèses verbales présentes
en surface sont considérées comme le résultat de redistributions à partir de diathèses canoniques et ce sont ces diathèses
canoniques qui sont représentées au niveau profond.
Nous sommes partis de l’annotation de surface du corpus Sequoia (Candito & Seddah, 2012b) et nous avons réalisé son
annotation en syntaxe profonde, en nous inspirant des méthodes de Bonfante et al. (2011). Le corpus Sequoia annoté
∗. Ce travail a été partiellement financé par l’Agence Nationale de la Recherche au titre du programme Investissements d’Avenir, au sein du Labex
EFL (ANR-10-LABX-0083). Nous avons bénéficié de discussions avec Alain Polguère et Sylvain Kahane, que nous remercions.
574

[P-C.4]
G UY P ERRIER , M ARIE C ANDITO , B RUNO G UILLAUME , C ORENTIN R IBEYRE , K ARËN F ORT, D JAMÉ S EDDAH
en syntaxe profonde est librement disponible 1 . Si nous décrivons, dans (Candito et al., 2014), l’ensemble du travail
d’annotation du corpus, nous nous concentrons ici sur la présentation du schéma d’annotation. Une description plus
précise des phénomènes est fournie dans le guide d’annotation sur le site du corpus.

2     Schéma d’annotation en syntaxe profonde
Nous avons défini notre schéma d’annotation en syntaxe profonde en partant du schéma d’annotation en dépendances de
surface du corpus Sequoia, ce qui a eu une influence certaine sur le résultat. Il est donc important de détailler comment
le schéma de surface a été lui-même défini : le corpus Sequoia a été annoté d’abord en arbres de constituants, en suivant
le schéma d’annotation du corpus arboré de Paris 7 (Abeillé & Barrier, 2004) ou French Treebank (FTB). Le corpus
en constituants a ensuite été converti automatiquement en dépendances de surface, en suivant la procédure décrite dans
(Candito et al., 2010). Les arbres de dépendances résultants suivent ainsi très largement les choix linguistiques du FTB
(Abeillé et al., 2004; Abeillé, 2004), dans la mesure où la majorité des phénomènes syntaxiques sont mécaniquement
traduits en dépendances 2 . Enfin, les dépendances longue distance ont été corrigées manuellement (Candito & Seddah,
2012a) dans les arbres de dépendance obtenus par conversion automatique, ce qui a introduit quelques arcs non projectifs 3 .
Le schéma d’annotation de surface résultant est notre point de départ. Nous avons cherché, pour des raisons pragma-
tiques, à minimiser les divergences entre les niveaux surfacique et profond, pour nous concentrer sur les phénomènes
non directement représentables dans les arbres de surface. Aussi, par exemple, avons-nous conservé la réprésentation des
coordinations avec le premier conjoint comme tête, bien que cela ne permette pas de distinguer les dépendants partagés
par plusieurs conjoints des dépendants du seul premier conjoint.

2.1     Choix théoriques
L’objectif principal de nos représentations syntaxiques profondes (R EPR S YNT P ROF dans la suite) est de généraliser sur
la variation syntaxique autant que possible sans faire de distinctions ni de généralisations purement sémantiques. Nous
utilisons pour cela la notion de sous-catégorisation canonique et représentons les changements de diathèse comme des
redistributions des fonctions grammaticales sous-catégorisées par un lexème. Nous inspirant de la Grammaire Relation-
nelle (Perlmutter, 1983), nous distinguons fonction grammaticale canonique et fonction grammaticale finale d’une part,
et cadre de sous-catégorisation canonique (CS canonique) et cadre de sous-catégorisation final (CS final) 4 .
Définissons d’abord la notion de sous-catégorisation finale pour un verbe : elle contient d’une part les fonctions finales
associées aux arguments exprimés du verbe, et d’autre part, dans le cas d’ellipse et/ou de verbes non conjugués, les fonc-
tions des éléments qui seraient des arguments du verbe si celui-ci était utilisé sans ellipse et conjugué. Cette formulation
permet de faire entrer dans le CS final par exemple le sujet des infinitifs, le sujet ou l’objet des participes épithètes, le
sujet de verbes coordonnés ou plus généralement tout argument partagé par plusieurs prédicats. Par exemple, dans « Anna
veut dormir, mais devra peut-être veiller », le CS final de dormir est [sujet] rempli par Anna, et le CS final de devra est
[sujet, objet], rempli par Anna et veiller. Entrent également dans le CS final l’élément modifié par un participe épithète :
par exemple pour « les personnes nées en 40 », le nom personnes est la tête du sujet final de nées.
Passons maintenant à la définition précise de la sous-catégorisation canonique. Afin de neutraliser la variation syntaxique
due aux changements de diathèse, nous considérons ceux-ci comme des redistributions des fonction canoniques associées
aux arguments syntaxiques. Suivant la Grammaire Relationnelle (Perlmutter, 1983), le CS final est vu comme résultant de
l’application de 0 à n redistributions sur un CS canonique. Etant donnée une occurrence de verbe, le CS canonique peut
donc par définition être obtenu par application inverse des redistributions appropriées. Un exemple simple est le cas d’un
verbe au passif dont le CS final est [sujet, par-objet] et le CS canonique est [sujet, objet]. Les éléments du CS canonique,
appelés arguments syntaxiques profonds, sont obligatoirement sémantiquement pleins. Le il explétif n’appartient qu’au CS
final. Donc dans l’exemple « Trois personnes arrivent », à partir du CS canonique [sujet], la redistribution de l’impersonnel
rétrograde le sujet en objet direct, et un il explétif remplit la fonction sujet final : « Il arrive trois personnes » a pour CS
canonique [sujet] et pour CS final [sujet, objet].
Pour définir nos R EPR S YNT P ROF, nous n’avons considéré que les redistributions qui comportent un marquage morpho-
syntaxique (typiquement l’auxiliaire pour le passif, ou le clitique sémantiquement vide se pour les alternances moyennes

1. http ://deep-sequoia.inria.fr
2. Des informations supplémentaires sont prédites par la procédure de conversion, en cas de sous-spécification dans la version en constituants (c’est
le cas pour les étiquettes de dépendances pour les dépendants de gouverneurs non verbaux).
3. Par exemple, à la conversion de l’arbre syntagmatique de « ... le succès que la municipalité était en droit d’attendre », le pronom relatif est
mécaniquement rattaché comme dépendant de était, et manuellement corrigé pour dépendre de attendre.
4. La Grammaire Relationnelle utilise l’adjectif initial plutôt que canonique.
575

[P-C.4]
U N SCHÉMA D ’ ANNOTATION EN DÉPENDANCES SYNTAXIQUES PROFONDES POUR LE FRANÇAIS
et neutres). Les alternances syntaxiques sans marquage morpho-syntaxique ne sont pas capturées au sein de nos R EPR -
S YNT P ROF, et donnent lieu à des CS canoniques différents. Les repérer, en l’absence de marquage formel, relève pour
nous de l’analyse sémantique. Par exemple, pour le verbe baisser, les deux emplois X baisse Y et Y baisse, typiquement
reliés par l’alternance causative/inchoative non marquée, ne sont pas reliés dans nos R EPR S YNT P ROF, et ont deux CS ca-
noniques distinctes [sujet, objet] et [sujet] respectivement, qui sont aussi ici les CS finaux en l’absence de redistribution.
En revanche, pour l’alternance moyenne (par exemple « On avale facilement ce médicament / Ce médicament s’avale
facilement ») ou l’alternance neutre (qui “efface” l’actant agentif ou causal, comme dans « Cela dissout le médicament
/ Le médicament se dissout (de lui-même) »), le lien entre les deux versions est capturé par redistribution, et pour ces
deux alternances, l’objet direct dans la version transitive (médicament) est sujet final mais objet canonique dans la version
intransitive. Nous retenons comme redistributions : le passif, l’impersonnel, le moyen, le neutre et le causatif (voir les
exemples en section 3) 5 , certaines pouvant interagir. Nous renvoyons à (Candito, 1999) pour une étude des interactions
entre redistributions pour le français.
Nous avons volontairement fait deux distinctions différentes : représentation profonde versus de surface, et fonction
grammaticale finale versus canonique. Cela nous est utile pour capturer par exemple la régularité concernant le contrôle
des sujets des infinitifs : il s’agit du sujet final de l’infinitif, quelle que soit la diathèse de celui-ci. Ainsi, pour « Paul veut
être embauché », dans la représentation de surface, Paul est le sujet final de veut. Dans la représentation profonde, il est
aussi le sujet final de être embauché et son objet canonique.
Outre les verbes, nous traitons dans nos annotations les adjectifs, auxquels la notion de sous-catégorisation peut être
étendue, mais sans distinction entre final et canonique (ce que nous ne détaillons pas ici par manque de place). Le travail
reste à faire pour les autres catégories de prédicats, en particulier les noms.
Nous pouvons maintenant lister précisément les informations qui sont explicitées dans nos R EPR S YNT P ROF par rapport
aux représentations de surface :
– le statut sémantique des mots de la phrase (sémantiquement vide ou plein),
– la liste complète des arguments syntaxiques profonds des verbes et des adjectifs,
– et leur fonction grammaticale canonique (et la ou les redistributions amenant à l’emploi observé en surface).
Dans nos R EPR S YNT P ROF, chaque argument syntaxique profond d’un prédicat lui est directement rattaché. Plus préci-
sément, nous prenons comme tête d’un argument syntaxique profond l’élément sémantiquement plein le plus haut (les
complémenteurs vides et les prépositions régies sont court-circuitées, comme le sont aussi les pronoms relatifs dont la
référence peut être résolue syntaxiquement, comme illustré dans la phrase (2) de la figure 1).

2.2     Caractéristiques formelles
Nous définissons une représentation complète comme un graphe de dépendances contenant à la fois la R EPR S YNT S URF et
la R EPR S YNT P ROF. La figure 1 fournit des exemples, tirés du corpus Sequoia, de représentations complètes. Les nœuds
sont les mots de la phrase (ou des composants de composés réguliers), et sont typés comme sémantiquement vides (en
rouge dans la figure) ou pleins (en noir). Les arcs portent :
– une information sur leur appartenance à la R EPR S YNT S URF ou pas, et leur appartenance à la R EPR S YNT P ROF ou
pas : un arc peut être surfacique mais non profond (arcs rouges), profond mais non surfacique (arcs bleus), et à la fois
profond et surfacique (arcs noirs) ;
– une étiquette qui est constituée soit d’une seule fonction, à la fois finale et canonique, pour les fonctions n’intervenant
jamais dans des changements de diathèse (comme Mod), soit de la fonction finale et de la fonction canonique (dans
toute la suite, une étiquette notée “f : c” correspond à la fonction finale f et la fonction canonique c).
La R EPR S YNT P ROF pour une phrase donnée est formée des nœuds sémantiquement pleins et des arcs profonds, c’est-
à-dire à la fois les arcs qui sont profonds et surfaciques (noirs) et les arcs profonds non surfaciques (bleus). Il s’agit
formellement d’un graphe orienté, qui peut contenir des cycles et des arcs multiples (un même couple gouverneur /
dépendant peut être relié par plusieurs arcs de même orientation mais d’étiquettes différentes).

3      Illustration sur des exemples de phénomènes relatifs au verbe
Dans cette section, nous montrons comment le schéma qui vient d’être défini s’applique à certains phénomènes linguis-
tiques intéressants. Compte tenu des contraintes de l’article, nous nous limitons à des phénomènes relatifs au verbe. Les
exemples présentés dans la figure 1 illustrent ces phénomènes et les interactions complexes qui existent entre l’ajout de
relations prédicat-argument liées à la syntaxe profonde, la redistribution entre les fonctions grammaticales canoniques et

5. Le réfléchi, bien que présentant des propriétés « intransitivantes » qui peuvent justifier une représentation via redistributions, n’est pas traité dans
nos R EPR S YNT P ROF via redistribution (cf. l’exemple (1), dans la figure 1).
576

[P-C.4]
G UY P ERRIER , M ARIE C ANDITO , B RUNO G UILLAUME , C ORENTIN R IBEYRE , K ARËN F ORT, D JAMÉ S EDDAH
finales, et la suppression de mots sémantiquement vides. 6

Transformation de marqueurs grammaticaux en traits Sont concernés les auxiliaires et les clitiques ne représentant
aucun argument mais destinés à modifier la sémantique du verbe auquel ils s’appliquent.
En R EPR S YNT S URF, les auxiliaires (pour les temps composés, les passifs et les causatifs) sont systématiquement
rattachés au dernier verbe du noyau verbal (le verbe sémantiquement plein) et ils sont supprimés en R EPR S YNT-
P ROF. L’information de mode, de temps et éventuellement de diathèse dont ils sont porteurs est conservée sous
forme de traits profonds attachés au verbe plein.
Les clitiques réfléchis intrinsèques se sont également traités comme des marqueurs et systématiquement transformés
en traits (cf. infra la section spécifique sur les réfléchis).
(1) auxiliaire, réfléchi, clivée
mod.cleft
ats:ats
mod                                                                                           suj:suj                                                                               obj.p
dep                                                                                         obj:obj                                                                                det
suj:suj             mod                                                                          det                                                                                           aux.tps                        mod                                                    mod
Ce              sont        déjà                    plus          de                trente                   personnes                              qui             se               sont                      inscrites                  pour             la      prochaine                          séance
CL               V             ADV                     ADV         P                       D                                N                      PRO                 CL                  V                     V                         P               D                 A                             N
dm=ind
dt=passe_comp

dep.de                                                                                               obj:obj                                                                                                            suj:suj
det                                                                               suj:suj
(2) causatif, verbe à montée, relative épithète
obj:obj
mod.rel                                                                             obj:obj                                                   obj.p
mod                                      mod                                                                    suj:suj                                               aux.caus              a_obj:suj                    det                   dep                 obj.p
...       différentes                         techniques                           artistiques                           qu'              ils             pourront                          faire                   partager                à        l'       ensemble                        du            groupe
...                   A                                       N                                  A                       PRO              CL                       V                             V                         V                P        D                 N                      P+D               N
dm=inf
dt=pst
diat=caus

suj:suj                                  suj:suj                                                                                              suj:argc                                                   a_obj:suj                                     dep.de
obj:obj
(3) réfléchi, impersonnel
obj.cpl                                                                                                 mod
obj:obj                                                                                  suj:_                                                                obj:suj
suj:suj                     mod                                                           obj:obj                                                aff                                    mod                                                                dep                        obj.p                          obj.p
Je          voudrais                       seulement                          rappeler                          qu'              il         se                  passe                          actuellement                               quelque_chose                            de           terrible                en     Serbie
CL                    V                                 ADV                                 V                    CS              CL         CL                      V                                          ADV                                   PRO                             P                 A                P            N
dl=se_passer

suj:suj                                                                            obj:obj                                                                                                                                       suj:suj
dep.de
(4) réfléchi, coordination
coord
suj:suj                                               obj:obj                                                                                                       obj.p                                                                 obj:obj
det                     mod                                                                            aff.demsuj             a_obj:a_obj              obj.p                dep                                det                          dep.coord                                        det
les             saisies              illégales                    pourraient                             s'           étendre                        au             reste                      de              l'           Afrique             et        entraîner                      une          catastrophe
D                    N                             A                           V                        CL                 V                        P+D                     N                   P             D                N               CC                 V                      D                         N
diat=demsuj

suj:suj                                                                                                          a_obj:a_obj                                          dep.de
suj:obj
suj:suj
F IGURE 1 – Exemples d’annotation de certains phénomènes. La R EPR S YNT P ROF est formée des mots en noir, des arcs
noirs et des arcs bleus. La R EPR S YNT S URF contient tous les mots, et les arcs rouges et noirs.

Les causatifs La phrase (2) illustre une construction causative dont le noyau est « faire partager » et le passage de la
R EPR S YNT S URF à la R EPR S YNT P ROF montre l’interférence entre plusieurs phénomènes.
Le verbe pourront est ici un verbe à montée du sujet, si bien qu’ils est sujet profond final de partager. Le changement
de la diathèse finale causative en diathèse canonique fait qu’ils passe de la fonction finale SUJ à la fonction canonique
6. Les schémas sont simplifiés et ne font apparaître que les traits pertinents concernant les phénomènes décrits dans l’article. Dans la version finale,
une adresse URL renverra à un tableau complet des traits utilisés.
577

[P-C.4]
U N SCHÉMA D ’ ANNOTATION EN DÉPENDANCES SYNTAXIQUES PROFONDES POUR LE FRANÇAIS
ARGC , pour « argument causateur ». Parallèlement, à passe de AOBJ à SUJ , mais comme en R EPR S YNT P ROF , la
préposition régie est effacée, ces fonctions sont transférées sur ensemble.
Enfin, l’auxiliaire causatif faire est supprimé et une indication de diathèse causative (diat=caus) est ajoutée au
verbe partager 7 .
Les clitiques réfléchis Le clitique réfléchi se a été annoté de trois manières différentes. Dans la phrase (1), se est un
vrai réfléchi, objet direct du verbe inscrites en R EPR S YNT S URF et il disparaît en R EPR S YNT P ROF ; sa fonction
OBJ est alors reportée sur son antécédent personnes (via le pronom relatif qui qui disparaît lui aussi en R EPR S YNT-
P ROF). Dans la phrase (3), se est un réfléchi intrinsèque. Affixe du verbe essentiellement pronominal se passer, il
disparaît en R EPR S YNT P ROF, où il est remplacé par un trait de lemme profond dl=se_passer attaché à passer.
Enfin, dans la phrase (4), s’ avec le verbe étendre s’inscrit dans une diathèse neutre. Cette diathèse provient d’une
redistribution dans laquelle l’objet canonique devient le sujet final et le sujet canonique n’a aucun référent claire-
ment identifié. Cela la distingue de la diathèse moyenne où ce référent n’est pas exprimé mais existe. Comme il
est difficile de distinguer les deux constructions, nous les marquons toutes deux en R EPR S YNT P ROF par un trait
diat=demsuj attaché au verbe, étendre dans notre exemple.
Les sujets impersonnels Le clitique il explétif peut apparaître soit comme sujet final de verbes essentiellement imper-
sonnels, soit dans le cas de diathèses impersonnelles de verbes qui peuvent également apparaître avec des sujets
référentiels. La phrase (3), avec « il se passe », illustre le second cas. Le clitique il disparaît en R EPR S YNT P ROF et
l’objet final quelque chose y est sujet canonique.
Les coordinations Selon le schéma d’annotation du FTB, la tête de la structure de coordination est la tête du premier
conjoint. La conjonction de coordination est reliée à la tête avec une étiquette COORD et la tête du second conjoint
à la conjonction avec une étiquette DEP. COORD. Ces dépendances sont présentes aussi bien en surface qu’en pro-
fondeur. Pour des raisons sémantiques, nous traitons au niveau profond les dépendances entrantes différemment
des dépendances sortantes. D’un point du vue entrant, une coordination est vue comme un tout et les dépendances
ne sont pas distribuées entre les conjoints (autrement dit, nous ne désambiguïsons pas entre lecture collective et
distributive de la coordination). En revanche, les dépendants d’un élément coordonné qui sont reliés à son premier
conjoint en surface sont distribués aux autres conjoints en profondeur (sauf exception non détaillée ici). Ainsi, dans
la phrase (4), le sujet profond saisies des conjoints étendre et entraîner est distribué, alors que le gouverneur de la
coordination pourraient n’est pas distribué.
Les clivées La phrase (1) est une clivée : le syntagme nominal « plus de trente personnes » est extrait de la phrase
canonique « plus de trente personnes se sont inscrites pour la prochaine séance » pour être placé en attribut du sujet
de sont, afin que l’accent soit mis sur lui. La trace du syntagme extrait est représentée par le pronom relatif qui et la
phrase canonique devient une proposition relative rattachée à sont par une dépendance MOD . CLEFT.
En R EPR S YNT P ROF, la phrase canonique n’est pas complètement restaurée et la dépendance MOD . CLEFT est
conservée afin de permettre au verbe sont de recevoir des modificateurs, comme l’adverbe déjà dans l’exemple.
Seuls ce et qui sont supprimés et la fonction sujet du pronom relatif est transférée sur son antécédent personnes.

4      Représentation syntaxique profonde versus représentation sémantique
Si nos R EPR S YNT P ROF partagent certaines caractéristiques avec des représentations sémantiques (qui sont parfois appe-
lées profondes), elles s’en différencient sur des points importants. Le premier point est que dans nos R EPR S YNT P ROF, le
sens des nœuds lexicaux n’est pas désambiguisé. La sémantique n’est utilisée que pour désambiguiser des rattachements
syntaxiques ou des changements de diathèse. Ensuite, même si les mots sémantiquement vides sont écartés de nos R E -
PR S YNT P ROF, les nœuds restants ne forment pas nécessairement une unité sémantique. En effet, les unités polylexicales
(expressions figées, constructions à verbe support, composés syntaxiquement réguliers) ne sont pas marquées comme for-
mant une unité, et sont représentées par une structure syntaxique régulière.
En outre, la sous-catégorisation canonique des prédicats explicitée dans les R EPR S YNT P ROF peut contenir des éléments
qui ne sont pas des arguments sémantiques du prédicat (comme par exemple le sujet ils du verbe à montée pourront de la
phrase (2), dans la figure 1). Enfin, alors que les représentations sémantiques typent en général les actants sémantiques des
prédicats au moyen de rôles sémantiques ou de simples numéros, nos R EPR S YNT P ROF utilisent seulement les fonctions
canoniques, ce qui est cohérent avec le fait que les lemmes ne sont pas désambiguisés. Les fonctions demeurent alors des
indices importants pour la (future) désambiguisation des prédicats.
Pour résumer, pour obtenir des structures prédicat-arguments (sémantiques) à partir de nos R EPR S YNT P ROF, il serait
nécessaire de repérer les unités polylexicales, désambiguiser les prédicats, ne retenir que les arguments sémantiques de
7. Par manque de place, nous ne détaillons pas ici les différents types de causatifs.
578

[P-C.4]
G UY P ERRIER , M ARIE C ANDITO , B RUNO G UILLAUME , C ORENTIN R IBEYRE , K ARËN F ORT, D JAMÉ S EDDAH
ceux-ci parmi les arguments syntaxiques, et les associer à un rôle sémantique ou une simple numérotation. Des numéros
d’argument peuvent être obtenus en utilisant un ordre d’oblicité des fonctions grammaticales canoniques (typiquement
sujet < objet < obliques). A noter cependant que les sujets canoniques de nos R EPR S YNT P ROF peuvent correspondre
aussi bien à des proto-agents qu’à des proto-patients (au sens de (Dowty, 1991)), cette distinction étant considérée comme
sémantique. Nos R EPR S YNT P ROF ne capturent donc pas que dans une alternance causative/inchoative, par exemple X
baisse Y versus Y baisse, l’argument Y joue le même rôle sémantique. Cette généralisation est capturée notamment dans
la ressource PropBank (Palmer et al., 2005) en utilisant un même rôle arg1 pour l’argument Y dans les deux variantes.
D’autres travaux prévoient plutôt un lien au niveau du lexique, entre le lexème transitif et le lexème intransitif : c’est ce qui
est prévu dans le dictionnaire explicatif et combinatoire de la TST (Mel’ˇcuk et al., 1999), et dans le DeepBank (Flickinger
et al., 2012), constitué de représentations syntactico-sémantiques dérivées d’analyses HPSG de l’anglais (Oepen, p.c).
Quoi qu’il en soit, nous espérons que la libre mise à disposition du corpus et de son guide d’annotation permettront tant
le développement de ressources syntaxico-sémantiques plus riches que des perspectives nouvelles d’application.

References
A BEILLÉ A. (2004). Guide des annotateurs, annotation fonctionnelle. LLF Annotation Guide.
A BEILLÉ A. & BARRIER N. (2004). Enriching a French treebank. In Proc. of LREC, Lisbonne, Portugal.
A BEILLÉ A., T OUSSENEL F. & M ARTINE C. (2004). Corpus le monde, annotations en constituants, guide pour les
correcteurs. LLF Annotation Guide.
B ONFANTE G., G UILLAUME B., M OREY M. & P ERRIER G. (2011). Enrichissement de structures en dépendances par
réécriture de graphes. In Proc. of TALN, Montpellier, France.
C ANDITO M. (1999). Organisation modulaire et paramétrable de grammaires électroniques lexicalisées application au
français et à l’italien. PhD thesis, Université Paris Diderot.
C ANDITO M., C RABBÉ B. & D ENIS P. (2010). Statistical french dependency parsing: Treebank conversion and first
results. In Proc. of LREC, La Valette, Malte.
C ANDITO M., P ERRIER G., G UILLAUME B., R IBEYRE C., F ORT K., S EDDAH D. & D E L A C LERGERIE É. (2014).
Deep Syntax Annotation of the Sequoia French Treebank. In International Conference on Language Resources and
Evaluation (LREC), Reykjavik, Islande.
C ANDITO M. & S EDDAH D. (2012a). Effectively long-distance dependencies in French: annotation and parsing evalu-
ation. In Proc. of TLT 11, Lisbonne, Portugal.
C ANDITO M. & S EDDAH D. (2012b). Le corpus Sequoia : annotation syntaxique et exploitation pour l’adaptation
d’analyseur par pont lexical. In Proc. of TALN, Grenoble, France.
D OWTY D. (1991). Thematic proto-roles and argument selection. Language, 67(3), 547–619.
F LICKINGER D., Z HANG Y. & KORDONI V. (2012). Deepbank: A dynamically annotated treebank of the wall street
journal. In Proc. of the Eleventh International Workshop on Treebanks and Linguistic Theories, p. 85–96.
H AJIC J., PANEVOVÁ J., H AJICOVÁ E., S GALL P., PAJAS P., Š TEPÁNEK J., H AVELKA J., M IKULOVÁ M., Z A -
BOKRTSK Y ` Z. & R AZIMOVÁ M. Š. (2006). Prague dependency treebank 2.0. CD-ROM, Linguistic Data Consortium,
LDC Catalog No.: LDC2006T01, Philadelphie, 98.
M EL’ CUK I. (1988). Dependency syntax: theory and practice. State University Press of New York.
M EL’ CUK   I., A RBATCHEWSKY-J UMARIE N., I ORDANSKAJA L., M ANTHA S. & P OLGUÈRE A. (1999). Dictionnaire
explicatif et combinatoire du français contemporain. Recherches lexico-sémantiques IV [Explanatory and Combinatorial
Dictionary of Contemporary French. Lexico-Semantic Research IV]. Montréal: Les Presses de l’Université de Montréal.
PALMER M., G ILDEA D. & K INGSBURY P. (2005). The proposition bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1), 71–106.
P ERLMUTTER D. (1983). Studies in Relational Grammar 1. University of Chicago Press.
S GALL P., H AJI COVÁ E. & PANEVOVÁ J. (1986). The Meaning of the Sentence in Its Semantic and Pragmatic Aspects.
Dordrecht:Reidel Publishing Company and Prague:Academia.
Z ABOKRTSKY Z. (2005). Resemblances between Meaning-Text Theory and Functional Generative Description. In
Proc. of MTT 2005, p. 549–557, Moscou, Russie.
579
21ème Traitement Automatique des Langues Naturelles, Marseille, 2014                                                                                                                         [P-Demo1.6]
Z OMBI L INGO :
manger des têtes pour annoter en syntaxe de dépendances

Karën Fort1 Bruno Guillaume 2 Valentin Stern 1
(1) LORIA, Université de Lorraine
(2) LORIA, Inria Nancy Grand-Est
karen.fort@loria.fr, bruno.guillaume@loria.fr, valentin.stern@loria.fr

Résumé.         Cet article présente Z OMBILINGO un jeu ayant un but (Game with a purpose) permettant d’annoter des
corpus en syntaxe de dépendances. Les annotations créées sont librement disponibles sur le site du jeu.
Abstract.        This paper presents Z OMBILINGO, a Game With A Purpose (GWAP) that allows for the dependency
syntax annotation of French corpora. The created resource is freely available on the game Web site.
Mots-clés :                      jeux ayant un but, complexité, annotation, syntaxe en dépendances.

Keywords:                        GWAP, complexity, annotation, dependency syntax.
La production de ressources linguistiques de grande taille est très coûteuse, en particulier en main d’œuvre. Ainsi, le
coût d’annotation du Prague Dependency Treebank a été estimé à 600 000 dollars (Böhmová et al., 2001). Une alterna-
tive pour produire des ressources est l’utilisation de la myriadisation (crowdsourcing), c’est-à-dire le recours à la « foule
pour réaliser une tâche. Les jeux ayant un but, par exemple, ont été utilisés pour différentes tâches en TAL : J EUX D E -
M OTS 1 (Lafourcade, 2007) a pour but de créer un réseau lexical ; P HRASE D ETECTIVES 2 (Chamberlain et al., 2008) fait
annoter un corpus en anaphores. Ces deux jeux ont eu un succès considérable et ont permis de créer des ressources de qua-
lité raisonnable pour un coût réduit. Le premier fait appel au sens commun et le deuxième à des connaissances scolaires.
Dans d’autres domaines, il a été possible d’utiliser un jeu pour des tâches nettement plus complexes et qui nécessitent
une formation des personnes qui participent. Ainsi, dans F OLD I T (Cooper et al., 2010) les joueurs doivent manipuler des
représentations 3D de protéines pour étudier la façon dont elle peuvent interagir. Z OMBILINGO est inspiré de ces succès
et a pour but de faire réaliser à des joueurs une tâche de TAL réputée complexe : annoter des dépendances syntaxiques.
Les données que nous souhaitons produire sont des analyses en dépendances syntaxiques compatibles avec celles utilisées
pour le corpus Sequoia (Candito & Seddah, 2012). Elles sont illustrées par l’exemple ci-dessous.

obj.cpl                                                                                                                 mod.rel
suj                         obj                  a_obj                                                                  obj
suj            obj                                 mod             mod                 obj                   obj.p          dep        obj.p                               suj
J'         espère         que   nous             ne           devrons         pas   avoir         recours   au            type         de      mesures   que       vous               suggérez
CL            V             C     CL              ADV             V            ADV     V               N     P+D            N           P            N    PRO             CL              V
Ce choix nous permet d’utiliser le corpus Sequoia comme amorce pour Z OMBILINGO, notamment pour la phase de
formation des joueurs. Le système sera ensuite alimenté par des phrases issues de textes libres de droits, qui seront
pré-annotés à l’aide d’analyseurs syntaxiques. Quand une nouvelle phrase est ajoutée dans la base de données, sa pré-
annotation est considérée comme correcte ; dans la suite du jeu, si suffisamment de joueurs donnent un avis contraire
à la pré-annotation, l’annotation de la phrase considérée est modifiée pour en tenir compte. Il est donc possible à tout
moment de faire une extraction de la ressource annotée en syntaxe, qui tient compte de ce que tous les joueurs ont fait
précédemment.
L’un des enjeux essentiels de ce jeu est d’être capable de gérer la complexité de la tâche. Il n’est bien entendu pas possible
de demander à un joueur de produire l’annotation d’une phrase complète ; il faut décomposer la tâche globale en une série
de tâches plus élémentaires qui peuvent être confiées à des joueurs sans les décourager. Dans Z OMBILINGO, cette gestion
1. Voir : http://www.jeuxdemots.org.
2. Voir : http://anawiki.essex.ac.uk/phrasedetectives.
15

K ARËN F ORT, B RUNO G UILLAUME , VALENTIN S TERN                                                  [P-Demo1.6]
de la complexité s’appuie sur le découpage de la tâche suivant les différents phénomènes linguistiques présents dans la
phrase. Ce découpage permet également de mettre en place des séances de formations pour chacun des phénomènes et
donc de ne pas surcharger les joueurs d’informations : le joueur choisit un phénomène, suit la formation correspondante,
et peut ensuite commencer à jouer avec ce phénomène.
Un autre élément essentiel à la réussite de Z OMBILINGO est la motivation des joueurs. En effet, la production d’une
ressource de grande ampleur de qualité n’est possible que si beaucoup de joueurs utilisent le jeu et si une proportion
raisonnable d’entre eux restent longtemps et reviennent régulièrement jouer. Pour attirer les joueurs, le design est un
élément essentiel. Nous avons choisi le thème des zombies parce qu’il est fédérateur dans le monde du jeu et par clin
d’œil à la notion de tête d’une dépendance linguistique : annoter c’est « manger des têtes », c’est donc une tâche pour les
zombies ! La capture d’écran ci-dessous présente l’interface du jeu.
ZOMBILINGO                                                                                                              1. profil du joueur
2. progression de la partie
joueur 1
pourquoi
ce jeu ?
F.A.Q.
forum                                                                                                       3. aide interactive
1           JOUEUR 1
2
Invasion - Phrase 3/5                     3   Besoin d’aide pour
cette relation ?
4a. mot joué
Zombie affamé                                                                        Demande-moi !
Niveau
Level 4 4                                                                                                                  4b. relation ou phénomène à annoter
340                  Tu as gagné 334 points et 23 pièces pour le moment
4c. « main » pour le choix de la réponse
4a

ELLE A FAIT MONTER SON ASSISTANT
5. accès aux objets du jeu
4c
4b
Trouve le mot associé à la relation SUJET en glissant la main dessus
Récompense pour l’analyse de cette phrase : 40 points et 5 pièces
utiliser                                                                                             passer        soumettre
un objet                                                                                                               quitter
la partie

5
1. Player profile                      4b. Relation type or phenomena to find
2. Game progression                    4c.‘’Hand’’ to drag-and-drop to link the
3. Interactive help                    word played to the chosen answer
4a. Word played                        5. Access to in-game objects
Les mécanismes qui encouragent les joueurs à jouer suffisamment longtemps et à revenir régulièrement sont aussi un
élément clé de la réussite du jeu. En se basant sur les notions souvent utilisées pour les jeux (sérieux ou non), nous avons
prévu différents mécanismes qui correspondent aux différents types de joueurs existants. Ainsi, les mécanismes que nous
avons mis en place ont pour but de répondre aux attentes des quatre types de joueurs identifiés par Bartle (1996) : killers,
achievers, explorers et socializers.
Les données produites par les joueurs permettront de produire un corpus annoté en dépendances de surface qui sera mis à
jour en continu en fonction des actions des joueurs. Ce corpus sera mis à disposition librement.
Les auteurs tiennent à remercier Hadrien Chastant pour la première maquette, Charles Ancé pour ses magnifique dessins,
Alice Guyot pour les éléments de design et Mathieu Lafourcade pour son aide dans la conception du jeu.
Références
BARTLE R. (1996). Hearts, clubs, diamonds, spades : Players who suit MUDs. The Journal of Virtual Environments.
B ÖHMOVÁ A., H AJI Cˇ J., H AJI COVÁ
ˇ     E. & H LADKÁ B. (2001). The prague dependency treebank : Three-level annota-
tion scenario. In A. A BEILLÉ, Ed., Treebanks : Building and Using Syntactically Annotated Corpora. Kluwer Academic
Publishers.
C ANDITO M. & S EDDAH D. (2012). Le corpus Sequoia : annotation syntaxique et exploitation pour l’adaptation
d’analyseur par pont lexical. In Traitement Automatique des Langues Naturelles (TALN), Grenoble, France.
C HAMBERLAIN J., P OESIO M. & K RUSCHWITZ U. (2008). Phrase Detectives : a web-based collaborative annotation
game. In Proceedings of the International Conference on Semantic Systems (I-Semantics’08).
C OOPER S., T REUILLE A., BARBERO J., L EAVER -FAY A., T UITE K., K HATIB F., S NYDER A. C., B EENEN M.,
S ALESIN D., BAKER D. & P OPOVI C´ Z. (2010). The challenge of designing scientific discovery games. In Proceedings
of the Fifth International Conference on the Foundations of Digital Games, FDG ’10, p. 40–47.
L AFOURCADE M. (2007). Making people play for lexical acquisition. In Proceedings of the 7th Symposium on Natural
Language Processing (SNLP 2007).
16
21ème Traitement Automatique des Langues Naturelles, Marseille, 2014                                              [O-S2.1]
Étude quantitative des disfluences dans le discours de schizophrènes :
automatiser pour limiter les biais

Maxime Amblard, Karën Fort
Université de Lorraine, LORIA, INRIA, CNRS
UMR 7503, Vandœuvre-lès-Nancy 54500 France
{maxime.amblard, karen.fort}@loria.fr

Résumé.          Nous présentons dans cet article les résultats d’expériences que nous avons menées concernant les dis-
fluences dans le discours de patients schizophrènes (en remédiation). Ces expériences ont eu lieu dans le cadre d’une étude
plus large recouvrant d’autres niveaux d’analyse linguistique, qui devraient aider à l’identification d’indices linguistiques
conduisant au diagnostic de schizophrénie. Cette étude fait la part belle aux outils de traitement automatique des langues
qui permettent le traitement rapide de grandes masses de données textuelles (ici, plus de 375 000 mots). La première
phase de l’étude, que nous présentons ici, a confirmé la corrélation entre l’état schizophrène et le nombre de disfluences
présentes dans le discours.
Abstract.         We present in this article the results of experiments we led concerning disfluencies in the discourse of
schizophrenic patients (in remediation). These experiments are part of a larger study dealing with other levels of linguistic
analysis, that could eventually help identifying clues leading to the diagnostic of the disease. This study largely relies on
natural language processing tools, which allow for the rapid processing of massive textual data (here, more than 375,000
words). The first phase of the study, which we present here, confirmed the correlation between schizophrenia and the
number of disfluences appearing in the discourse.
Mots-clés :         discours pathologique, schizophrénie, disfluences.

Keywords:           pathological discourse, schizophrenia, disfluencies.
1     Introduction

1.1   Contexte et motivations de l’étude

Cette étude participe d’un projet plus large portant sur les pratiques langagières chez les schizophrènes en situation
d’entretiens semi-dirigés par un psychologue. Cette étude s’inscrit dans la continuité des premiers travaux de Chaika
(1974) et Fromkin (1975), qui cherchaient à mettre en avant les particularités langagières chez les schizophrènes.
Plusieurs aspects sont ainsi étudiés, notamment les capacités neuro-cognitives par une série de tests, le comportement
oculomoteur du patient par une série d’enregistrements par oculomètre (eye-tracker), l’activité encéphale par des enregis-
trements par électro-encéphalographe (EEG) et la pratique langagière par l’étude linguistique des entretiens. Dans cette
partie du projet, nous nous concentrons sur le dernier aspect et laissons donc de côté les autres mesures. Cependant, il
conviendra dans une phase ultérieure de revenir sur l’ensemble des analyses pour identifier des corrélations spécifiques.
Concernant la partie linguistique du projet, nous nous basons sur un résultat de psycho-linguistique mettant en avant des
usages pathologiques de la langue chez les schizophrènes, au travers de la notion de discontinuités pragmatiques déci-
sives (Musiol & Trognon, 1996; Verhaegen, 2007). Rebuschi et al. (2013) et Musiol et al. (2013) ont montré que dans
la succession des focus thématiques de la conversation, les schizophrènes rejouent une ambiguïté linguistique précédem-
ment introduite, rendant l’interprétation pragmatique et rhétorique impossible. Comme eux, nous souhaitons produire des
analyses formelles des extraits discontinus, afin d’en donner une interprétation dans un modèle formel du type SDRT
(Asher & Lascarides, 2003) (Segmented Discourse Representation Theory), extension à la rhétorique et à la pragmatique
de la DRT (Kamp & Reyle, 1993) (Discourse Representation Theory). Par ailleurs, il apparaît nécessaire de discuter les
292

M AXIME A MBLARD , K ARËN F ORT                                                        [O-S2.1]
règles du cadre formel car les extraits de dialogue nécessitent l’usage de règles non conventionnelles pour rattraper la
construction de telles structures.
Le projet général cherche à ré-interroger ces résultats sur un corpus plus large et sur un faisceaux d’indices diversifiés.
D’où, en particulier, l’utilisation des oculomètres et des EEG dans les protocoles. Nous cherchons à interroger également
d’autres niveaux linguistiques, en proposant une annotation multi-niveaux de la ressource.
Pour ce faire, la transcription, qui est la clé de voûte de l’ensemble du projet, doit être de qualité. Les outils de transcription
automatique que nous avons pu tester ont donné des résultats insuffisants. Elle a donc été réalisé manuellement. Nous
avons défini un guide d’annotation précis, dans la tradition de Blanche-Benveniste & Jeanjean (1987). Cependant, l’une
des difficultés du projet réside dans la nature des sujets, qui implique une gestion stricte de l’anonymat (voir section 2.3),
dont la conséquence est que nous devons minimiser le nombre de personnes ayant accès aux données non transformées.
Nous ne disposons donc pour l’instant que d’une seule version des transcriptions, ce qui ne nous permet pas de les évaluer
correctement en calculant un accord inter-annotateur.
À partir de ces transcriptions, plusieurs autres annotations vont être proposées, dont une partie va être produite par des
outils de traitement automatique des langues (TAL), et une autre par des humains (autre que les transcripteurs). Nous
présentons ici le premier niveau d’annotation du corpus, l’annotation en disfluences, réalisée grâce à l’outil Distagger
(Constant & Dister, 2010). Outre son utilité intrinsèque, l’annotation en disfluences permet de normaliser les corpus avant
d’y appliquer des analyseurs syntaxiques ou sémantiques.
1.2       Travaux précédents

Les travaux précédents menés sur le discours des schizophrènes ont donné peu de résultats concernant les disfluences
et ces résultats ne sont souvent qu’un élément accessoire de l’expérience décrite. Ainsi, Feldstein (1962) a travaillé sur
l’impact du type de contenu d’éléments à commenter (affectif ou non) et a, ce faisant, montré que les perturbations du
discours (speech disturbances) étaient plus élevées chez les schizophrènes 1 . Ces résultats sont confirmés par la méta
étude de Maher (Maher, 1972). Plus récemment, Kremen et al. (2003) ont montré, dans le cadre d’une étude concernant la
comparaison entre fluence phonémique et fluence sémantique chez les schizophrènes, que ceux-ci ont une fluence verbale
(quel que soit son type) légèrement dégradée par rapport aux témoins et aux patients bipolaires 2 .
Si les résultats semblent concorder, il n’existe à notre connaissance aucune étude publiée à ce sujet concernant des patients
francophones. Par ailleurs, la manière dont les données précédentes ont été annotées ou notées n’est jamais précisée,
mais on peut aisément supposer qu’elles l’ont été par des humains, ce qui, en l’absence d’accord inter-annotateurs, est
évidemment source de biais 3 . L’étude que nous proposons se singularise donc par l’utilisation d’outils de TAL.
Le présent article est organisé de la manière suivante : nous détaillons dans la section 2 la constitution du corpus et ses
implications dans l’étude, tant sur la couverture qu’il propose que sur la délicatesse avec laquelle il est nécessaire de
manipuler les données ; puis nous présentons l’outil utilisé pour produire les annotations en disfluences et le protocole
d’expérimentation dans la section 3 ; à partir de ces expérimentations, nous analysons dans la section 4 les résultats
obtenus, leur significativité et les biais potentiels de l’étude ; enfin, nous présentons les travaux à venir dans la conclusion.
2        Difficultés de constitution du corpus

2.1       Présentation du corpus

Le corpus utilisé pour cette étude est constitué de transcriptions d’entretiens. L’étude fait intervenir 79 sujets, 48 schi-
zophrènes et 31 témoins. Les entretiens ont été réalisés par des psychologues, en milieu hospitalier. Deux recueils de
données ont pu être réalisés, dans des unités médicales spécialisées : le premier à Ville1 4 , par deux psychologues, et le
second à Ville2, par une seule psychologue.
Le sous-corpus Ville1 a été constitué au second semestre 2013. Il est composé de 18 patients diagnostiqués schizophrènes
1.   Cette étude a impliqué 30 schizophrènes et 30 témoins.
2.   Cette étude a impliqué 83 schizophrènes, 15 patients bipolaires et 83 témoins.
3.   Ce même biais affecte nos transcriptions, mais nos annotateurs ne savaient pas à quoi celles-ci allaient servir, ce qui limite l’impact du biais.
4.   Nous avons anonymisé les noms de villes par respect pour la confiance des patients schizophrènes.
293

É TUDE QUANTITATIVE DES DISFLUENCES DANS LE DISCOURS DE SCHIZOPHRÈNES                                                [O-S2.1]
en remédiation et sous traitement, ainsi que de 23 témoins. Le sous-corpus Ville2 a été constitué au printemps 2002. Il est
composé de 30 patients diagnostiqués schizophrènes en remédiation et sous traitement, à l’exception de sept d’entre eux
(qui n’étaient pas sous traitement), et de huit témoins. Le tableau 1 présente la ventilation des sujets en fonction de leur
type (schizophrène ou témoin) et de leur sexe.

corpus Ville1                        corpus Ville2                        total
hommes femmes               total    hommes femmes              total
schizophrènes           15          3             18         20         10            30       48
témoins                 15          8             23          4          4             8       31
total                   30         11             41         24         14            38       79
TABLE 1 – Répartition des sujets dans le corpus.

L’interaction mise en place pour cette étude est un entretien semi-directif conduit par un psychologue. Dans ce type
d’entretien, le psychologue n’est pas personnellement engagé dans l’interaction. Il doit maintenir un échange dans lequel
le patient revient sur son environnement et ses relations au sein de l’hôpital et avec l’extérieur. Il est clairement expliqué,
tant à l’équipe médicale qu’au patient, que le contenu de l’entretien ne peut être utilisé comme base médicale.
Le protocole expérimental a consisté à identifier des patients intéressés, puis à leur faire passer des tests de mesure de
capacité cognitive. Dans le sous-corpus de Ville2 aucune mesure supplémentaire n’a été faite, dans le sous-corpus de
Ville1, les entretiens ont eu lieu en présence d’un double système d’oculomètre 5 .
Le protocole a été défini de manière à être le moins invasif possible. Pour le sous-corpus Ville1, trois tests psychocognitifs
mesurant les capacités de mémoire à court terme, d’attention, et la mémoire de travail ont été passés par les sujets : (i)
le Wechsler Adult Intelligence Scale-III (mesure du quotient intellectuel, ou QI), (ii) le California
Verbal Learning Test (capacité cognitive et de stratégie), et (iii) le Trail Making Test (dépréciation de la
flexibilité cognitive et de l’inhibition). Nous n’utiliserons ici que les résultats du test de QI.
(a)                                                                        (b)
F IGURE 1 – Distribution des entretiens par la taille : (a) en nombre de tours de parole ; (b) en nombre de mots.

Une fois les entretiens enregistrés, ils ont été transcrits manuellement par deux annotateurs (le ou la psychologue qui a
mené tout ou partie des entretiens, et une autre personne) qui n’ont pas annoté en parallèle et qui ne savaient pas que nous
allions compter les disfluences. En moyenne, les entretiens du sous-corpus de Ville1 sont constitués de 552,73 tours de
parole, alors que les entretiens du sous-corpus Ville2 en contiennent 234,5. L’ensemble du corpus comprend 31 575 tours
de parole, soit environ 375 000 mots. Le tableau 2 présente le découpage en tours de parole et en mots du corpus, et la
figure 1 illustre la distribution des corpus en fonction de leur taille en nombre de tours de parole et de mots. Le caractère
spécifique de l’entretien semi-directif transparaît ici clairement : le psychologue produit quasiment le même nombre de
5. Ce double système permet de capter les points de fixation du regard du sujet sur ’ce qu’il voit’, mais également de capter ceux du psychologue.
Ainsi, il est possible d’interpréter si un mouvement est déclenché par une interaction visuelle (regard de l’interlocuteur) ou non.
294

M AXIME A MBLARD , K ARËN F ORT                                       [O-S2.1]
corpus Ville1                               corpus Ville2
nb tours de parole       nb de mots          nb tours de parole      nb de mots
S         3 863               46 859                   4 062               66 725
} 11 145               } 119 762             } 4 433              } 79 081
T         7 282               72 903                   371                 12 356
P +S      3 819               30 293                   4 098               33 686
} 11 517               } 138 571             } 4 480              } 37 842
P +T      7 698               108 278                  382                 4 156
total                22 662                258 333                  8 913            116 923
TABLE 2 – Décomposition du corpus en sous-corpus, en nombre de tours de parole et nombre de mots, en fonction du
type d’interlocuteur. S (schizophrènes), T (témoins), P + S (psychologue avec un schizophrène), P + T (psychologue avec
un témoin).
tours de parole que le sujet, pour un volume de mots très inférieur. Par exemple, dans le sous-corpus Ville1, le ratio entre
le nombre de tours de parole des schizophrènes et des psychologues devant un schizophrène est de 1,003 (seulement 44
tours de parole), alors qu’il est de 1,54 en nombre de mots. Seuls les témoins du sous-corpus de Ville1 ne présentent
pas cette caractéristique, mais une analyse plus fine des entretiens montre que pour six entretiens, les témoins sont restés
réticents à prendre la parole.
2.2    Difficultés d’accès aux patients

Le nombre de 79 sujets peut sembler limité pour une étude de ce type, mais la constitution d’une telle ressource implique
de surmonter de nombreuses difficultés, en particulier pour accéder aux patients. De ce fait, disposer d’une cinquantaine
de transcriptions d’entretiens avec des schizophrènes représente un corpus significatif.
Pour s’entretenir avec une personne prise en charge par le milieu hospitalier, il est en effet nécessaire d’obtenir une
autorisation du CPP (Comité de Protection de la Personne) de la région de l’établissement. Les demandes déposées
doivent contenir explicitement et exactement le contenu du protocole de test. L’instruction du dossier requiert plusieurs
mois et elle demande la contraction d’une assurance pour prendre en charge les possibles dommages. De fait, ce dernier
point augmente considérablement les budgets nécessaires pour ce type d’expérience. Une fois les accords obtenus, il n’est
alors plus possible de modifier les protocoles.
Mais ce qui rend la constitution d’une telle ressource complexe est principalement la difficulté de faire participer les
patients. Plusieurs problèmes se posent. Il faut d’abord identifier, au sein d’un service, les patients répondant aux critères
de l’étude en capacité d’interagir avec une personne tierce au service. Puis il faut, au sein de cette population, trouver les
patients qui acceptent de participer à l’étude. Une première réticence vient du fait qu’il n’y a pas de conséquence positive,
en terme médical, pour le patient à participer à l’étude. Il faut ajouter à cela des inquiétudes compréhensibles des patients
schizophrènes concernant la possible publication de leur histoire, bien qu’une anonymisation totale soit garantie par le
protocole. Bien entendu, la sous-catégorie des schizophrènes paranoïdes est encore plus difficile à rencontrer.
Par ailleurs, le protocole requérant de passer des tests psycho-cognitifs et un entretien, le temps nécessaire est relativement
élevé, de l’ordre de deux heures. Ce n’est pas tant la disponibilité des patients qui est alors en jeu, que leur aptitude à
rester concentrés. Lorsque le patient présente soudainement des difficultés, il faut convenir d’un second rendez-vous pour
finaliser le protocole. La multiplication des rendez-vous génère également des défections. À titre d’exemple, lors de la
phase de collecte des entretiens du sous-corpus Ville1, 45 % (18) des patients contactés ont refusé de participer, 10 % ont
accepté un premier rendez-vous mais ne sont pas présentés au second, et 45 % (18 sujets) ont participé à toute l’étude.
2.3    Anonymisation

La tâche d’anonymisation recouvre deux phases. La première, tout à fait classique, consiste à identifier les entités nom-
mées et à les substituer par des marqueurs sémantiquement vides. Un outil automatique performant a été identifié pour
ce faire, mais n’a pu être opérationnel à temps pour cette étude. Nous avons pour cela programmé une série de scripts
en Python qui recherchent, grâce à des expressions régulières, les mots commençant par une majuscule qui ne sont pas
295

É TUDE QUANTITATIVE DES DISFLUENCES DANS LE DISCOURS DE SCHIZOPHRÈNES                             [O-S2.1]
en début de phrase. Une intervention humaine a été ensuite nécessaire pour classer ces mots en 10 catégories : prenomF,
prenomM, nom, pays, département, ville, capitale, institution, montagne et non_pris_en_compte. Les éléments de cette
dernière catégorie ont été laissés tels quels dans le corpus, les autres ont été substitués par le nom de la catégorie suivie
d’un identifiant unique. Ainsi, les références à Paris sont toutes identifiées par capitale1. Une fois ces substitutions réali-
sées, nous avons extrait l’ensemble des débuts de phrases et procédé à une vérification manuelle pour affiner les listes des
catégories précédentes. Nous pouvons ainsi assurer une anonymisation fiable du corpus.
L’anonymisation du corpus ne s’arrête cependant pas là. En effet, les sujets relatant des événements s’inscrivant dans une
temporalité et une géographie particulière, un certain nombre d’indices sont disséminés dans les entretiens. Il est donc
relativement aisé d’identifier les personnes et il est difficile de trouver une solution à ce problème tout en conservant
l’intégrité des entretiens. Cette particularité a des conséquences importantes sur notre projet.
Pour les traitements qui ne nécessitent qu’un faible contexte, en général celui de la phrase ou du tour de parole, nous
avons créé une version de la ressource constituée de tous les tours de paroles randomisés. Les 31 575 tours de paroles
sont donc mélangés et il devient impossible de reconstituer les historiques de chacun. Pour reconstruire les entretiens
originaux, nous conservons une trace de la randomisation sous forme de table. Il est donc tout à fait possible de fournir
la ressource pour des analyses du type morpho-syntaxe ou analyse syntaxique en dépendances, sans compromettre les
données initiales.
Mais l’un des objectif du projet global reste l’analyse sémantico-pragmatique et, pour ces aspects, il est impossible de
dissocier une prise de parole de son contexte sans perdre l’essence même de l’entretien. Seuls les membres engagés dans
le projet et soumis à un devoir de confidentialité peuvent donc travailler sur cette partie. Un problème similaire se pose
pour la partie transcription, puisque, bien que les bandes puissent être bippées, elles ne peuvent pas être randomisées en
tours de parole. Cette contrainte explique que le nombre d’intervenants sur la transcription reste limité.
3     Protocole expérimental

3.1    Traitements automatiques

Étant donnée la taille importante des corpus et notre volonté de limiter les interventions humaines, nous avons utilisé
l’outil Distagger (Constant & Dister, 2010) pour identifier automatiquement les disfluences dans les textes transcrits.
Pour cet outil, les disfluences regroupent plusieurs types de réalisations orales qui brisent la continuité syntaxique. Il
est donc possible de produire une version reconstruite de la ressource pour obtenir des tours de parole plus cohérents
du point de vue syntaxique, donc améliorer les résultats des annotations pour d’autres couches (en particulier l’analyse
morphosyntaxique ou en dépendances).
L’outil permet d’identifier des réalisations de natures différentes, pour lesquelles quatre restent prédominantes dans les
corpus oraux : les euh, les répétitions, les autocorrections immédiates et les amorces de morphèmes. Nous revenons sur
chacune d’elles en présentant un exemple extrait du corpus.
1. Les différentes réalisations de euh sont définies dans un fichier passé en argument de Distagger.
(1)   moi ça m’est presque plus euh difficile et euh anti-naturel de parler
2. Les répétitions sont entendues comme la reprise explicite et identique d’un même mot ou d’un même groupe de
mots dans le contexte immédiat d’apparition. La répétition peut malgré tout contenir ou être précédée d’un mot
creux comme oui, non, ou un euh :
(2)   j’ arrive à être à être concentrée quand il faut faire quelque chose
3. L’autocorrection immédiate est une variante de la répétition dans laquelle un trait morphologique peut varier (ce
qui apparaît régulièrement avec les déterminants) :
(3)   enfin je sais pas trop le les termes
4. L’amorce est une interruption de morphème en cours d’énonciation. La fin du mot est marquée par un -.
(4)   pis progressivement vous av- pouvez travailler sur votre concentration
296

M AXIME A MBLARD , K ARËN F ORT                                                      [O-S2.1]
Les auteurs ont évalué leur outil sur un corpus oral marqué en disfluences et validé manuellement. Le corpus de référence
comprend au total 1 297 tours de parole, 22 476 mots, 5 817 méta-étiquettes et 1 280 disfluences. Ils obtiennent des
f-scores significatifs, de 95,5 % (précision de 95,3 %, rappel 95,8 %) 6 .
L’outil prend en entrée des données au format Valibel 7 (sans structure prédéfinie) ou au format transcriber (Bar-
ras et al., 1998) (structuré et semi-annoté). Il fournit deux types de sortie, l’un correspondant au format Valibel, l’autre
au format transcriber. Distagger est implémenté en Java, et peut être appelé en ligne de commande, ce qui nous a
permis de l’intégrer facilement à notre chaîne de traitement. Par ailleurs, l’outil ajoute plusieurs annotations particulières
structurant son résultat, qui ne sont pas informatives pour les disfluences.
Les annotations de Distagger sur le corpus font apparaître sept étiquettes : {IGN +EU H}, {IGN +REP }, {IGN +
CORR}, {IGN +F RAG}, {IGN +short_pause}, {IGN +slot} et {IGN +speaker}. Les deux dernières sont des
étiquettes spécifiques permettant de repérer les tours de parole et les interlocuteurs à qui sont associés ces tours de parole.
Leur nombre n’apporte pas d’information caractéristique ici et elles seront écartées dans la suite. Par ailleurs, les premiers
traitements ont fait apparaître les étiquettes {IGN +short_pause} et {IGN +F RAG} dans des volumes très faibles
(respectivement 5 et 1 étiquettes). Les short_pause correspondent à des reliquats de scories de la transcription qui ont
été mal interprétés par l’outil.
Nous avons par ailleurs mis en place une série de programmes en Python pour pré-traiter les corpus, appliquer Distagger
et post-traiter les résultats fournis.
3.2     Normalisation des corpus

Le sous-corpus Ville2 initial n’ayant pas été prévu pour être traité par des outils de TAL, une première étape a donc
consisté à extraire le contenu des documents en format MS Word et à normaliser le corpus à l’aide d’une trentaine
d’expressions régulières. Il a fallu réinterpréter les marques spécifiques à la transcriptions originelle vers des marques
explicites pour Distagger (↑ pour une intonation montante, ↓ pour une intonation descendante, etc.). Cinq traitements
sont nécessaires pour ajouter aux fichiers les informations permettant à Distagger de fonctionner (utilisation de spk1
et spk2 pour les interlocuteurs 8 , chemin explicite des fichiers, etc.). Ainsi, pour chaque fichier du sous-corpus de départ,
nous obtenons sa version annotée par Distagger.
Puis, l’ensemble des résultats obtenus est fusionné pour produire une représentation générale pour ce sous-corpus, en
associant à chaque tour de parole le numéro du corpus (ici 1), l’identifiant du sujet (deux chiffres et trois lettres), le
numéro du tour de parole dans l’entretien, ainsi qu’une marque explicite de qui est l’interlocuteur (P a pour le patient, et
P pour le psychologue). Enfin, il manque une information discriminante qui est le statut du sujet : schizophrène ou témoin.
Pour cela, une base qui distingue entre les deux, et dont la hiérarchie est fixée préalablement, est automatiquement produite
à partir de la structure du corpus de départ. Nous construisons alors une représentation abstraite du corpus à partir de la
fréquence d’apparition des différentes étiquettes de Distagger.
Le sous-corpus Ville1 s’inscrit dans le cadre du projet général. Les annotateurs ont utilisé l’outil CLAN pour réaliser la
transcription, ce qui nous a permis d’extraire facilement le contenu textuel. Cependant, de nombreuses marques dépen-
dantes du logiciel perdurent et il a été nécessaire de les supprimer. À nouveau, une trentaine d’expressions régulières a été
utilisée. Comme pour Ville2, nous produisons une ressource contenant l’ensemble des tours de parole randomisé. C’est
sur cette ressource que Distagger est utilisé. Puis, en utilisant la table de mémorisation, les entretiens sont reconstruits
et envoyés à la même série de traitements que le corpus Ville2. Enfin, pour faciliter les études sur le contenu des entretiens,
chacun est mémorisé sans aucune autre marque que l’interlocuteur (P ou S ou T) 9 .
Il apparaît dans notre corpus que certaines annotations ne correspondent pas à des disfluences traditionnelles. En effet,
le psychologue ayant une interaction particulière dans l’entretien, puisque son rôle est d’abord de maintenir l’échange,
il utilise régulièrement des interventions de type mmh mmh, ou oui oui, ou non non. Nous avons donc mis en place un
post-traitement qui redresse les résultats en supprimant les étiquettes correspondantes. De plus, la forme pronominale est
très fréquemment utilisée, puisque les interlocuteurs se vouvoient et que le psychologue relance la conversation en posant
des questions à caractère personnel. Nous avons donc inclus à ces post-traitements les formes vous vous qui apparaissent
6. Une évaluation de l’outil sur un échantillon de données (4 entretiens) a mis au jour un taux d’erreur compris entre 5 et 10 %. Une analyse de ces
erreurs a montré qu’elles étaient majoritairement dues à des interruptions mal identifiées, problème que nous avons corrigé depuis.
7. Voir : https://www.uclouvain.be/cps/ucl/doc/valibel/documents/conventions_valibel_2004.PDF.
8. spk1 est attribué au premier locuteur et ne correspond pas nécessairement au psychologue.
9. Il nous semble important de prendre en considération les disfluences du psychologue qui influencent le cours de l’interaction.
297

É TUDE QUANTITATIVE DES DISFLUENCES DANS LE DISCOURS DE SCHIZOPHRÈNES                               [O-S2.1]
(a)                                                       (b)
F IGURE 2 – Distribution des étiquettes de disfluence au cours de l’entretien 011HAM du sous-corpus Ville1 (a est un tour
de parole du patient et s, du psychologue).
en nombre très supérieur par rapport aux répétitions de vous.
Enfin, nous avons automatisé la production des graphiques pour chaque entretien, reprenant le nombre de disfluences
par tour de parole, comme présenté dans la figure 2. Pour chaque sous-corpus nous produisons une surface où chaque
entretien est ramené à l’échelle en pourcentage (le nombre de tours de parole fluctuant beaucoup) en fonction du nombre
de disfluences présent dans la portion de texte (voir figure 3. Enfin, nous produisons plusieurs documents LATEX reprenant
les moyennes des disfluences par tour de parole et par nombre de mots, et nous calculons pour chaque sous-corpus
leur indice de significativité. Nous produisons ensuite une représentation graphique de la position des disfluences dans
l’entretien. L’ensemble de ces programmes de normalisation représente environ 1 500 lignes de code.
4     Résultats

4.1    Analyse quantitative

La figure 2 présente un exemple des résultats obtenus, pour le patient 011HAM (sous-corpus Ville1). Dans la première
figure (a), une couleur de courbe est attribuée à chacune des trois étiquettes principales, l’axe des abscisses correspond
aux tours de parole de l’entretien et celui des ordonnées au nombre de disfluences dans ce tour de parole. Pour les points
où l’ordonnée est différente de 0, une étiquette est ajoutée, a pour les tours de paroles du patient, et s pour le psychologue.
La seconde figure présente les mêmes données, en exhibant la somme du nombre d’étiquettes pour le même tour de parole.
Ainsi, la première valeur significative est à 4 dans la figure a et à 5 dans la figure b. Il se trouve que dans ce tour de parole
Distagger identifie 4 euh et 1 rep, ce qui explique la différence observée.
Une lecture de l’ensemble des graphiques fait apparaître une régularité avec deux pics de disfluences, le premier en début
d’entretien, le second au cours du dernier tiers. Le premier pic peut simplement s’entendre comme un pic de stress en
début d’échange. Il est intéressant de constater que le second pic amène la fin de l’entretien. Du côté des témoins, on
retrouve une entame d’entretien avec un pic, mais pas nécessairement un second.
La table 3 présente l’ensemble des résultats de Distagger. Pour chacun des deux sous-corpus nous calculons, pour
les trois étiquettes principales (IGN +CORR, IGN +REP et IGN +EU H), leur fréquence d’apparition dans les tours
de parole, des schizophrènes (S), des témoins (T) ou du/de la psychologue (P). Nous normalisons d’une part par rapport
au nombre de tours de parole (pour chaque catégorie d’interlocuteur), et d’autre part par rapport au nombre de mots (à
nouveau pour chaque type d’interlocuteur). Nous calculons les mêmes valeurs pour les sujets (S + T), le ou la psychologue
lorsqu’il ou elle est en face d’un schizophrènes (P+S) ou devant un témoin (P+T). Les résultats totaux reprennent la
298

M AXIME A MBLARD , K ARËN F ORT                                              [O-S2.1]
corpus Ville2                                                   corpus Ville1
S         T         S+T      P+S        P+T        P           S         T         S+T      P+S        P+T       P
IGN +CORR
par tour de parole   0, 0087   0, 0071   0, 008    0, 0015   0          0, 0012     0, 0180   0, 0085   0, 0127   0, 0052   0, 0109   0, 0084
par / nb mots        0, 0004   9e − 05   0, 0003   0, 0001   0          0, 0001     0, 0013   0, 0007   0, 0010   0, 0006   0, 0007   0, 0006
IGN +REP
par tour de parole   0, 2223   0, 2519   0, 2285   0, 0646   0, 0897    0, 0699     0, 2735   0, 138    0, 1978   0, 1336   0, 2608   0, 205
par / nb mots        0, 0125   0, 0078   0, 0115   0, 0064   0, 0079    0, 0067     0, 0211   0, 0134   0, 0168   0, 0171   0, 0177   0, 0174
IGN +EU H
par tour de parole   0, 3107   0, 2999   0, 3084   0, 0738   0, 0616    0, 0712     0, 4201   0, 3372   0, 3736   0, 1948   0, 4651   0, 3464
par / nb mots        0, 0190   0, 0089   0, 0169   0, 0077   0, 0058    0, 0073     0, 0369   0, 0326   0, 0345   0, 0244   0, 0312   0, 0282
Resultats totaux
par tour de parole   0, 5417   0, 5589   0, 545    0, 1400   0, 1513    0, 1424     0, 7117   0, 484    0, 5842   0, 3338   0, 7369   0, 5599
par / nb mots        0, 032    0, 0168   0, 0288   0, 0144   0, 0138    0, 0142     0, 0595   0, 0468   0, 0524   0, 0421   0, 0496   0, 0463

TABLE 3 – Répartition quantitative des étiquettes de Distagger dans les sous-corpus.
somme des valeurs intermédiaires pour chaque catégorie d’interlocuteur. Distagger n’annote aucune correction pour
le psychologue avec des témoins dans le sous-corpus Ville2, ce qui explique les deux valeurs à zéro.
La lecture des résultats totaux met en avant une variabilité importante des résultats normalisés par rapport au nombre de
tours de parole. Les résultats normalisés par rapport au nombre de mots sont plus significatifs. En effet, si les disfluences
produites par les témoins et le psychologue (quel que soit son interlocuteur) sont du même ordre : 1, 68 % et 1, 42 %
pour le sous-corpus Ville2, et 4, 68 % et 4, 63 % pour le sous-corpus Ville1, les productions des schizophrènes sont
bien supérieures : 3, 2 % et 5, 95 %. Il existe ainsi une différence entre le nombre de disfluences identifiées chez les
schizophrènes et les non schizophrènes de 1, 63 % dans le sous-corpus Ville2 et de 1, 29 % dans le sous-corpus Ville1.
La variabilité des résultats peut s’expliquer par la différence des transcriptions entre les deux sous-corpus, ainsi que par le
nombre de sujets dans chacun. S’il n’est pas raisonnable de proposer le calcul d’un résultat pour l’ensemble du corpus, la
constance de la différence de résultats conduit à notre conclusion.
Enfin, pour visualiser la répartition des résultats, nous produisons une surface où nous normalisons le nombre de tours
de parole sur une échelle de 100. Pour chaque entretien seuls les tours de parole, soit du schizophrène, soit du témoin,
sont utilisés et nous recalculons le nombre de disfluences sur un intervalle de 1 % de l’entretien. Nous obtenons ainsi des
pics où, en valeur, le nombre est supérieur aux résultats précédents, mais qui correspondent à une combinaison linéaire de
plusieurs tours de parole. La figure 3 présente les surfaces calculées pour les deux sous-corpus. Les deux figures de gauche
correspondent au sous-corpus de Ville1, les deux figures de droite à celui de Ville2. Les figures en haut correspondent aux
tours de parole des schizophrènes et celles en bas à ceux des témoins.
Dans le cas de Ville2, il apparaît de manière évidente que les témoins produisent beaucoup moins de disfluences que
les schizophrènes. La surface en bas à gauche est en effet quasiment plane. L’interprétation des graphiques pour le sous-
corpus Ville1 est plus délicate. Pour cela, nous présentons la projection de l’ensemble des entretiens sur l’axe pourcentage
du graphique. La couleur bleu correspond à une densité importante sur la projection et la couleur rouge à une densité plus
faible. En étudiant les valeurs ainsi trouvées pour le sous-corpus, on trouve une distribution régulière chez les témoins,
mais toujours plus marginale que chez les schizophrènes.
4.2     Significativité

Afin de valider les résultats que nous avons pu mettre en avant, nous reprenons ici la mesure de significativité utilisée dans
(de Mareüil et al., 2013). Celle-ci permet de calculer un indice de distribution en fonction du nombre de mots entre deux
catégories d’interlocuteurs. La valeur trouvée doit être supérieure à 1,96 pour être considérée comme significative.

(p1 − p2 )
s=
p(1 − p)( n11 +      1
n2 )
299

É TUDE QUANTITATIVE DES DISFLUENCES DANS LE DISCOURS DE SCHIZOPHRÈNES                            [O-S2.1]
(a)                                        (b)
(c)                                        (d)
F IGURE 3 – Répartitions des étiquettes de disfluences : dans les entretiens de schizophrènes, sous-corpus Ville1 (a), Ville2
(b) ; dans les entretiens de témoins, sous-corpus Ville1 (c), Ville2 (d).
où :
p = (n1 p1 + n2 p2 )/(n1 + n2 )
–   n1 est le nombre de mots prononcés par la première catégorie d’interlocuteur,
–   n2 est le nombre de mots prononcés par la seconde catégorie d’interlocuteur,
–   p1 est la proportion de disfluences produites par la première catégorie d’interlocuteur,
–   p2 est la proportion de disfluences produites par la seconde catégorie d’interlocuteur.

Cette formule nous permet d’interpréter les résultats entre deux catégories d’interlocuteurs. Nous calculons donc l’indice
pour les trois appariements que nous pouvons proposer. Les résultats obtenus sont présentés dans le tableau 4.

corpus Ville1          corpus Ville2
S et Psy 10, 6806923083         19, 4197596818
T et Psy 0, 422898291704        3, 23530253756
S et T   10, 2827554261         16, 0376100956

TABLE 4 – Significativité des différences dans le nombre de disfluences entre interlocuteurs.
Il apparaît que les significativités entre les témoins et les psychologues sont faibles, voire non significatives, ce qui per-
met de rapprocher le comportement des témoins de celui des psychologues. Par contre, la significativité est importante
(toujours supérieure à 10) dans les appariements qui comprennent des schizophrènes, ce qui nous permet de conclure que
le nombre de disfluences produites par des schizophrènes est significativement différent de celui des non-schizophrènes
de l’expérimentation (psychologue et témoins). Ce résultat est d’autant plus pertinent qu’il est mis en avant par des outils
automatiques et ne fait pas intervenir de subjectivité humaine.
300

M AXIME A MBLARD , K ARËN F ORT                               [O-S2.1]
nb étiquettes      /nb mots / nb tours de parole
IGN +EN R               2                  2e − 05 0, 00037
IGN +REP                2 208              0, 01857 0, 40314
IGN +M eta              161                0, 00135 0, 0294
IGN +EU H               2 773              0, 02333 0, 5063
IGN +CORR               138                0, 00116 0, 0252

TABLE 5 – Analyse des disfluences dans le corpus TCOF-POS par Distagger.
4.3    Biais potentiels des expériences

Malgré la significativité des résultats que nous avons obtenus, il nous paraît important de revenir sur plusieurs biais
potentiels de l’étude.
Les deux sous-corpus ayant été transcrits par des moyens différents et jamais en parallèle, comme nous l’avons discuté
dans la section 2, il est difficile de proposer une évaluation qualitative des transcriptions. Afin de vérifier que nos résultats
ne dévient pas de la réalité linguistique, nous avons appliqué Distagger sur le corpus de parole spontanée TCOF-
POS (Benzitoun et al., 2012). La table 5 reprend la ventilation des étiquettes trouvées par Distagger. Sur les trois
étiquettes que nous retrouvons et avons analysé, le nombre de disfluences est de 4, 3 %, ce qui est comparable aux
résultats précédents et nous conduit à considérer la transcription comme un faible biais.
Un autre biais réside dans la répartition entre témoins et patients à l’intérieur même de chaque sous-corpus. Ainsi, le sous-
corpus Ville2 ne contient que 8 témoins, alors que le sous-corpus Ville1 en contient 23. Nous avons décidé de conserver
tous les témoins dont nous disposions pour équilibrer le corpus général. Le projet s’attache à rééquilibrer la répartition.
Les témoins du sous-corpus Ville1 produisent davantage de disfluences que ceux du sous-corpus Ville2. Une lecture des
entretiens montre que la psychologue qui a recueilli les entretiens du sous-corpus Ville1 produit plus de disfluences, ce
qui peut inciter les interlocuteurs à l’imiter, donc à produire davantage de disfluences.
Il existe par ailleurs une différence d’âge et de QI entre les participants schizophrènes et témoins (voir tableau 6). En effet,
les schizophrènes sont significativement plus âgés que les témoins (près de 29 ans au lieu de 23, avec p = 0, 0058 10 ) et
leur QI est inférieur (à peu près 95 au lieu de 103, avec p = 0, 0203), pour un nombre d’années d’école ou d’études très
semblable (environ 13 pour les témoins et 12,4 pour les schizophrènes).

QI     années d’études    âge
femmes
témoins                    105, 5           13         22, 37
schizophrènes              98, 33           13          30
hommes
témoins                   102, 73         13, 26       23, 66
schizophrènes              94, 53         12, 28       28, 66
moyenne générale
témoins                   103, 70         13, 17       23, 22
schizophrènes              95, 17         12, 41       28, 89

TABLE 6 – Moyennes des QI, du nombre d’années d’études et des âges sur les participants au corpus Ville1.

Un autre biais important, mais inévitable, de l’étude est que les patients sont en remédiation, donc sous traitement (Chlor-
promazine à Ville2 et neuroleptiques non spécifiés à Ville1). Levy (1968) a identifié des effets négatifs (en l’occurrence,
une baisse des performances) de la Chlorpromazine sur la syntaxe de quatre patients schizophrènes en calculant le ratio
du nombre de propositions subordonnées produites sur la totalité des propositions produites. En outre, cet antipsycho-
tique semble provoquer des bégaiements (Ward, 2008). Cependant, Goldman-Eisler et al. (1965) a montré (sur des sujets
non schizophrènes) que les effets de cette même molécule sur les temps de pause du locuteur sont très variables selon
les individus et qu’un temps de pause supérieur permet au groupe testé de générer des structures verbales complexes,
10. Les significativités ont ici été calculées à l’aide du test de Student.
301

É TUDE QUANTITATIVE DES DISFLUENCES DANS LE DISCOURS DE SCHIZOPHRÈNES                            [O-S2.1]
comme chez les témoins. Pour ajouter à ces incertitudes, Kremen et al. (2003) ont montré que des patients bipolaires sous
antipsychotiques (dont fait partie la Chlorpromazine) présentent une meilleure fluence sémantique que les témoins. Il est
donc aujourd’hui extrêmement difficile d’évaluer l’influence exacte du traitement sur les productions des patients. Cette
question est récurrente dans la littérature. Il apparaît néanmoins que les effets secondaires des médicaments sont moins
prégnants aujourd’hui, les traitements ayant considérablement évolué depuis les années 60. Par ailleurs, nous disposons
d’un sous-groupe de 7 patients schizophrènes sans traitement dans le sous-corpus Ville2. Les différences entre eux et les
autres patients schizophrènes n’apparaissent pas significatives.
5    Conclusions et perspectives
Cette étude nous a permis de mettre en lumière un usage pathologique des disfluences chez les patients schizophrènes
grâce à des outils et des méthodes issus du TAL. Pour cela, nous avons utilisé l’outil Distagger pour procéder à une
annotation des disfluences. Il apparaît que les schizophrènes produisent, respectivement dans chaque corpus, 1,63 % et
1,29 % de disfluences de plus (par rapport au nombre de mots) que des sujets non diagnostiqués. Nous avons validé ce
résultat par un calcul de significativité qui isole clairement les patients schizophrènes. Ce sont les outils de TAL qui ont
permis d’aboutir à cette conclusion, en dehors de toute interprétation humaine.
Nous avons par ailleurs discuté des différents biais possibles de l’étude, tant sur la constitution du corpus de départ que
sur la méthodologie utilisée. La suite du projet s’attachera à revenir sur ces derniers pour les corriger lorsque cela est
possible, en particulier en calculant une mesure de qualité des transcriptions. Mais l’une des difficultés principales réside
dans la nature de l’objet d’étude. D’une part, les patients schizophrènes doivent être suffisamment communiquant pour
passer le protocole de tests et donc, généralement, être sous traitement. D’autre part, le caractère personnel de l’entretien
pose plusieurs questions d’éthique. Par conséquent, si il est difficile d’accéder aux patients, il est tout aussi délicat de
gérer l’accès à la ressource.
Dans la continuité du projet, nous souhaitons annoter la ressource en morpho-syntaxe et en syntaxe en dépendances. L’an-
notation en morpho-syntaxe nous permettra de réaliser automatiquement une lemmatisation du corpus, notamment pour
évaluer la richesse du vocabulaire des sujets. Pour cela, nous allons utiliser l’outil MElt (Denis & Sagot, 2009) entraîné
pour le français oral sur le corpus TCOF-POS (Benzitoun et al., 2012) 11 . L’analyse en dépendances nous permettra de
revenir sur les résultats anciens auxquels nous avons fait référence sur la complexité de la syntaxe utilisée par les patients.
Pour cela, plusieurs outils sont disponibles, dont FRMG (De La Clergerie et al., 2009), Leopar (Perrier & Guillaume,
2013) et Talismane (Urieli & Tanguy, 2013), plusieurs tests préliminaires ont d’ailleurs été d’ores et déjà réalisés.
L’utilisation de plusieurs outils nous permettra de valider les analyses proposées.
Comme il a été fait mention dans la première partie de cet article, l’objectif est également de proposer une annotation
en sémantique-pragmatique. Pour cela, nous conduirons deux campagnes d’annotation manuelle, l’une pour identifier les
discontinuités décisives, l’autre, sur les extraits identifiés, pour annoter en SDRT. L’ensemble de ces indices seront corrélés
avec les autres mesures dont nous disposons, dont les résultats aux tests psycho-cognitifs, les mesures oculométriques et
les EEG.
La contribution apportée par cette étude au projet général montre l’importance d’utiliser des outils automatiques pour
mettre en avant des indices objectifs. Il n’en reste pas moins qu’il est nécessaire d’affiner les résultats. Notre perspective
principale est de proposer une ressource normalisée, riche en méta-données (dont le manque et l’importance sont mis en
valeur dans (Ghio et al., 2006)), malgré les nombreuses difficultés éthiques que posent ces travaux.
Références
A SHER N. & L ASCARIDES A. (2003). Logics of Conversation. Studies in Natural Language Processing. Cambridge
University Press.
BARRAS C., G EOFFROIS E., W U Z. & L IBERMAN M. (1998). Transcriber : a free tool for segmenting, labeling and
transcribing speech. In International Conference on Language Resources and Evaluation (LREC), p. 1373–1376.
B ENZITOUN C., F ORT K. & S AGOT B. (2012). TCOF-POS : un corpus libre de français parlé annoté en morphosyntaxe.
In Traitement Automatique des Langues Naturelles (TALN), p. 99–112, Grenoble, France.
11. Les performances de ce outil avec ce modèle atteignent 97,61 % d’exactitude.
302

M AXIME A MBLARD , K ARËN F ORT                                     [O-S2.1]
B LANCHE -B ENVENISTE C. & J EANJEAN C. (1987). Le Francais parlé. Transcription et édition. Paris, France : Didier
Érudition.
C HAIKA E. (1974). A linguist looks at “schizophrenic” language. Brain and Language, 1(3), 257–276.
C ONSTANT M. & D ISTER A. (2010). Automatic detection of disfluencies in speech transcriptions. In I. C. F. D.
M. P ETTORINO , A. G IANNINI, Ed., Spoken Communication, volume 1, p. 259–272. Cambridge Scholars Publishing.
D E L A C LERGERIE É., S AGOT B., N ICOLAS L. & G UÉNOT M.-L. (2009). FRMG : évolutions d’un analyseur syn-
taxique TAG du français. In É. V ILLEMONTE DE LA C LERGERIE & P. PAROUBEK, Eds., Journée de l’ATALA sur :
Quels analyseurs syntaxiques pour le français ?, Paris, France : ATALA. Journée de l’ATALA organisée conjointement
à la conférence IWPT 2009.
DE M AREÜIL P. B., A DDA G., A DDA -D ECKER M., BARRAS C., H ABERT B. & PAROUBEK P. (2013). Une étude
quantitative des marqueurs discursifs, disfluences et chevauchements de parole dans des interviews politiques. TIPA.
Travaux interdisciplinaires sur la parole et le langage [En ligne], 29. mis en ligne le 19 décembre 2013, consulté le 14
février 2014.
D ENIS P. & S AGOT B. (2009). Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art POS
tagging with less human effort. In Pacific Asia Conference on Language Information and Computing (PACLIC).
F ELDSTEIN S. (1962). The relationship of interpersonal involvement and affectiveness of content to the verbal commu-
nication of schizophrenic patients. Journal of Abnormal and Social Psychology, 64, 39–45.
F ROMKIN V. A. (1975). A linguist looks at “a linguist looks at ‘schizophrenic language”’. Brain and Language, 2(0),
498 – 503.
G HIO A., T ESTON B., V IALLET F., JANKOWSKI L., P URSON A., D UEZ D., L OCCO J., L EGOU T., P INTO S., M AR -
CHAL A., G IOVANNI A., ROBERT D., R ÉVIS J., F REDOUILLE C., B ONASTRE J.-F., P OUCHOULIN G. & N GUYEN
N. (2006). Corpus de parole pathologique, état d’avancement et enjeux méthodologiques. Travaux Interdisciplinaires
du Laboratoire Parole et Langage d’Aix-en-Provence (TIPA), 25, 109–126. Autorisation No.3015 : TIPA est la revue du
Laboratoire Parole et Langage 3015 3015.
G OLDMAN -E ISLER F., S KARBEK A. & H ENDERSON A. (1965). The effect of chorpromazine on speech behaviour.
Psychopharmacologia, 7(3), 220–229.
K AMP H. & R EYLE U. (1993). From Discourse to Logic. Kluwer Academic Publishers.
K REMEN W. S., S EIDMAN L. J., FARAONE S. V. & T SUANG M. T. (2003). Is there disproportionate impairment or
phonemic fluency in schizophrenia ? Journal of the International Neuropsychological Society, 9, 79–88.
L EVY R. (1968). The effect of chlorpromazine on sentence structure of schizophrenic patients. Psychopharmacologia,
13(5), 426–432.
M AHER B. (1972). The language of schizophrenia : A review and interpretation. The British Journal of Psychiatry, 120,
3–17.
M USIOL M., A MBLARD M. & R EBUSCHI M. (2013). Approche sémantico-formelle des troubles du discours : les
conditions de la saisie de leurs aspects pyscholinguistiques. In 27ème Congrès International de Linguistique et de
Philologie Romanes, Nancy, France.
M USIOL M. & T ROGNON A. (1996). L’accomplissement interactionnel du trouble schizophrénique. Raisons Pratiques
7, p. 179–209.
P ERRIER G. & G UILLAUME B. (2013). Leopar : an Interaction Grammar Parser. In Workshop on High-level Methodo-
logies for Grammar Engineering, ESSLLI, p. 121–122, Dusseldorf.
R EBUSCHI M., A MBLARD M. & M USIOL M. (2013). Using SDRT to analyze pathological conversations. Logicality,
rationality and pragmatic deviances. In M. R EBUSCHI , M. BATT, G. H EINZMANN , F. L IHOREAU , M. M USIOL & A.
T ROGNON, Eds., Interdisciplinary Works in Logic, Epistemology, Psychology and Linguistics : Dialogue, Rationality,
and Formalism, Logic, Argumentation & Reasoning, p. 1–24. Springer.
U RIELI A. & TANGUY L. (2013). L’apport du faisceau dans l’analyse syntaxique en dépendances par transitions : études
de cas avec l’analyseur Talismane. In Actes de la 20e conférence sur le Traitement Automatique des Langues Naturelles
(TALN’2013), p. 188–201, Les Sables d’Olonne, France.
V ERHAEGEN F. (2007). Psychopathologie cognitive des processus intentionnels schizophréniques dans l’interaction
verbale. PhD thesis, Université Nancy 2, France.
WARD D. (2008). Stuttering and Cluttering : Frameworks for Understanding and Treatment. Taylor & Francis.
303
